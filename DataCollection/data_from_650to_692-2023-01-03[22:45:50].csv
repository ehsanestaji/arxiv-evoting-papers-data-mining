,Title,Summary,DOI,Categories,Publish Time,Authors,Journal
0,Risk Automatic Prediction for Social Economy Companies using Camels,"Governments have to supervise and inspect social economy enterprises (SEEs).
However, inspecting all SEEs is not possible due to the large number of SEEs
and the low number of inspectors in general. We proposed a prediction model
based on a machine learning approach. The method was trained with the random
forest algorithm with historical data provided by each SEE. Three consecutive
periods of data were concatenated. The proposed method uses these periods as
input data and predicts the risk of each SEE in the fourth period. The model
achieved 76\% overall accuracy. In addition, it obtained good accuracy in
predicting the high risk of a SEE. We found that the legal nature and the
variation of the past-due portfolio are good predictors of the future risk of a
SEE. Thus, the risk of a SEE in a future period can be predicted by a
supervised machine learning method. Predicting the high risk of a SEE improves
the daily work of each inspector by focusing only on high-risk SEEs.",2210.05052v1,cs.LG,2022-10-10 23:50:02+00:00,"[arxiv.Result.Author('Joseph Gallego-Mejia'), arxiv.Result.Author('Daniela Martin-Vega'), arxiv.Result.Author('Fabio Gonzalez')]",
1,Triviality and the (Supersymmetric) See-Saw,"For the D=5 Majorana neutrino mass operator to have a see-saw ultraviolet
completion that is viable up to the Planck scale, the see-saw scale is bounded
above due to triviality limits on the see-saw couplings. For supersymmetric
see-saw models, with realistic neutrino mass textures, we compare constraints
on the see-saw scale from triviality bounds, with those arising from
experimental limits on induced charged-lepton flavour violation, for both the
CMSSM and for models with split supersymmetry.",hep-ph/0603053v2,hep-ph,2006-03-07 16:47:09+00:00,"[arxiv.Result.Author('Bruce A. Campbell'), arxiv.Result.Author('David W. Maybury')]","JHEP 0704:077,2007"
2,Astronomical Seeing at Maidanak Observatory during the year 2018,"Astronomical seeing measurements were carried out at Maidanak observatory
during the period from August to November 2018 using DIMM (Differential Image
Motion Monitor). The median value of seeing for the entire period was
determined as 0.54 arcseconds. This value was compared to the seeing data of
the period 1996-2002.",2012.08110v1,astro-ph.IM,2020-12-15 06:19:17+00:00,"[arxiv.Result.Author('Y. A. Tillayev'), arxiv.Result.Author('A. M. Azimov'), arxiv.Result.Author('A. R. Hafizov')]",
3,"The see-saw mechanism: neutrino mixing, leptogenesis and lepton flavor violation","The see-saw mechanism to generate small neutrino masses is reviewed. After
summarizing our current knowledge about the low energy neutrino mass matrix we
consider reconstructing the see-saw mechanism. Low energy neutrino physics is
not sufficient to reconstruct see-saw, a feature which we refer to as ``see-saw
degeneracy''. Indirect tests of see-saw are leptogenesis and lepton flavor
violation in supersymmetric scenarios, which together with neutrino mass and
mixing define the framework of see-saw phenomenology. Several examples are
given, both phenomenological and GUT-related. Variants of the see-saw mechanism
like the type II or triplet see-saw are also discussed. In particular, we
compare many general aspects regarding the dependence of LFV on low energy
neutrino parameters in the extreme cases of a dominating conventional see-saw
term or a dominating triplet term. For instance, the absence of mu -> e gamma
or tau -> e gamma in the pure triplet case means that CP is conserved in
neutrino oscillations. Scanning models, we also find that among the decays mu
-> e gamma, tau -> e gamma and tau -> mu gamma the latter one has the largest
branching ratio in (i) SO(10) type I see-saw models and in (ii) scenarios in
which the triplet term dominates in the neutrino mass matrix.",0804.3925v3,hep-ph,2008-04-24 13:44:29+00:00,[arxiv.Result.Author('Werner Rodejohann')],"Pramana 72:217-227,2009"
4,Statistics of turbulence profile at Cerro Tololo,"Results of 3-month continuous monitoring of turbulence profile and seeing at
Cerro Tololo (Chile) in May-July 2002 are presented. Some 28000 low-resolution
profiles were measured by a new MASS single-star turbulence monitor,
accompanied by seeing data from DIMM. The median seeing was 0.95 arcseconds.
The first 500 m contribute 60% to the total seeing, the free-atmosphere median
seeing was 0.55 arcseconds. Free-atmosphere seeing is almost never better than
0.15 arcseconds because there is always some turbulence above 12 km. A 4-day
period of calm upper atmosphere with a stable free-atmosphere seeing of 0.2-0.3
arcseconds was noted. A gain in resolution from adaptive compensation of ground
layer will be 1.7 times typically and 2-3 times during such calm periods.
Correlations of the free-atmosphere turbulence with the wind speed at
tropopause and of the ground-layer turbulence with ground wind are studied.
Temporal evolution of turbulence is characterized by recurrent bursts, their
typical duration increases from 15 minutes in low layers to 1-2 hours in high
layers. The large data base of turbulence profiles can be used to test
meso-scale modeling of astronomical seeing.",astro-ph/0209432v1,astro-ph,2002-09-20 16:32:20+00:00,"[arxiv.Result.Author('A. Tokovinin'), arxiv.Result.Author('S. Baumont'), arxiv.Result.Author('J. Vasquez')]",Mon.Not.Roy.Astron.Soc. 340 (2003) 52
5,The Spectrum of HD 3651B: An Extrasolar Nemesis?,This article has been withdrawn; see astro-ph/0611542,astro-ph/0609556v2,astro-ph,2006-09-20 00:36:50+00:00,[arxiv.Result.Author('Adam J. Burgasser')],
6,Criticality versus q in the 2+1-dimensional $Z_q$ clock model,"Paper has been withdrawn, see comment.",cond-mat/0212255v2,cond-mat.supr-con,2002-12-11 17:32:31+00:00,"[arxiv.Result.Author('J. Hove'), arxiv.Result.Author('A. Sudbo')]",
7,Closedness of the Space of AHE Metrics on 4-Manifolds,See comment above.,math/0012167v2,math.DG,2000-12-18 19:05:21+00:00,[arxiv.Result.Author('Michael T. Anderson')],
8,A Machine Learning Approach to Correcting Atmospheric Seeing in Solar Flare Observations,"Current post-processing techniques for the correction of atmospheric seeing
in solar observations -- such as Speckle interferometry and Phase Diversity
methods -- have limitations when it comes to their reconstructive capabilities
of solar flare observations. This, combined with the sporadic nature of flares
meaning observers cannot wait until seeing conditions are optimal before taking
measurements, means that many ground-based solar flare observations are marred
with bad seeing. To combat this, we propose a method for dedicated flare seeing
correction based on training a deep neural network to learn to correct
artificial seeing from flare observations taken during good seeing conditions.
This model uses transfer learning, a novel technique in solar physics, to help
learn these corrections. Transfer learning is when another network already
trained on similar data is used to influence the learning of the new network.
Once trained, the model has been applied to two flare datasets: one from
AR12157 on 2014/09/06 and one from AR12673 on 2017/09/06. The results show good
corrections to images with bad seeing with a relative error assigned to the
estimate based on the performance of the model. Further discussion takes place
of improvements to the robustness of the error on these estimates.",2011.12814v1,astro-ph.SR,2020-11-25 15:17:26+00:00,"[arxiv.Result.Author('John A. Armstrong'), arxiv.Result.Author('Lyndsay Fletcher')]",
9,Daytime Seeing and Solar Limb Positions,"A method to measure the seeing from video made during drift-scan solar
transits is proposed. The limb of the Sun is projected over a regular grid
evenly spaced. The temporal dispersion of the time intervals among the contacts
between solar limb and grid's rows is proportional to the atmospheric seeing.
Seeing effects on the position of the inflexion point of the limb's luminosity
profile are calculated numerically with Fast Fourier Transform. Observational
examples from Locarno and Paris Observatories are presented to show the
asymmetric contributions of the seeing at the beginning and the end of each
drift-scan transit.",1106.2539v1,astro-ph.IM,2011-06-13 19:58:47+00:00,[arxiv.Result.Author('Costantino Sigismondi')],
10,The effect of aperture and seeing on the visibility of sunspots when using early modern and modern telescopes of modest size,"Using the convolution of seeing and diffraction, the relation between seeing
and aperture in the visibility of sunspots is explored. It is shown that even
telescopes with apertures smaller than 5 centimetres are significantly affected
by seeing. Although larger aperture instruments suffer more from seeing than
smaller ones, their level of detail always remains better under the same
atmospheric conditions.",2208.07244v3,astro-ph.SR,2022-08-15 15:00:51+00:00,"[arxiv.Result.Author('Nicolàs de Hilster'), arxiv.Result.Author('Siebren van der Werf')]",
11,Typical duration of good seeing sequences at Concordia,"Context: The winter seeing at Concordia is essentially bimodal, excellent or
quite poor, with relative proportions that depend on altitude above the snow
surface. This paper studies the temporal behavior of the good seeing sequences.
Aims: An efficient exploitation of extremely good seeing with an adaptive
optics system needs long integrations. It is then important to explore the
temporal distribution of the fraction of time providing excellent seeing.
Methods: Temporal windows of good seeing are created by a simple binary
process. Good or bad. Their autocorrelations are corrected for those of the
existing data sets, since these are not continuous, being often interrupted by
technical problems in addition to the adverse weather gaps. At the end these
corrected autocorrelations provide the typical duration of good seeing
sequences. This study has to be a little detailed as its results depend on the
season, summer or winter. Results: Using a threshold of 0.5 arcsec to define
the ""good seeing"", three characteristic numbers are found to describe the
temporal evolution of the good seeing windows. The first number is the mean
duration of an uninterrupted good seeing sequence: it is $\tau_0=7.5$ hours at
8 m above the ground (15 hours at 20 m). These sequences are randomly
distributed in time, with a negative exponential law of damping time
$\tau_1=29$ hours (at elevation 8 m and 20 m). The third number is the mean
time between two 29 hours episodes. It is T=10 days at 8 m high (5 days at 20
m).",1003.3583v1,astro-ph.IM,2010-03-18 14:09:06+00:00,"[arxiv.Result.Author('Eric Fossat'), arxiv.Result.Author('Eric Aristidi'), arxiv.Result.Author('Karim Agabi'), arxiv.Result.Author('Erick Bondoux'), arxiv.Result.Author('Zalpha Challita'), arxiv.Result.Author('Francois Jeanneaux'), arxiv.Result.Author('Djamel Mekarnia')]",
12,Excellent daytime seeing at Dome Fuji on the Antarctic plateau,"Context. Dome Fuji, the second highest region on the Antarctic plateau, is
expected to have some of the best astronomical seeing on Earth. However, site
testing at Dome Fuji is still in its very early stages.
  Aims. To investigate the astronomical seeing in the free atmosphere above
Dome Fuji, and to determine the height of the surface boundary layer.
  Methods. A Differential Image Motion Monitor was used to measure the seeing
in the visible (472 nm) at a height of 11 m above the snow surface at Dome Fuji
during the austral summer of 2012/2013.
  Results. Seeing below 0.2'' has been observed. The seeing often has a local
minimum of ~0.3'' near 18 h local time. Some periods of excellent seeing, 0.3''
or smaller, were also observed, sometimes extending for several hours at local
midnight. The median seeing is higher, at 0.52''---this large value is believed
to be caused by periods when the telescope was within the turbulent boundary
layer.
  Conclusions. The diurnal variation of the daytime seeing at Dome Fuji is
similar to that reported for Dome C, and the height of the surface boundary
layer is consistent with previous simulations for Dome Fuji. The free
atmosphere seeing is ~0.2'', and the height of the surface boundary layer can
be as low as ~11 m.",1305.5109v1,astro-ph.IM,2013-05-22 12:38:07+00:00,"[arxiv.Result.Author('H. Okita'), arxiv.Result.Author('T. Ichikawa'), arxiv.Result.Author('M. C. B. Ashley'), arxiv.Result.Author('N. Takato')]",
13,Night-time measurements of astronomical seeing at Dome A in Antarctica,"Seeing, the angular size of stellar images blurred by atmospheric turbulence,
is a critical parameter used to assess the quality of astronomical sites.
Median values at the best mid-latitude sites are generally in the range of
0.6--0.8\,arcsec. Sites on the Antarctic plateau are characterized by
comparatively-weak turbulence in the free-atmosphere above a strong but thin
boundary layer. The median seeing at Dome C is estimated to be 0.23--0.36
arcsec above a boundary layer that has a typical height of 30\,m. At Dome A and
F, the only previous seeing measurements were made during daytime. Here we
report the first direct measurements of night-time seeing at Dome A, using a
Differential Image Motion Monitor. Located at a height of just 8\,m, it
recorded seeing as low as 0.13\,arcsec, and provided seeing statistics that are
comparable to those for a 20\,m height at Dome C. It indicates that the
boundary layer was below 8\,m 31\% of the time. At such times the median seeing
was 0.31\,arcsec, consistent with free-atmosphere seeing. The seeing and
boundary layer thickness are found to be strongly correlated with the
near-surface temperature gradient. The correlation confirms a median thickness
of approximately 14\,m for the boundary layer at Dome A, as found from a sonic
radar. The thinner boundary layer makes it less challenging to locate a
telescope above it, thereby giving greater access to the free-atmosphere.",2007.15365v1,astro-ph.IM,2020-07-30 10:36:24+00:00,"[arxiv.Result.Author('Bin Ma'), arxiv.Result.Author('Zhaohui Shang'), arxiv.Result.Author('Yi Hu'), arxiv.Result.Author('Keliang Hu'), arxiv.Result.Author('Yongjiang Wang'), arxiv.Result.Author('Xu Yang'), arxiv.Result.Author('Michael C. B. Ashley'), arxiv.Result.Author('Paul Hickson'), arxiv.Result.Author('Peng Jiang')]","Nature 583, 771-774 (2020)"
14,Gaze-Vergence-Controlled See-Through Vision in Augmented Reality,"Augmented Reality (AR) see-through vision is an interesting research topic
since it enables users to see through a wall and see the occluded objects. Most
existing research focuses on the visual effects of see-through vision, while
the interaction method is less studied. However, we argue that using common
interaction modalities, e.g., midair click and speech, may not be the optimal
way to control see-through vision. This is because when we want to see through
something, it is physically related to our gaze depth/vergence and thus should
be naturally controlled by the eyes. Following this idea, this paper proposes a
novel gaze-vergence-controlled (GVC) see-through vision technique in AR. Since
gaze depth is needed, we build a gaze tracking module with two infrared cameras
and the corresponding algorithm and assemble it into the Microsoft HoloLens 2
to achieve gaze depth estimation. We then propose two different GVC modes for
see-through vision to fit different scenarios. Extensive experimental results
demonstrate that our gaze depth estimation is efficient and accurate. By
comparing with conventional interaction modalities, our GVC techniques are also
shown to be superior in terms of efficiency and more preferred by users.
Finally, we present four example applications of gaze-vergence-controlled
see-through vision.",2207.02645v1,cs.CV,2022-07-06 13:11:34+00:00,"[arxiv.Result.Author('Zhimin Wang'), arxiv.Result.Author('Yuxin Zhao'), arxiv.Result.Author('Feng Lu')]",
15,Vlasov Simulation of Emissive Plasma Sheath with Energy-Dependent Secondary Emission Coefficient and Improved Modeling for Dielectric Charging Effects,"A one dimensional Vlasov Poisson simulation code is employed to investigate
the plasma sheath considering electron induced secondary electron emission
(SEE) and backscattering. The SEE coefficient is commonly treated as constant
in a range of plasma simulations, here improved SEE model of a charged
dielectric wall is constructed which includes the wall charging effect on SEE
coefficient and the energy dependency of SEE coefficient. Pertinent algorithms
to implement above SEE model in plasma simulation are studied in detail. It is
found that the SEE coefficient increases with the amount of negative wall
charges, which in turn reduces the emissive sheath potential. With energy
dependent SEE coefficient, the sheath potential is a nonlinear function of the
plasma electron temperature, as opposed to the linear relation predicted by
classic emissive sheath theory. Simulation combining both wall charging effect
and SEE coefficient energy dependency suggests that the space charged limited
sheath is formed at high plasma electron temperature levels, where both sheath
potential and surface charging saturate. Additionally, different algorithms to
implement the backscattering in kinetic simulation are tested and compared.
Converting backscattered electron to secondary electron via an effective SEE
coefficient barely affects the sheath properties. The simulation results are
shown to be commensurate with the upgraded sheath theory predictions.",2209.09567v1,physics.plasm-ph,2022-09-20 09:13:56+00:00,"[arxiv.Result.Author('Guang-Yu Sun'), arxiv.Result.Author('Shu Zhang'), arxiv.Result.Author('Bao-Hong Guo'), arxiv.Result.Author('An-Bang Sun'), arxiv.Result.Author('Guan-Jun Zhang')]",
16,Leptogenesis and LHC Physics with Type III See-Saw,"The See-Saw mechanism provides a nice way to explain why neutrino masses are
so much lighter than their charged lepton partners. It also provides a nice way
to explain baryon asymmetry in our universe via the leptogenesis mechanism. In
this talk we review leptogenesis and LHC physics in a See-Saw model proposed in
1989, now termed the Type III See-Saw model. In this model, $SU(2)_L$ triplet
leptons are introduced with the neutral particles of the triplets playing the
role of See-Saw. The triplet leptons have charged partners with standard model
gauge interactions resulting in many new features. The gauge interactions of
these particles make it easier for leptognesis with low masses, as low as a TeV
is possible. The gauge interactions also make the production and detection of
triplet leptons at LHC possible. The See-Saw mechanism and leptogenesis due to
Type III See-Saw may be tested at LHC.",0901.1264v2,hep-ph,2009-01-09 16:14:07+00:00,"[arxiv.Result.Author('Shao-Long Chen'), arxiv.Result.Author('Xiao-Gang He')]",
17,Avoiding the gauge heirarchy problem with see-sawed neutrino masses,"We show that the see-saw neutrino mass mechanism can coexist naturally with
an extended gauge symmetry (i.e. without any gauge heirarchy problem) provided
that the gauge symmetry contains gauged lepton number differences. The simplest
such `natural' see-saw models are constructed and their implications for
neutrino anomalies discussed.",hep-ph/0505154v1,hep-ph,2005-05-18 00:16:53+00:00,[arxiv.Result.Author('R. Foot')],Mod.Phys.Lett. A20 (2005) 3035-3044
18,On the F-expanding of Homoclinic class,"We establish a closing property for thin trapped homoclinic classes. Taking
advantage of this property, we proved that if the homoclinic class $H(p)$
admits a dominated splitting $T_{H(p)}M=E\oplus_{<}F$, where $E$ is thin
trapped (see Definition \ref{Def:TP}) and all periodic points homoclinically
related to $p$ are uniformly $F$-expanding at the period (see Definition
\ref{Def:expanding}), then $F$ is expanded (see Definition \ref{Def:TP}).",1710.08487v1,math.DS,2017-10-23 20:06:47+00:00,"[arxiv.Result.Author('Wanlou Wu'), arxiv.Result.Author('Bo Li')]",
19,See-saw Enhancement of Neutrino Mixing due to the Right-handed Phases,"We study the see-saw enhancement mechanism in presence of the right-handed
phases of the Dirac neutrino mass matrix and the Majorana mass matrix. The
enhancement condition given by Smirnov is modified. We point out that the
see-saw enhancement could be obtained due to the right-handed phases even if
the Majorana matrix is proportional to the unit matrix. We show a realistic
Dirac mass matrix which causes the see-saw enhancement.",hep-ph/9503318v1,hep-ph,1995-03-14 01:47:22+00:00,[arxiv.Result.Author('Morimitsu Tanimoto')],Phys.Lett. B345 (1995) 477-482
20,Risk Automatic Prediction for Social Economy Companies using Camels,"Governments have to supervise and inspect social economy enterprises (SEEs).
However, inspecting all SEEs is not possible due to the large number of SEEs
and the low number of inspectors in general. We proposed a prediction model
based on a machine learning approach. The method was trained with the random
forest algorithm with historical data provided by each SEE. Three consecutive
periods of data were concatenated. The proposed method uses these periods as
input data and predicts the risk of each SEE in the fourth period. The model
achieved 76\% overall accuracy. In addition, it obtained good accuracy in
predicting the high risk of a SEE. We found that the legal nature and the
variation of the past-due portfolio are good predictors of the future risk of a
SEE. Thus, the risk of a SEE in a future period can be predicted by a
supervised machine learning method. Predicting the high risk of a SEE improves
the daily work of each inspector by focusing only on high-risk SEEs.",2210.05052v1,cs.LG,2022-10-10 23:50:02+00:00,"[arxiv.Result.Author('Joseph Gallego-Mejia'), arxiv.Result.Author('Daniela Martin-Vega'), arxiv.Result.Author('Fabio Gonzalez')]",
21,Triviality and the (Supersymmetric) See-Saw,"For the D=5 Majorana neutrino mass operator to have a see-saw ultraviolet
completion that is viable up to the Planck scale, the see-saw scale is bounded
above due to triviality limits on the see-saw couplings. For supersymmetric
see-saw models, with realistic neutrino mass textures, we compare constraints
on the see-saw scale from triviality bounds, with those arising from
experimental limits on induced charged-lepton flavour violation, for both the
CMSSM and for models with split supersymmetry.",hep-ph/0603053v2,hep-ph,2006-03-07 16:47:09+00:00,"[arxiv.Result.Author('Bruce A. Campbell'), arxiv.Result.Author('David W. Maybury')]","JHEP 0704:077,2007"
22,Astronomical Seeing at Maidanak Observatory during the year 2018,"Astronomical seeing measurements were carried out at Maidanak observatory
during the period from August to November 2018 using DIMM (Differential Image
Motion Monitor). The median value of seeing for the entire period was
determined as 0.54 arcseconds. This value was compared to the seeing data of
the period 1996-2002.",2012.08110v1,astro-ph.IM,2020-12-15 06:19:17+00:00,"[arxiv.Result.Author('Y. A. Tillayev'), arxiv.Result.Author('A. M. Azimov'), arxiv.Result.Author('A. R. Hafizov')]",
23,"The see-saw mechanism: neutrino mixing, leptogenesis and lepton flavor violation","The see-saw mechanism to generate small neutrino masses is reviewed. After
summarizing our current knowledge about the low energy neutrino mass matrix we
consider reconstructing the see-saw mechanism. Low energy neutrino physics is
not sufficient to reconstruct see-saw, a feature which we refer to as ``see-saw
degeneracy''. Indirect tests of see-saw are leptogenesis and lepton flavor
violation in supersymmetric scenarios, which together with neutrino mass and
mixing define the framework of see-saw phenomenology. Several examples are
given, both phenomenological and GUT-related. Variants of the see-saw mechanism
like the type II or triplet see-saw are also discussed. In particular, we
compare many general aspects regarding the dependence of LFV on low energy
neutrino parameters in the extreme cases of a dominating conventional see-saw
term or a dominating triplet term. For instance, the absence of mu -> e gamma
or tau -> e gamma in the pure triplet case means that CP is conserved in
neutrino oscillations. Scanning models, we also find that among the decays mu
-> e gamma, tau -> e gamma and tau -> mu gamma the latter one has the largest
branching ratio in (i) SO(10) type I see-saw models and in (ii) scenarios in
which the triplet term dominates in the neutrino mass matrix.",0804.3925v3,hep-ph,2008-04-24 13:44:29+00:00,[arxiv.Result.Author('Werner Rodejohann')],"Pramana 72:217-227,2009"
24,Statistics of turbulence profile at Cerro Tololo,"Results of 3-month continuous monitoring of turbulence profile and seeing at
Cerro Tololo (Chile) in May-July 2002 are presented. Some 28000 low-resolution
profiles were measured by a new MASS single-star turbulence monitor,
accompanied by seeing data from DIMM. The median seeing was 0.95 arcseconds.
The first 500 m contribute 60% to the total seeing, the free-atmosphere median
seeing was 0.55 arcseconds. Free-atmosphere seeing is almost never better than
0.15 arcseconds because there is always some turbulence above 12 km. A 4-day
period of calm upper atmosphere with a stable free-atmosphere seeing of 0.2-0.3
arcseconds was noted. A gain in resolution from adaptive compensation of ground
layer will be 1.7 times typically and 2-3 times during such calm periods.
Correlations of the free-atmosphere turbulence with the wind speed at
tropopause and of the ground-layer turbulence with ground wind are studied.
Temporal evolution of turbulence is characterized by recurrent bursts, their
typical duration increases from 15 minutes in low layers to 1-2 hours in high
layers. The large data base of turbulence profiles can be used to test
meso-scale modeling of astronomical seeing.",astro-ph/0209432v1,astro-ph,2002-09-20 16:32:20+00:00,"[arxiv.Result.Author('A. Tokovinin'), arxiv.Result.Author('S. Baumont'), arxiv.Result.Author('J. Vasquez')]",Mon.Not.Roy.Astron.Soc. 340 (2003) 52
25,The Spectrum of HD 3651B: An Extrasolar Nemesis?,This article has been withdrawn; see astro-ph/0611542,astro-ph/0609556v2,astro-ph,2006-09-20 00:36:50+00:00,[arxiv.Result.Author('Adam J. Burgasser')],
26,Criticality versus q in the 2+1-dimensional $Z_q$ clock model,"Paper has been withdrawn, see comment.",cond-mat/0212255v2,cond-mat.supr-con,2002-12-11 17:32:31+00:00,"[arxiv.Result.Author('J. Hove'), arxiv.Result.Author('A. Sudbo')]",
27,Closedness of the Space of AHE Metrics on 4-Manifolds,See comment above.,math/0012167v2,math.DG,2000-12-18 19:05:21+00:00,[arxiv.Result.Author('Michael T. Anderson')],
28,A Machine Learning Approach to Correcting Atmospheric Seeing in Solar Flare Observations,"Current post-processing techniques for the correction of atmospheric seeing
in solar observations -- such as Speckle interferometry and Phase Diversity
methods -- have limitations when it comes to their reconstructive capabilities
of solar flare observations. This, combined with the sporadic nature of flares
meaning observers cannot wait until seeing conditions are optimal before taking
measurements, means that many ground-based solar flare observations are marred
with bad seeing. To combat this, we propose a method for dedicated flare seeing
correction based on training a deep neural network to learn to correct
artificial seeing from flare observations taken during good seeing conditions.
This model uses transfer learning, a novel technique in solar physics, to help
learn these corrections. Transfer learning is when another network already
trained on similar data is used to influence the learning of the new network.
Once trained, the model has been applied to two flare datasets: one from
AR12157 on 2014/09/06 and one from AR12673 on 2017/09/06. The results show good
corrections to images with bad seeing with a relative error assigned to the
estimate based on the performance of the model. Further discussion takes place
of improvements to the robustness of the error on these estimates.",2011.12814v1,astro-ph.SR,2020-11-25 15:17:26+00:00,"[arxiv.Result.Author('John A. Armstrong'), arxiv.Result.Author('Lyndsay Fletcher')]",
29,Daytime Seeing and Solar Limb Positions,"A method to measure the seeing from video made during drift-scan solar
transits is proposed. The limb of the Sun is projected over a regular grid
evenly spaced. The temporal dispersion of the time intervals among the contacts
between solar limb and grid's rows is proportional to the atmospheric seeing.
Seeing effects on the position of the inflexion point of the limb's luminosity
profile are calculated numerically with Fast Fourier Transform. Observational
examples from Locarno and Paris Observatories are presented to show the
asymmetric contributions of the seeing at the beginning and the end of each
drift-scan transit.",1106.2539v1,astro-ph.IM,2011-06-13 19:58:47+00:00,[arxiv.Result.Author('Costantino Sigismondi')],
30,The effect of aperture and seeing on the visibility of sunspots when using early modern and modern telescopes of modest size,"Using the convolution of seeing and diffraction, the relation between seeing
and aperture in the visibility of sunspots is explored. It is shown that even
telescopes with apertures smaller than 5 centimetres are significantly affected
by seeing. Although larger aperture instruments suffer more from seeing than
smaller ones, their level of detail always remains better under the same
atmospheric conditions.",2208.07244v3,astro-ph.SR,2022-08-15 15:00:51+00:00,"[arxiv.Result.Author('Nicolàs de Hilster'), arxiv.Result.Author('Siebren van der Werf')]",
31,Typical duration of good seeing sequences at Concordia,"Context: The winter seeing at Concordia is essentially bimodal, excellent or
quite poor, with relative proportions that depend on altitude above the snow
surface. This paper studies the temporal behavior of the good seeing sequences.
Aims: An efficient exploitation of extremely good seeing with an adaptive
optics system needs long integrations. It is then important to explore the
temporal distribution of the fraction of time providing excellent seeing.
Methods: Temporal windows of good seeing are created by a simple binary
process. Good or bad. Their autocorrelations are corrected for those of the
existing data sets, since these are not continuous, being often interrupted by
technical problems in addition to the adverse weather gaps. At the end these
corrected autocorrelations provide the typical duration of good seeing
sequences. This study has to be a little detailed as its results depend on the
season, summer or winter. Results: Using a threshold of 0.5 arcsec to define
the ""good seeing"", three characteristic numbers are found to describe the
temporal evolution of the good seeing windows. The first number is the mean
duration of an uninterrupted good seeing sequence: it is $\tau_0=7.5$ hours at
8 m above the ground (15 hours at 20 m). These sequences are randomly
distributed in time, with a negative exponential law of damping time
$\tau_1=29$ hours (at elevation 8 m and 20 m). The third number is the mean
time between two 29 hours episodes. It is T=10 days at 8 m high (5 days at 20
m).",1003.3583v1,astro-ph.IM,2010-03-18 14:09:06+00:00,"[arxiv.Result.Author('Eric Fossat'), arxiv.Result.Author('Eric Aristidi'), arxiv.Result.Author('Karim Agabi'), arxiv.Result.Author('Erick Bondoux'), arxiv.Result.Author('Zalpha Challita'), arxiv.Result.Author('Francois Jeanneaux'), arxiv.Result.Author('Djamel Mekarnia')]",
32,Excellent daytime seeing at Dome Fuji on the Antarctic plateau,"Context. Dome Fuji, the second highest region on the Antarctic plateau, is
expected to have some of the best astronomical seeing on Earth. However, site
testing at Dome Fuji is still in its very early stages.
  Aims. To investigate the astronomical seeing in the free atmosphere above
Dome Fuji, and to determine the height of the surface boundary layer.
  Methods. A Differential Image Motion Monitor was used to measure the seeing
in the visible (472 nm) at a height of 11 m above the snow surface at Dome Fuji
during the austral summer of 2012/2013.
  Results. Seeing below 0.2'' has been observed. The seeing often has a local
minimum of ~0.3'' near 18 h local time. Some periods of excellent seeing, 0.3''
or smaller, were also observed, sometimes extending for several hours at local
midnight. The median seeing is higher, at 0.52''---this large value is believed
to be caused by periods when the telescope was within the turbulent boundary
layer.
  Conclusions. The diurnal variation of the daytime seeing at Dome Fuji is
similar to that reported for Dome C, and the height of the surface boundary
layer is consistent with previous simulations for Dome Fuji. The free
atmosphere seeing is ~0.2'', and the height of the surface boundary layer can
be as low as ~11 m.",1305.5109v1,astro-ph.IM,2013-05-22 12:38:07+00:00,"[arxiv.Result.Author('H. Okita'), arxiv.Result.Author('T. Ichikawa'), arxiv.Result.Author('M. C. B. Ashley'), arxiv.Result.Author('N. Takato')]",
33,Night-time measurements of astronomical seeing at Dome A in Antarctica,"Seeing, the angular size of stellar images blurred by atmospheric turbulence,
is a critical parameter used to assess the quality of astronomical sites.
Median values at the best mid-latitude sites are generally in the range of
0.6--0.8\,arcsec. Sites on the Antarctic plateau are characterized by
comparatively-weak turbulence in the free-atmosphere above a strong but thin
boundary layer. The median seeing at Dome C is estimated to be 0.23--0.36
arcsec above a boundary layer that has a typical height of 30\,m. At Dome A and
F, the only previous seeing measurements were made during daytime. Here we
report the first direct measurements of night-time seeing at Dome A, using a
Differential Image Motion Monitor. Located at a height of just 8\,m, it
recorded seeing as low as 0.13\,arcsec, and provided seeing statistics that are
comparable to those for a 20\,m height at Dome C. It indicates that the
boundary layer was below 8\,m 31\% of the time. At such times the median seeing
was 0.31\,arcsec, consistent with free-atmosphere seeing. The seeing and
boundary layer thickness are found to be strongly correlated with the
near-surface temperature gradient. The correlation confirms a median thickness
of approximately 14\,m for the boundary layer at Dome A, as found from a sonic
radar. The thinner boundary layer makes it less challenging to locate a
telescope above it, thereby giving greater access to the free-atmosphere.",2007.15365v1,astro-ph.IM,2020-07-30 10:36:24+00:00,"[arxiv.Result.Author('Bin Ma'), arxiv.Result.Author('Zhaohui Shang'), arxiv.Result.Author('Yi Hu'), arxiv.Result.Author('Keliang Hu'), arxiv.Result.Author('Yongjiang Wang'), arxiv.Result.Author('Xu Yang'), arxiv.Result.Author('Michael C. B. Ashley'), arxiv.Result.Author('Paul Hickson'), arxiv.Result.Author('Peng Jiang')]","Nature 583, 771-774 (2020)"
34,Gaze-Vergence-Controlled See-Through Vision in Augmented Reality,"Augmented Reality (AR) see-through vision is an interesting research topic
since it enables users to see through a wall and see the occluded objects. Most
existing research focuses on the visual effects of see-through vision, while
the interaction method is less studied. However, we argue that using common
interaction modalities, e.g., midair click and speech, may not be the optimal
way to control see-through vision. This is because when we want to see through
something, it is physically related to our gaze depth/vergence and thus should
be naturally controlled by the eyes. Following this idea, this paper proposes a
novel gaze-vergence-controlled (GVC) see-through vision technique in AR. Since
gaze depth is needed, we build a gaze tracking module with two infrared cameras
and the corresponding algorithm and assemble it into the Microsoft HoloLens 2
to achieve gaze depth estimation. We then propose two different GVC modes for
see-through vision to fit different scenarios. Extensive experimental results
demonstrate that our gaze depth estimation is efficient and accurate. By
comparing with conventional interaction modalities, our GVC techniques are also
shown to be superior in terms of efficiency and more preferred by users.
Finally, we present four example applications of gaze-vergence-controlled
see-through vision.",2207.02645v1,cs.CV,2022-07-06 13:11:34+00:00,"[arxiv.Result.Author('Zhimin Wang'), arxiv.Result.Author('Yuxin Zhao'), arxiv.Result.Author('Feng Lu')]",
35,Vlasov Simulation of Emissive Plasma Sheath with Energy-Dependent Secondary Emission Coefficient and Improved Modeling for Dielectric Charging Effects,"A one dimensional Vlasov Poisson simulation code is employed to investigate
the plasma sheath considering electron induced secondary electron emission
(SEE) and backscattering. The SEE coefficient is commonly treated as constant
in a range of plasma simulations, here improved SEE model of a charged
dielectric wall is constructed which includes the wall charging effect on SEE
coefficient and the energy dependency of SEE coefficient. Pertinent algorithms
to implement above SEE model in plasma simulation are studied in detail. It is
found that the SEE coefficient increases with the amount of negative wall
charges, which in turn reduces the emissive sheath potential. With energy
dependent SEE coefficient, the sheath potential is a nonlinear function of the
plasma electron temperature, as opposed to the linear relation predicted by
classic emissive sheath theory. Simulation combining both wall charging effect
and SEE coefficient energy dependency suggests that the space charged limited
sheath is formed at high plasma electron temperature levels, where both sheath
potential and surface charging saturate. Additionally, different algorithms to
implement the backscattering in kinetic simulation are tested and compared.
Converting backscattered electron to secondary electron via an effective SEE
coefficient barely affects the sheath properties. The simulation results are
shown to be commensurate with the upgraded sheath theory predictions.",2209.09567v1,physics.plasm-ph,2022-09-20 09:13:56+00:00,"[arxiv.Result.Author('Guang-Yu Sun'), arxiv.Result.Author('Shu Zhang'), arxiv.Result.Author('Bao-Hong Guo'), arxiv.Result.Author('An-Bang Sun'), arxiv.Result.Author('Guan-Jun Zhang')]",
36,Leptogenesis and LHC Physics with Type III See-Saw,"The See-Saw mechanism provides a nice way to explain why neutrino masses are
so much lighter than their charged lepton partners. It also provides a nice way
to explain baryon asymmetry in our universe via the leptogenesis mechanism. In
this talk we review leptogenesis and LHC physics in a See-Saw model proposed in
1989, now termed the Type III See-Saw model. In this model, $SU(2)_L$ triplet
leptons are introduced with the neutral particles of the triplets playing the
role of See-Saw. The triplet leptons have charged partners with standard model
gauge interactions resulting in many new features. The gauge interactions of
these particles make it easier for leptognesis with low masses, as low as a TeV
is possible. The gauge interactions also make the production and detection of
triplet leptons at LHC possible. The See-Saw mechanism and leptogenesis due to
Type III See-Saw may be tested at LHC.",0901.1264v2,hep-ph,2009-01-09 16:14:07+00:00,"[arxiv.Result.Author('Shao-Long Chen'), arxiv.Result.Author('Xiao-Gang He')]",
37,Avoiding the gauge heirarchy problem with see-sawed neutrino masses,"We show that the see-saw neutrino mass mechanism can coexist naturally with
an extended gauge symmetry (i.e. without any gauge heirarchy problem) provided
that the gauge symmetry contains gauged lepton number differences. The simplest
such `natural' see-saw models are constructed and their implications for
neutrino anomalies discussed.",hep-ph/0505154v1,hep-ph,2005-05-18 00:16:53+00:00,[arxiv.Result.Author('R. Foot')],Mod.Phys.Lett. A20 (2005) 3035-3044
38,On the F-expanding of Homoclinic class,"We establish a closing property for thin trapped homoclinic classes. Taking
advantage of this property, we proved that if the homoclinic class $H(p)$
admits a dominated splitting $T_{H(p)}M=E\oplus_{<}F$, where $E$ is thin
trapped (see Definition \ref{Def:TP}) and all periodic points homoclinically
related to $p$ are uniformly $F$-expanding at the period (see Definition
\ref{Def:expanding}), then $F$ is expanded (see Definition \ref{Def:TP}).",1710.08487v1,math.DS,2017-10-23 20:06:47+00:00,"[arxiv.Result.Author('Wanlou Wu'), arxiv.Result.Author('Bo Li')]",
39,See-saw Enhancement of Neutrino Mixing due to the Right-handed Phases,"We study the see-saw enhancement mechanism in presence of the right-handed
phases of the Dirac neutrino mass matrix and the Majorana mass matrix. The
enhancement condition given by Smirnov is modified. We point out that the
see-saw enhancement could be obtained due to the right-handed phases even if
the Majorana matrix is proportional to the unit matrix. We show a realistic
Dirac mass matrix which causes the see-saw enhancement.",hep-ph/9503318v1,hep-ph,1995-03-14 01:47:22+00:00,[arxiv.Result.Author('Morimitsu Tanimoto')],Phys.Lett. B345 (1995) 477-482
40,"Electryo, In-person Voting with Transparent Voter Verifiability and Eligibility Verifiability","Selene is an e-voting protocol that allows voters to directly check their
individual vote, in cleartext, in the final tally via a tracker system, while
providing good coercion mitigation. This is in contrast to conventional,
end-to-end verifiable schemes in which the voter verifies the presence of an
encryption of her vote on the bulletin board. The Selene mechanism can be
applied to many e-voting schemes, but here we present an application to the
polling station context, resulting in a voter-verifiable electronic tally with
a paper audit trail. The system uses a smartcard-based public key system to
provide the individual verification and universal eligibility verifiability.
The paper record contains an encrypted link to the voter's identity, requiring
stronger assumptions on ballot privacy than normal paper voting, but with the
benefit of providing good auditability and dispute resolution as well as
supporting (comparison) risk limiting audits.",2105.14783v1,cs.CR,2021-05-31 08:22:29+00:00,"[arxiv.Result.Author('Peter B. Roenne'), arxiv.Result.Author('Peter Y. A Ryan'), arxiv.Result.Author('Marie-Laure Zollinger')]",
41,UC Modelling and Security Analysis of the Estonian IVXV Internet Voting System,"Estonian Internet voting has been used in national-wide elections since 2005.
However, the system was initially designed in a heuristic manner, with very few
proven security guarantees. The Estonian Internet voting system has constantly
been evolving throughout the years, with the latest version (code-named IVXV)
implemented in 2018. Nevertheless, to date, no formal security analysis of the
system has been given. In this work, for the first time, we provide a rigorous
security modeling for the Estonian IVXV system as a ceremony, attempting to
capture the effect of actual human behavior on election verifiability in the
universal composability (UC) framework. Based on the voter behavior statistics
collected from three actual election events in Estonia, we show that IVXV
achieves end-to-end verifiability in practice despite the fact that only $4\%$
(on average) of the Estonian voters audit their ballots.",2109.01994v1,cs.CR,2021-09-05 05:04:14+00:00,"[arxiv.Result.Author('Bingsheng Zhang'), arxiv.Result.Author('Zengpeng Li'), arxiv.Result.Author('Jan Willemson')]",
42,Boardroom Voting: Verifiable Voting with Ballot Privacy Using Low-Tech Cryptography in a Single Room,"A boardroom election is an election that takes place in a single room -- the
boardroom -- in which all voters can see and hear each other. We present an
initial exploration of boardroom elections with ballot privacy and voter
verifiability that use only ""low-tech cryptography"" without using computers to
mark or collect ballots. Specifically, we define the problem, introduce several
building blocks, and propose a new protocol that combines these blocks in novel
ways. Our new building blocks include ""foldable ballots"" that can be rotated to
hide the alignment of ballot choices with voting marks, and ""visual secrets""
that are easy to remember and use but hard to describe. Although closely seated
participants in a boardroom election have limited privacy, the protocol ensures
that no one can determine how others voted. Moreover, each voter can verify
that their ballot was correctly cast, collected, and counted, without being
able to prove how they voted, providing assurance against undue influence.
Low-tech cryptography is useful in situations where constituents do not trust
computer technology, and it avoids the complex auditing requirements of
end-to-end cryptographic voting systems such as Pr\^{e}t-\`{a}-Voter. This
paper's building blocks and protocol are meant to be a proof of concept that
might be tested for usability and improved.",2007.14916v2,cs.CR,2020-07-29 15:40:51+00:00,"[arxiv.Result.Author('Enka Blanchard'), arxiv.Result.Author('Ted Selker'), arxiv.Result.Author('Alan T. Sherman')]",
43,Auditing for Core Stability in Participatory Budgeting,"We consider the participatory budgeting problem where each of $n$ voters
specifies additive utilities over $m$ candidate projects with given sizes, and
the goal is to choose a subset of projects (i.e., a committee) with total size
at most $k$. Participatory budgeting mathematically generalizes multiwinner
elections, and both have received great attention in computational social
choice recently. A well-studied notion of group fairness in this setting is
core stability: Each voter is assigned an ""entitlement"" of $\frac{k}{n}$, so
that a subset $S$ of voters can pay for a committee of size at most $|S| \cdot
\frac{k}{n}$. A given committee is in the core if no subset of voters can pay
for another committee that provides each of them strictly larger utility. This
provides proportional representation to all voters in a strong sense.
  In this paper, we study the following auditing question: Given a committee
computed by some preference aggregation method, how close is it to the core?
Concretely, how much does the entitlement of each voter need to be scaled down
by, so that the core property subsequently holds? As our main contribution, we
present computational hardness results for this problem, as well as a
logarithmic approximation algorithm via linear program rounding. We show that
our analysis is tight against the linear programming bound. Additionally, we
consider two related notions of group fairness that have similar audit
properties. The first is Lindahl priceability, which audits the closeness of a
committee to a market clearing solution. We show that this is related to the
linear programming relaxation of auditing the core, leading to efficient exact
and approximation algorithms for auditing. The second is a novel weakening of
the core that we term the sub-core, and we present computational results for
auditing this notion as well.",2209.14468v1,cs.GT,2022-09-28 23:13:06+00:00,"[arxiv.Result.Author('Kamesh Munagala'), arxiv.Result.Author('Yiheng Shen'), arxiv.Result.Author('Kangning Wang')]",
44,A note on efficient audit sample selection,"Auditing is a widely used method for quality improvement, and many guidelines
are available advising on how to draw samples for auditing. However,
researchers or auditors sometimes find themselves in situations that are not
straightforward and the standard sampling techniques are not sufficient, for
example when a selective sample has initially been audited and the auditor
desires to re-use as many cases as possible from this initial audit in a new
audit sample that is representative with respect to some background
characteristics. In this paper, we introduce a method that selects an audit
sample that re-uses initially audited cases by considering the selection of a
representative audit sample as a constrained minimization problem. In addition,
we evaluate the performance of this method by means of a simulation study and
we apply the method to draw an audit sample of establishments to evaluate the
quality of an establishment registry used to produce statistics on energy
consumption per type of economic activity.",2105.10737v1,stat.ME,2021-05-22 14:23:36+00:00,"[arxiv.Result.Author('Laura Boeschoten'), arxiv.Result.Author('Sander Scholtus'), arxiv.Result.Author('Arnout van Delden')]",
45,Auditable Register Emulations,"The widespread prevalence of data breaches amplifies the importance of
auditing storage systems. In this work, we initiate the study of auditable
storage emulations, which provide the capability for an auditor to report the
previously executed reads in a register. We precisely define the notion of
auditable register and its properties, and establish tight bounds and
impossibility results for auditable storage emulations in the presence of
faulty storage objects. Our formulation considers loggable read-write registers
that securely store data using information dispersal and support fast reads. In
such a scenario, given a maximum number~$f$ of faulty storage objects and a
minimum number~$\tau$ of data blocks required to recover a stored value, we
prove that (1) auditability is impossible if $\tau \leq 2f $; (2) implementing
a weak form of auditability requires $\tau \geq 3f+1$; and (3) a stronger form
of auditability is impossible. We also show that signing read requests
overcomes the lower bound of weak auditability, while totally ordering
operations or using non-fast reads enables strong auditability.",1905.08637v2,cs.DC,2019-05-21 13:44:22+00:00,"[arxiv.Result.Author('Vinicius V. Cogo'), arxiv.Result.Author('Alysson Bessani')]",
46,Electt: running auditable and verifiable elections in untrusted environments,"We present a system for running auditable and verifiable elections in
untrusted environments. Votes are anonymous since the order of candidates on a
ballot sheet is random. Tellers see only the position of the candidate. Voters
can check their vote. An election is auditable using blockchain log.
Threshold-encryption, which is used to implement the quorum, prevents a
deadlock from occurring if a minority of candidates or observers tries to
sabotage the election. Candidates and observers can indicate that the election
was free and fair by exposing their keys, which are used by the system to
decrypt each vote. Ballot sheets are encrypted by onion-routing, which has a
layer with the key of the election instance, so it's impossible for a quorum to
decode the results before they have announced their decision by exposing their
keys. A register of voters ensures that only verified voters can vote without
compromising their identity. If there any doubts about the identity of a voter,
their vote can be excluded from the election, if a quorum agrees. This system
is designed to scale from one instance to a distributed system that runs over
an unlimited number of instances, which can be achieved using cloud instances
or smartphones belonging to voters or tellers.",2011.10902v2,cs.CR,2020-11-22 00:58:34+00:00,[arxiv.Result.Author('Kirill A. Korinsky')],
47,On the security of ballot marking devices,"A recent debate among election experts has considered whether electronic
ballot marking devices (BMDs) have adequate security against the risks of
malware. A malicious BMD might produce a printed ballot that disagrees with a
voter's actual intent, with the hope that voters would be unlikely to detect
this subterfuge. This essay considers how an election administrator can create
reasonable auditing procedures to gain confidence that their fleet of BMDs is
operating correctly, allowing voters to benefit from the usability and
accessibility features of BMDs while the overall election still benefits from
the same security and reliability properties we expect from hand-marked paper
ballots.",1908.01897v2,cs.CR,2019-08-05 23:04:16+00:00,[arxiv.Result.Author('Dan S. Wallach')],
48,BVOT: Self-Tallying Boardroom Voting with Oblivious Transfer,"A boardroom election is an election with a small number of voters carried out
with public communications. We present BVOT, a self-tallying boardroom voting
protocol with ballot secrecy, fairness (no tally information is available
before the polls close), and dispute-freeness (voters can observe that all
voters correctly followed the protocol).
  BVOT works by using a multiparty threshold homomorphic encryption system in
which each candidate is associated with a masked unique prime. Each voter
engages in an oblivious transfer with an untrusted distributor: the voter
selects the index of a prime associated with a candidate and receives the
selected prime in masked form. The voter then casts their vote by encrypting
their masked prime and broadcasting it to everyone. The distributor does not
learn the voter's choice, and no one learns the mapping between primes and
candidates until the audit phase. By hiding the mapping between primes and
candidates, BVOT provides voters with insufficient information to carry out
effective cheating. The threshold feature prevents anyone from computing any
partial tally---until everyone has voted. Multiplying all votes, their
decryption shares, and the unmasking factor yields a product of the primes each
raised to the number of votes received.
  In contrast to some existing boardroom voting protocols, BVOT does not rely
on any zero-knowledge proof; instead, it uses oblivious transfer to assure
ballot secrecy and correct vote casting. Also, BVOT can handle multiple
candidates in one election. BVOT prevents cheating by hiding crucial
information: an attempt to increase the tally of one candidate might increase
the tally of another candidate. After all votes are cast, any party can tally
the votes.",2010.02421v1,cs.CR,2020-10-06 01:28:34+00:00,"[arxiv.Result.Author('Farid Javani'), arxiv.Result.Author('Alan T. Sherman')]",
49,Risk-Limiting Tallies,"Many voter-verifiable, coercion-resistant schemes have been proposed, but
even the most carefully designed systems necessarily leak information via the
announced result. In corner cases, this may be problematic. For example, if all
the votes go to one candidate then all vote privacy evaporates. The mere
possibility of candidates getting no or few votes could have implications for
security in practice: if a coercer demands that a voter cast a vote for such an
unpopular candidate, then the voter may feel obliged to obey, even if she is
confident that the voting system satisfies the standard coercion resistance
definitions. With complex ballots, there may also be a danger of ""Italian""
style (aka ""signature"") attacks: the coercer demands the voter cast a ballot
with a specific, identifying pattern. Here we propose an approach to tallying
end-to-end verifiable schemes that avoids revealing all the votes but still
achieves whatever confidence level in the announced result is desired. Now a
coerced voter can claim that the required vote must be amongst those that
remained shrouded. Our approach is based on the well-established notion of
Risk-Limiting Audits, but here applied to the tally rather than to the audit.
We show that this approach counters coercion threats arising in extreme tallies
and ""Italian"" attacks. We illustrate our approach by applying it to the Selene
scheme, and we extend the approach to Risk-Limiting Verification, where not all
vote trackers are revealed, thereby enhancing the coercion mitigation
properties of Selene.",1908.04947v1,cs.CR,2019-08-14 04:23:10+00:00,"[arxiv.Result.Author('Wojciech Jamroga'), arxiv.Result.Author('Peter B. Roenne'), arxiv.Result.Author('Peter Y. A. Ryan'), arxiv.Result.Author('Philip B. Stark')]",
50,Auditing Indian Elections,"Indian Electronic Voting Machines (EVMs) will be fitted with printers that
produce Voter-Verifiable Paper Audit Trails (VVPATs) in time for the 2019
general election. VVPATs provide evidence that each vote was recorded as the
voter intended, without having to trust the perfection or security of the EVMs.
  However, confidence in election results requires more: VVPATs must be
preserved inviolate and then actually used to check the reported election
result in a trustworthy way that the public can verify. A full manual tally
from the VVPATs could be prohibitively expensive and time-consuming; moreover,
it is difficult for the public to determine whether a full hand count was
conducted accurately. We show how Risk-Limiting Audits (RLAs) could provide
high confidence in Indian election results. Compared to full hand recounts,
RLAs typically require manually inspecting far fewer VVPATs when the outcome is
correct, and are much easier for the electorate to observe in adequate detail
to determine whether the result is trustworthy.",1901.03108v2,cs.CR,2019-01-10 11:38:41+00:00,"[arxiv.Result.Author('Vishal Mohanty'), arxiv.Result.Author('Nicholas Akinyokun'), arxiv.Result.Author('Andrew Conway'), arxiv.Result.Author('Chris Culnane'), arxiv.Result.Author('Philip B. Stark'), arxiv.Result.Author('Vanessa Teague')]",
51,Voter Perceptions of Trust in Risk-Limiting Audits,"Risk-limiting audits (RLAs) are expected to strengthen the public confidence
in the correctness of an election outcome. We hypothesize that this is not
always the case, in part because for large margins between the winner and the
runner-up, the number of ballots to be drawn can be so small that voters lose
confidence. We conduct a user study with 105 participants resident in the US.
Our findings confirm the hypothesis, showing that our study participants felt
less confident when they were told the number of ballots audited for RLAs. We
elaborate on our findings and propose recommendations for future use of RLAs.",2109.07918v1,cs.CY,2021-09-15 07:36:17+00:00,"[arxiv.Result.Author('Asmita Dalela'), arxiv.Result.Author('Oksana Kulyk'), arxiv.Result.Author('Carsten Schürmann')]",
52,Combinatorial Voter Control in Elections,"Voter control problems model situations such as an external agent trying to
affect the result of an election by adding voters, for example by convincing
some voters to vote who would otherwise not attend the election. Traditionally,
voters are added one at a time, with the goal of making a distinguished
alternative win by adding a minimum number of voters. In this paper, we
initiate the study of combinatorial variants of control by adding voters: In
our setting, when we choose to add a voter~$v$, we also have to add a whole
bundle $\kappa(v)$ of voters associated with $v$. We study the computational
complexity of this problem for two of the most basic voting rules, namely the
Plurality rule and the Condorcet rule.",1406.6859v1,cs.MA,2014-06-26 11:55:44+00:00,"[arxiv.Result.Author('Laurent Bulteau'), arxiv.Result.Author('Jiehua Chen'), arxiv.Result.Author('Piotr Faliszewski'), arxiv.Result.Author('Rolf Niedermeier'), arxiv.Result.Author('Nimrod Talmon')]",
53,Ballot-Polling Audits of Instant-Runoff Voting Elections with a Dirichlet-Tree Model,"Instant-runoff voting (IRV) is used in several countries around the world. It
requires voters to rank candidates in order of preference, and uses a counting
algorithm that is more complex than systems such as first-past-the-post or
scoring rules. An even more complex system, the single transferable vote (STV),
is used when multiple candidates need to be elected. The complexity of these
systems has made it difficult to audit the election outcomes. There is
currently no known risk-limiting audit (RLA) method for STV, other than a full
manual count of the ballots.
  A new approach to auditing these systems was recently proposed, based on a
Dirichlet-tree model. We present a detailed analysis of this approach for
ballot-polling Bayesian audits of IRV elections. We compared several choices
for the prior distribution, including some approaches using a Bayesian
bootstrap (equivalent to an improper prior). Our findings include that the
bootstrap-based approaches can be adapted to perform similarly to a full
Bayesian model in practice, and that an overly informative prior can give
counter-intuitive results. Via carefully chosen examples, we show why creating
an RLA with this model is challenging, but we also suggest ways to overcome
this.
  As well as providing a practical and computationally feasible implementation
of a Bayesian IRV audit, our work is important in laying the foundation for an
RLA for STV elections.",2209.03881v1,stat.AP,2022-09-08 15:35:50+00:00,"[arxiv.Result.Author('Floyd Everest'), arxiv.Result.Author('Michelle Blom'), arxiv.Result.Author('Philip B. Stark'), arxiv.Result.Author('Peter J. Stuckey'), arxiv.Result.Author('Vanessa Teague'), arxiv.Result.Author('Damjan Vukcevic')]",
54,Implementing Risk-Limiting Post-Election Audits in California,"Risk-limiting post-election audits limit the chance of certifying an
electoral outcome if the outcome is not what a full hand count would show.
Building on previous work, we report on pilot risk-limiting audits in four
elections during 2008 in three California counties: one during the February
2008 Primary Election in Marin County and three during the November 2008
General Elections in Marin, Santa Cruz and Yolo Counties. We explain what makes
an audit risk-limiting and how existing and proposed laws fall short. We
discuss the differences among our four pilot audits. We identify challenges to
practical, efficient risk-limiting audits and conclude that current approaches
are too complex to be used routinely on a large scale. One important logistical
bottleneck is the difficulty of exporting data from commercial election
management systems in a format amenable to audit calculations. Finally, we
propose a bare-bones risk-limiting audit that is less efficient than these
pilot audits, but avoids many practical problems.",0905.4691v4,stat.AP,2009-05-28 16:15:36+00:00,"[arxiv.Result.Author('Joseph Lorenzo Hall'), arxiv.Result.Author('Luke W. Miratrix'), arxiv.Result.Author('Philip B. Stark'), arxiv.Result.Author('Melvin Briones'), arxiv.Result.Author('Elaine Ginnold'), arxiv.Result.Author('Freddie Oakley'), arxiv.Result.Author('Martin Peaden'), arxiv.Result.Author('Gail Pellerin'), arxiv.Result.Author('Tom Stanionis'), arxiv.Result.Author('Tricia Webber')]",
55,Learning Sampling in Financial Statement Audits using Vector Quantised Autoencoder Neural Networks,"The audit of financial statements is designed to collect reasonable assurance
that an issued statement is free from material misstatement 'true and fair
presentation'. International audit standards require the assessment of a
statements' underlying accounting relevant transactions referred to as 'journal
entries' to detect potential misstatements. To efficiently audit the increasing
quantities of such entries, auditors regularly conduct a sample-based
assessment referred to as 'audit sampling'. However, the task of audit sampling
is often conducted early in the overall audit process. Often at a stage, in
which an auditor might be unaware of all generative factors and their dynamics
that resulted in the journal entries in-scope of the audit. To overcome this
challenge, we propose the application of Vector Quantised-Variational
Autoencoder (VQ-VAE) neural networks. We demonstrate, based on two real-world
city payment datasets, that such artificial neural networks are capable of
learning a quantised representation of accounting data. We show that the
learned quantisation uncovers (i) the latent factors of variation and (ii) can
be utilised as a highly representative audit sample in financial statement
audits.",2008.02528v1,cs.LG,2020-08-06 09:02:02+00:00,"[arxiv.Result.Author('Marco Schreyer'), arxiv.Result.Author('Timur Sattarov'), arxiv.Result.Author('Anita Gierbl'), arxiv.Result.Author('Bernd Reimer'), arxiv.Result.Author('Damian Borth')]",
56,SOBA: Secrecy-preserving Observable Ballot-level Audit,"SOBA is an approach to election verification that provides observers with
justifiably high confidence that the reported results of an election are
consistent with an audit trail (""ballots""), which can be paper or electronic.
SOBA combines three ideas: (1) publishing cast vote records (CVRs) separately
for each contest, so that anyone can verify that each reported contest outcome
is correct, if the CVRs reflect voters' intentions with sufficient accuracy;
(2) shrouding a mapping between ballots and the CVRs for those ballots to
prevent the loss of privacy that could occur otherwise; (3) assessing the
accuracy with which the CVRs reflect voters' intentions for a collection of
contests while simultaneously assessing the integrity of the shrouded mapping
between ballots and CVRs by comparing randomly selected ballots to the CVRs
that purport to represent them. Step (1) is related to work by the Humboldt
County Election Transparency Project, but publishing CVRs separately for
individual contests rather than images of entire ballots preserves privacy.
Step (2) requires a cryptographic commitment from elections officials.
Observers participate in step (3), which relies on the ""super-simple
simultaneous single-ballot risk-limiting audit."" Step (3) is designed to reveal
relatively few ballots if the shrouded mapping is proper and the CVRs
accurately reflect voter intent. But if the reported outcomes of the contests
differ from the outcomes that a full hand count would show, step (3) is
guaranteed to have a large chance of requiring all the ballots to be counted by
hand, thereby limiting the risk that an incorrect outcome will become official
and final.",1105.5803v2,stat.AP,2011-05-29 17:00:20+00:00,"[arxiv.Result.Author('Josh Benaloh'), arxiv.Result.Author('Douglas Jones'), arxiv.Result.Author('Eric Lazarus'), arxiv.Result.Author('Mark Lindeman'), arxiv.Result.Author('Philip B. Stark')]",
57,An AI-Powered VVPAT Counter for Elections in India,"The Election Commission of India has introduced Voter Verified Paper Audit
Trail since 2019. This mechanism has increased voter confidence at the time of
casting the votes. However, physical verification of the VVPATs against the
party level counts from the EVMs is done only in 5 (randomly selected) machines
per constituency. The time required to conduct physical verification becomes a
bottleneck in scaling this activity for 100% of machines in all constituencies.
We proposed an automated counter powered by image processing and machine
learning algorithms to speed up the process and address this issue.",2212.11124v1,cs.CV,2022-12-09 14:59:40+00:00,"[arxiv.Result.Author('Prasath Murugesan'), arxiv.Result.Author('Shamshu Dharwez Saganvali')]",
58,Voter model under stochastic resetting,"The voter model is a toy model of consensus formation based on
nearest-neighbour interactions. Each voter is endowed with a Boolean variable
(a binary opinion) that flips randomly at a rate set according to the opinions
of the nearest neighbours. In this paper we subject the system to local
resetting by considering stubborn voters. In addition to the usual dynamics,
voters revert independently to their initial opinion according to a Poisson
process of fixed intensity. The resulting kinetic equations for the average
magnetization and two-point function of the model are derived and solved in
closed form. They are formally identical to the heat equation coupled to a
thermostat, whose temperature profile is induced by the initial conditions. In
the case of initial conditions consisting of a single decided voter at the
origin in a environment full of undecided voters, the average magnetization
evolves as the probability density of a random walker with stochastic resetting
to the origin. However, the long-time behaviour of the two-point function
contains terms terms that cannot be obtained from a renewal argument. As a
result, the steady-state density of domain walls can be minimized as a function
of the resetting rate.",2207.08590v2,cond-mat.stat-mech,2022-07-18 13:30:53+00:00,[arxiv.Result.Author('Pascal Grange')],
59,Continual Learning for Unsupervised Anomaly Detection in Continuous Auditing of Financial Accounting Data,"International audit standards require the direct assessment of a financial
statement's underlying accounting journal entries. Driven by advances in
artificial intelligence, deep-learning inspired audit techniques emerged to
examine vast quantities of journal entry data. However, in regular audits, most
of the proposed methods are applied to learn from a comparably stationary
journal entry population, e.g., of a financial quarter or year. Ignoring
situations where audit relevant distribution changes are not evident in the
training data or become incrementally available over time. In contrast, in
continuous auditing, deep-learning models are continually trained on a stream
of recorded journal entries, e.g., of the last hour. Resulting in situations
where previous knowledge interferes with new information and will be entirely
overwritten. This work proposes a continual anomaly detection framework to
overcome both challenges and designed to learn from a stream of journal entry
data experiences. The framework is evaluated based on deliberately designed
audit scenarios and two real-world datasets. Our experimental results provide
initial evidence that such a learning scheme offers the ability to reduce
false-positive alerts and false-negative decisions.",2112.13215v2,cs.LG,2021-12-25 09:21:14+00:00,"[arxiv.Result.Author('Hamed Hemati'), arxiv.Result.Author('Marco Schreyer'), arxiv.Result.Author('Damian Borth')]",
60,Results of three quantitative predictions based on past regularities about voter turnout at the French 2009 European election,"Twelve turnout rates of French national elections by municipality have shown
statistical regularities, neither depend on the nature of the election, nor on
the national turnout level. Three quantitative predictions about voter turnout
at the French 2009 European election were made in arXiv:0905.4578. Here, we
give the results of these three predictions. Each one is confirmed by real
measures.",0906.2692v1,physics.soc-ph,2009-06-15 13:56:45+00:00,[arxiv.Result.Author('Christian Borghesi')],
61,Three quantitative predictions based on past regularities about voter turnout at the French 2009 European election,"The previous twelve turnout rates of French national elections by
municipality show regularities. These regularities do not depend on the
national turnout level, nor on the nature of the election. Based on past
statistical regularities we make three predictions. The first one deals with
the standard deviation of the turnout rate by municipality. The second one
refers to the continuity in time of the heterogeneity of turnout rates in the
vicinity of a municipality. The last one is about the correlation between the
heterogeneity of turnout rates in the vicinity of a municipality and the
population in its surroundings. Details, explanations and discussions will be
given in forthcoming papers.",0905.4578v1,physics.soc-ph,2009-05-28 09:11:39+00:00,[arxiv.Result.Author('Christian Borghesi')],
62,Asymmetric Partisan Voter Turnout Games,"Since Downs proposed that the act of voting is irrational in 1957, myriad
models have been proposed to explain voting and account for observed turnout
patterns. We propose a model in which partisans consider both the instrumental
and expressive benefits of their vote when deciding whether or not to abstain
in an election, introducing an asymmetry that most other models do not
consider. Allowing learning processes within our electorate, we analyze what
turnout states are rationalizable under various conditions. Our model predicts
comparative statics that are consistent with voter behavior. Furthermore,
relaxing some of our preliminary assumptions eliminates some of the
discrepancies between our model and empirical voter behavior.",2003.10313v1,physics.soc-ph,2020-03-23 15:01:26+00:00,"[arxiv.Result.Author('Cameron Guage'), arxiv.Result.Author('Feng Fu')]",
63,"Using auxiliary marginal distributions in imputations for nonresponse while accounting for survey weights, with application to estimating voter turnout","The Current Population Survey is the gold-standard data source for studying
who turns out to vote in elections. However, it suffers from potentially
nonignorable unit and item nonresponse. Fortunately, after elections, the total
number of voters is known from administrative sources and can be used to adjust
for potential nonresponse bias. We present a model-based approach to utilize
this known voter turnout rate, as well as other population marginal
distributions of demographic variables, in multiple imputation for unit and
item nonresponse. In doing so, we ensure that the imputations produce
design-based estimates that are plausible given the known margins. We introduce
and utilize a hybrid missingness model comprising a pattern mixture model for
unit nonresponse and selection models for item nonresponse. Using simulation
studies, we illustrate repeated sampling performance of the model under
different assumptions about the missingness mechanisms. We apply the model to
examine voter turnout by subgroups using the 2018 Current Population Survey for
North Carolina. As a sensitivity analysis, we examine how results change when
we allow for over-reporting, i.e., individuals self-reporting that they voted
when in fact they did not.",2209.05220v2,stat.ME,2022-09-12 13:08:33+00:00,"[arxiv.Result.Author('Jiurui Tang'), arxiv.Result.Author('D. Sunshine Hillygus'), arxiv.Result.Author('Jerome P. Reiter')]",
64,Voting From Jail,"We leverage new data on daily individual-level jail records and exploit the
timing of incarceration to estimate the causal effects of jail incarceration on
voting from jail in 2020. We find that registered voters booked into county
jails for the full duration of 2020 voting days were on average 46% less likely
to vote in 2020, relative to registered voters booked into the same jails
within 7-42 days after Election Day. The estimated negative effect of
incarceration on voting from jail was much larger for Black registered voters,
who were 78% less likely to vote in 2020 if booked into county jails for the
full duration of 2020 voting days, relative to Black registered voters booked
into the same jails just after Election Day. Placebo tests indicate no effects
of 2020 jail incarceration on the 2012 or 2016 turnout of registered voters. We
find inconsistent effects of jail incarceration on voter registration in 2020,
and effect sizes of comparable magnitude for turnout unconditional on
registration status. Our findings reveal the pressing need to enable
voting-eligible incarcerated individuals to exercise their constitutional right
to vote, and to address troubling racial disparities in the effect of jail
incarceration on the exercise of the right to vote.",2210.06542v1,stat.AP,2022-10-12 19:13:53+00:00,"[arxiv.Result.Author('Anna Harvey'), arxiv.Result.Author('Orion Taylor')]",
65,A transparent referendum protocol with immutable proceedings and verifiable outcome for trustless networks,"High voter turnout in elections and referendums is very desirable in order to
ensure a robust democracy. Secure electronic voting is a vision for the future
of elections and referendums. Such a system can counteract factors that hinder
strong voter turnout such as the requirement of physical presence during
limited hours at polling stations. However, this vision brings transparency and
confidentiality requirements that render the design of such solutions
challenging. Specifically, the counting must be implemented in a reproducible
way and the ballots of individual voters must remain concealed. In this paper,
we propose and evaluate a referendum protocol that ensures transparency,
confidentiality, and integrity, in trustless networks. The protocol is built by
combining Secure Multi-Party Computation (SMPC) and Distributed Ledger or
Blockchain technology. The persistence and immutability of the protocol
communication allows verifiability of the referendum outcome on the client
side. Voters therefore do not need to trust in third parties. We provide a
formal description and conduct a thorough security evaluation of our proposal.",1909.06462v1,cs.CR,2019-09-13 21:41:05+00:00,"[arxiv.Result.Author('Maximilian Schiedermeier'), arxiv.Result.Author('Omar Hasan'), arxiv.Result.Author('Tobias Mayer'), arxiv.Result.Author('Lionel Brunie'), arxiv.Result.Author('Harald Kosch')]",
66,Physics peeks into the ballot box,"Electoral results show universal features, such as statistics of candidates'
performance and turnout rates, in different countries and over time. Are voters
as predictable as atoms?",1210.2426v1,physics.soc-ph,2012-10-08 21:35:00+00:00,"[arxiv.Result.Author('Santo Fortunato'), arxiv.Result.Author('Claudio Castellano')]","Physics Today 65, 74-75 (2012)"
67,How a minority can win: Undemocratic outcomes in a simple model of voter turnout,"The outcome of an election depends not only on which candidate is more
popular, but also on how many of their voters actually turn out to vote. Here
we consider a simple model in which voters abstain from voting if they think
their vote would not matter. Specifically, they do not vote if they feel sure
their preferred candidate will win anyway (a condition we call complacency), or
if they feel sure their candidate will lose anyway (a condition we call
dejectedness). The voters reach these decisions based on a myopic assessment of
their local network, which they take as a proxy for the entire electorate:
voters know which candidate their neighbors prefer and they assume -- perhaps
incorrectly -- that those neighbors will turn out to vote, so they themselves
cast a vote if and only if it would produce a tie or a win for their preferred
candidate in their local neighborhood. We explore various network structures
and distributions of voter preferences and find that certain structures and
parameter regimes favor undemocratic outcomes where a minority faction wins,
especially when the locally preferred candidate is not representative of the
electorate as a whole.",2108.12503v1,physics.soc-ph,2021-08-27 21:09:53+00:00,"[arxiv.Result.Author('Ekaterina Landgren'), arxiv.Result.Author('Jonas L. Juul'), arxiv.Result.Author('Steven H. Strogatz')]",
68,"Efficient Choice, Inefficient Democracy? The Implications of Cable and Internet Access for Political Knowledge and Voter Turnout","This paper explains why, despite a marked increase in available political
information on cable television and the Internet, citizens' levels of political
knowledge have, at best, remained stagnant (Delli Carpini & Keeter, 1996).
Since the availability of entertainment content has increased too, the effect
of new media on knowledge and vote likelihood should be determined by people's
relative preferences for entertainment and information. Access to new media
should increase knowledge and vote likelihood among people who prefer news. At
the same time, it is hypothesized to have a negative effect on knowledge and
turnout for people who prefer entertainment content. Hypotheses are tested by
building a measure of Relative Entertainment Preference (REP) from existing NES
and Pew survey data. Results support the predicted interaction effect of media
environment (cable and/or Internet access) and motivation (REP) on political
knowledge and turnout. In particular, people who prefer entertainment to news
and have access to cable television and the Internet are less knowledgeable and
less likely to vote than any other group of people.",cs/0109110v1,cs.CY,2001-09-25 03:35:27+00:00,[arxiv.Result.Author('Markus Prior')],
69,"Election turnout statistics in many countries: similarities, differences, and a diffusive field model for decision-making","We study in details the turnout rate statistics for 77 elections in 11
different countries. We show that the empirical results established in a
previous paper for French elections appear to hold much more generally. We find
in particular that the spatial correlation of turnout rates decay
logarithmically with distance in all cases. This result is quantitatively
reproduced by a decision model that assumes that each voter makes his mind as a
result of three influence terms: one totally idiosyncratic component, one
city-specific term with short-ranged fluctuations in space, and one long-ranged
correlated field which propagates diffusively in space. A detailed analysis
reveals several interesting features: for example, different countries have
different degrees of local heterogeneities and seem to be characterized by a
different propensity for individuals to conform to the cultural norm. We
furthermore find clear signs of herding (i.e. strongly correlated decisions at
the individual level) in some countries, but not in others.",1201.0524v1,physics.soc-ph,2012-01-02 19:46:49+00:00,"[arxiv.Result.Author('Christian Borghesi'), arxiv.Result.Author('Jean-Claude Raynal'), arxiv.Result.Author('Jean-Philippe Bouchaud')]",
70,"The Efficiency Gap, Voter Turnout, and the Efficiency Principle","Recently, scholars from law and political science have introduced metrics
which use only election outcomes (and not district geometry) to assess the
presence of partisan gerrymandering. The most high-profile example of such a
tool is the efficiency gap. Some scholars have suggested that such tools should
be sensitive enough to alert us when two election outcomes have the same
percentage of votes going to political party $A$, but one of the two awards
party $A$ more seats. When a metric is able to distinguish election outcomes in
this way, that metric is said to satisfy the efficiency principle.
  In this article, we show that the efficiency gap fails to satisfy the
efficiency principle. We show precisely how the efficiency principle breaks
down in the presence of unequal voter turnout. To do this, we first present a
construction that, given any rationals $1/4< V<3/4$ and $0<S<1$, constructs an
election outcome with vote share $V$, seat share $S$, and EG = 0. (For
instance, one party can get 26% of the vote and anywhere from 1% to 99% of the
seats while the efficiency gap remains zero.) Then, for any election with vote
share $1/4<V<3/4$, seat share $S$, and EG= 0, we express the ratio $\rho$ of
average turnout in districts party $A$ lost to average turnout in districts
party $A$ won as a function in only $V$ and $S$. It is well known that when all
districts have equal turnout, EG can be expressed as a simple formula in $V$
and $S$; we express the efficiency gap of any election as an equation only in
$V, S,$ and $\rho$. We also report on the values of $\rho$ that can be observed
in actual elections.",1801.05301v2,physics.soc-ph,2018-01-13 16:38:53+00:00,[arxiv.Result.Author('Ellen Veomett')],
71,A coercion-resistant protocol for conducting elections by telephone,"We present a protocol that allows voters to phone in their votes. Our
protocol makes it expensive for a candidate and a voter to cooperate to prove
to the candidate who the voter voted for. When the electoral pool is large
enough, the cost to the candidate of manipulating sufficiently many votes to
have an influence on the election results becomes impossibly expensive. Hence,
the protocol provides candidates no incentive to attempt inducement or coercion
of voters, resulting in free and fair elections with the promise of cost
savings and higher voter turnout over traditional elections. One major
inadequacy with our suggested protocol is that we assume the existence of a
trusted election authority to count the votes.",1305.5359v2,cs.CR,2013-05-23 09:37:40+00:00,[arxiv.Result.Author('Manoj Gopalkrishnan')],
72,Influencing elections with statistics: Targeting voters with logistic regression trees,"In political campaigning substantial resources are spent on voter
mobilization, that is, on identifying and influencing as many people as
possible to vote. Campaigns use statistical tools for deciding whom to target
(""microtargeting""). In this paper we describe a nonpartisan campaign that aims
at increasing overall turnout using the example of the 2004 US presidential
election. Based on a real data set of 19,634 eligible voters from Ohio, we
introduce a modern statistical framework well suited for carrying out the main
tasks of voter targeting in a single sweep: predicting an individual's turnout
(or support) likelihood for a particular cause, party or candidate as well as
data-driven voter segmentation. Our framework, which we refer to as LORET (for
LOgistic REgression Trees), contains standard methods such as logistic
regression and classification trees as special cases and allows for a synthesis
of both techniques. For our case study, we explore various LORET models with
different regressors in the logistic model components and different
partitioning variables in the tree components; we analyze them in terms of
their predictive accuracy and compare the effect of using the full set of
available variables against using only a limited amount of information. We find
that augmenting a standard set of variables (such as age and voting history)
with additional predictor variables (such as the household composition in terms
of party affiliation) clearly improves predictive accuracy. We also find that
LORET models based on tree induction beat the unpartitioned models.
Furthermore, we illustrate how voter segmentation arises from our framework and
discuss the resulting profiles from a targeting point of view.",1311.7326v1,stat.AP,2013-11-28 14:21:41+00:00,"[arxiv.Result.Author('Thomas Rusch'), arxiv.Result.Author('Ilro Lee'), arxiv.Result.Author('Kurt Hornik'), arxiv.Result.Author('Wolfgang Jank'), arxiv.Result.Author('Achim Zeileis')]","Annals of Applied Statistics 2013, Vol. 7, No. 3, 1612-1639"
73,Low-Rank Approximations of Nonseparable Panel Models,"We provide estimation methods for nonseparable panel models based on low-rank
factor structure approximations. The factor structures are estimated by
matrix-completion methods to deal with the computational challenges of
principal component analysis in the presence of missing data. We show that the
resulting estimators are consistent in large panels, but suffer from
approximation and shrinkage biases. We correct these biases using matching and
difference-in-differences approaches. Numerical examples and an empirical
application to the effect of election day registration on voter turnout in the
U.S. illustrate the properties and usefulness of our methods.",2010.12439v2,econ.EM,2020-10-23 14:31:41+00:00,"[arxiv.Result.Author('Iván Fernández-Val'), arxiv.Result.Author('Hugo Freeman'), arxiv.Result.Author('Martin Weidner')]",
74,Group Incentives and Rational Voting,"Our model describes competition between groups driven by the choices of
self-interested voters within groups. Within a Poisson voting environment,
parties observe aggregate support from groups and can allocate prizes or
punishments to them. In a tournament style analysis, the model characterizes
how contingent allocation of prizes based on relative levels of support affects
equilibrium voting behavior. In addition to standard notions of pivotality,
voters influence the distribution of prizes across groups. Such prize
pivotality supports positive voter turnout even in non-competitive electoral
settings. The analysis shows that competition for a prize awarded to the most
supportive group is only stable when two groups actively support a party.
However, competition among groups to avoid punishment is stable in environments
with any number of groups. We conclude by examining implications for endogenous
group formation and how politicians structure the allocation of rewards and
punishments.",1106.3102v4,math.PR,2011-06-15 21:20:50+00:00,"[arxiv.Result.Author('Alastair Smith'), arxiv.Result.Author('Bruce Bueno de Mesquita'), arxiv.Result.Author('Tom LaGatta')]","Journal of Theoretical Politics February 22, 2016 0951629816630439"
75,A voter model on networks and multivariate beta distribution,"In elections, the vote shares or turnout rates show a strong spatial
correlation. The logarithmic decay with distance suggests that a 2D noisy
diffusive equation describes the system. Based on the study of U.S.
presidential elections data, it was determined that the fluctuations of vote
shares also exhibit a strong and long-range spatial correlation. Previously, it
was considered difficult to induce strong and long-range spatial correlation of
the vote shares without breaking the empirically observed narrow distribution.
We demonstrate that a voter model on networks shows such a behavior. In the
model, there are many voters in a node who are affected by the agents in the
node and by the agents in the linked nodes. A multivariate Wright-Fisher
diffusion equation for the joint probability density of the vote shares is
derived. The stationary distribution is a multivariate generalization of the
beta distribution. In addition, we also estimate the equilibrium values and the
covariance matrix of the vote shares and obtain a correspondence with a
multivariate normal distribution. This approach largely simplifies the
calibration of the parameters in the modeling of elections.",1810.05643v2,physics.soc-ph,2018-10-12 01:51:22+00:00,"[arxiv.Result.Author('Shintaro Mori'), arxiv.Result.Author('Masato Hisakado'), arxiv.Result.Author('Kazuaki Nakayama')]","Phys. Rev. E 99, 052307 (2019)"
76,Maximizing Contrasting Opinions in Signed Social Networks,"The classic influence maximization problem finds a limited number of
influential seed users in a social network such that the expected number of
influenced users in the network, following an influence cascade model, is
maximized. The problem has been studied in different settings, with further
generalization of the graph structure, e.g., edge weights and polarities,
target user categories, etc. In this paper, we introduce a unique influence
diffusion scenario involving a population that split into two distinct groups,
with opposing views. We aim at finding the top-$k$ influential seed nodes so to
simultaneously maximize the adoption of two distinct, antithetical opinions in
the two groups, respectively. Efficiently finding such influential users is
essential in a wide range of applications such as increasing voter engagement
and turnout, steering public debates and discussions on societal issues with
contentious opinions. We formulate this novel problem with the voter model to
simulate opinion diffusion and dynamics, and then design a linear-time and
exact algorithm COSiNeMax, while also investigating the long-term opinion
characteristics in the network. Our experiments with several real-world
datasets demonstrate the effectiveness and efficiency of the proposed
algorithm, compared to various baselines.",1910.12017v1,cs.SI,2019-10-26 07:53:46+00:00,"[arxiv.Result.Author('Kaivalya Rawal'), arxiv.Result.Author('Arijit Khan')]",
77,"A Unique One-Time Password Table Sequence Pattern Authentication: Application to Bicol University Union of Federated Faculty Association, Inc. (BUUFFAI) eVoting System","Electronic Voting System (EVS) is a type of voting program that deals
primarily with the selection, the casting of votes with embedded security
mechanism that detects errors, and the tamper-proof election of results done
through the use of an electronic system. It can include optical scan,
specialized voting kiosks and Internet voting approach. Most organizations have
difficulties when it comes to voting and the Bicol University Union of
Federated Faculty Association Incorporated (BUUFFAI) is not an exception. Some
of the problems involved include convenience, cost, geographical location of
the polling precinct, and voting turnouts. This study extends the scope of the
current BUUFFAI eVoting system to address such issues and to eliminate
inconvenience both to the faculty voters and the facilitators. This voting
scheme used an algorithmic OTP scheme based on table sequence pattern schedule
that randomly generates an XY coordinate unique to voters that will be sent to
voter registered email address. This study addressed the security requirements
and maintained election procedures with confidentiality, integrity and
availability.",1708.00562v1,cs.CR,2017-08-02 00:44:54+00:00,"[arxiv.Result.Author('Benedicto B. Balilo Jr.'), arxiv.Result.Author('Bobby D. Gerardo'), arxiv.Result.Author('Ruji P. Medina'), arxiv.Result.Author('Yungcheol Byun')]","International Journal of Computing Sciences Research (ISSN
  (print): 2546-0552)2017"
78,Leveraging Administrative Data for Bias Audits: Assessing Disparate Coverage with Mobility Data for COVID-19 Policy,"Anonymized smartphone-based mobility data has been widely adopted in devising
and evaluating COVID-19 response strategies such as the targeting of public
health resources. Yet little attention has been paid to measurement validity
and demographic bias, due in part to the lack of documentation about which
users are represented as well as the challenge of obtaining ground truth data
on unique visits and demographics. We illustrate how linking large-scale
administrative data can enable auditing mobility data for bias in the absence
of demographic information and ground truth labels. More precisely, we show
that linking voter roll data -- containing individual-level voter turnout for
specific voting locations along with race and age -- can facilitate the
construction of rigorous bias and reliability tests. These tests illuminate a
sampling bias that is particularly noteworthy in the pandemic context: older
and non-white voters are less likely to be captured by mobility data. We show
that allocating public health resources based on such mobility data could
disproportionately harm high-risk elderly and minority groups.",2011.07194v2,stat.AP,2020-11-14 02:04:14+00:00,"[arxiv.Result.Author('Amanda Coston'), arxiv.Result.Author('Neel Guha'), arxiv.Result.Author('Derek Ouyang'), arxiv.Result.Author('Lisa Lu'), arxiv.Result.Author('Alexandra Chouldechova'), arxiv.Result.Author('Daniel E. Ho')]","Proceedings of the 2021 ACM Conference on Fairness,
  Accountability, and Transparency. pp. 173-184"
79,Leveraging Auxiliary Information on Marginal Distributions in Nonignorable Models for Item and Unit Nonresponse,"Often, government agencies and survey organizations know the population
counts or percentages for some of the variables in a survey. These may be
available from auxiliary sources, for example, administrative databases or
other high quality surveys. We present and illustrate a model-based framework
for leveraging such auxiliary marginal information when handling unit and item
nonresponse. We show how one can use the margins to specify different
missingness mechanisms for each type of nonresponse. We use the framework to
impute missing values in voter turnout in a subset of data from the U.S.\
Current Population Survey (CPS). In doing so, we examine the sensitivity of
results to different assumptions about the unit and item nonresponse.",1907.06145v3,stat.ME,2019-07-13 23:02:47+00:00,"[arxiv.Result.Author('Olanrewaju Akande'), arxiv.Result.Author('Gabriel Madson'), arxiv.Result.Author('D. Sunshine Hillygus'), arxiv.Result.Author('Jerome P. Reiter')]",
80,"Electryo, In-person Voting with Transparent Voter Verifiability and Eligibility Verifiability","Selene is an e-voting protocol that allows voters to directly check their
individual vote, in cleartext, in the final tally via a tracker system, while
providing good coercion mitigation. This is in contrast to conventional,
end-to-end verifiable schemes in which the voter verifies the presence of an
encryption of her vote on the bulletin board. The Selene mechanism can be
applied to many e-voting schemes, but here we present an application to the
polling station context, resulting in a voter-verifiable electronic tally with
a paper audit trail. The system uses a smartcard-based public key system to
provide the individual verification and universal eligibility verifiability.
The paper record contains an encrypted link to the voter's identity, requiring
stronger assumptions on ballot privacy than normal paper voting, but with the
benefit of providing good auditability and dispute resolution as well as
supporting (comparison) risk limiting audits.",2105.14783v1,cs.CR,2021-05-31 08:22:29+00:00,"[arxiv.Result.Author('Peter B. Roenne'), arxiv.Result.Author('Peter Y. A Ryan'), arxiv.Result.Author('Marie-Laure Zollinger')]",
81,An AI-Powered VVPAT Counter for Elections in India,"The Election Commission of India has introduced Voter Verified Paper Audit
Trail since 2019. This mechanism has increased voter confidence at the time of
casting the votes. However, physical verification of the VVPATs against the
party level counts from the EVMs is done only in 5 (randomly selected) machines
per constituency. The time required to conduct physical verification becomes a
bottleneck in scaling this activity for 100% of machines in all constituencies.
We proposed an automated counter powered by image processing and machine
learning algorithms to speed up the process and address this issue.",2212.11124v1,cs.CV,2022-12-09 14:59:40+00:00,"[arxiv.Result.Author('Prasath Murugesan'), arxiv.Result.Author('Shamshu Dharwez Saganvali')]",
82,Auditing Indian Elections,"Indian Electronic Voting Machines (EVMs) will be fitted with printers that
produce Voter-Verifiable Paper Audit Trails (VVPATs) in time for the 2019
general election. VVPATs provide evidence that each vote was recorded as the
voter intended, without having to trust the perfection or security of the EVMs.
  However, confidence in election results requires more: VVPATs must be
preserved inviolate and then actually used to check the reported election
result in a trustworthy way that the public can verify. A full manual tally
from the VVPATs could be prohibitively expensive and time-consuming; moreover,
it is difficult for the public to determine whether a full hand count was
conducted accurately. We show how Risk-Limiting Audits (RLAs) could provide
high confidence in Indian election results. Compared to full hand recounts,
RLAs typically require manually inspecting far fewer VVPATs when the outcome is
correct, and are much easier for the electorate to observe in adequate detail
to determine whether the result is trustworthy.",1901.03108v2,cs.CR,2019-01-10 11:38:41+00:00,"[arxiv.Result.Author('Vishal Mohanty'), arxiv.Result.Author('Nicholas Akinyokun'), arxiv.Result.Author('Andrew Conway'), arxiv.Result.Author('Chris Culnane'), arxiv.Result.Author('Philip B. Stark'), arxiv.Result.Author('Vanessa Teague')]",
83,SOBA: Secrecy-preserving Observable Ballot-level Audit,"SOBA is an approach to election verification that provides observers with
justifiably high confidence that the reported results of an election are
consistent with an audit trail (""ballots""), which can be paper or electronic.
SOBA combines three ideas: (1) publishing cast vote records (CVRs) separately
for each contest, so that anyone can verify that each reported contest outcome
is correct, if the CVRs reflect voters' intentions with sufficient accuracy;
(2) shrouding a mapping between ballots and the CVRs for those ballots to
prevent the loss of privacy that could occur otherwise; (3) assessing the
accuracy with which the CVRs reflect voters' intentions for a collection of
contests while simultaneously assessing the integrity of the shrouded mapping
between ballots and CVRs by comparing randomly selected ballots to the CVRs
that purport to represent them. Step (1) is related to work by the Humboldt
County Election Transparency Project, but publishing CVRs separately for
individual contests rather than images of entire ballots preserves privacy.
Step (2) requires a cryptographic commitment from elections officials.
Observers participate in step (3), which relies on the ""super-simple
simultaneous single-ballot risk-limiting audit."" Step (3) is designed to reveal
relatively few ballots if the shrouded mapping is proper and the CVRs
accurately reflect voter intent. But if the reported outcomes of the contests
differ from the outcomes that a full hand count would show, step (3) is
guaranteed to have a large chance of requiring all the ballots to be counted by
hand, thereby limiting the risk that an incorrect outcome will become official
and final.",1105.5803v2,stat.AP,2011-05-29 17:00:20+00:00,"[arxiv.Result.Author('Josh Benaloh'), arxiv.Result.Author('Douglas Jones'), arxiv.Result.Author('Eric Lazarus'), arxiv.Result.Author('Mark Lindeman'), arxiv.Result.Author('Philip B. Stark')]",
84,"STAR-Vote: A Secure, Transparent, Auditable, and Reliable Voting System","In her 2011 EVT/WOTE keynote, Travis County, Texas County Clerk Dana
DeBeauvoir described the qualities she wanted in her ideal election system to
replace their existing DREs. In response, in April of 2012, the authors,
working with DeBeauvoir and her staff, jointly architected STAR-Vote, a voting
system with a DRE-style human interface and a ""belt and suspenders"" approach to
verifiability. It provides both a paper trail and end-to-end cryptography using
COTS hardware. It is designed to support both ballot-level risk-limiting
audits, and auditing by individual voters and observers. The human interface
and process flow is based on modern usability research. This paper describes
the STAR-Vote architecture, which could well be the next-generation voting
system for Travis County and perhaps elsewhere.",1211.1904v1,cs.CR,2012-11-08 17:06:33+00:00,"[arxiv.Result.Author('Josh Benaloh'), arxiv.Result.Author('Mike Byrne'), arxiv.Result.Author('Philip Kortum'), arxiv.Result.Author('Neal McBurnett'), arxiv.Result.Author('Olivier Pereira'), arxiv.Result.Author('Philip B. Stark'), arxiv.Result.Author('Dan S. Wallach')]",
85,OpenVoting: Making E2E-V voting transparent and recoverable,"End-to-end verifiable (E2E-V) voting systems have been around for some time.
However, their adoption in large elections is poor, seemingly because of the
inaccessibility of their underlying complex cryptography to the general
electorate. Meanwhile, risk-limiting audits based on voter-verified paper
records (VVPR) have been effective in bringing easy-to-understand verifiability
and recoverability to electronic voting, but they generally require the
electorate to trust the post-election custody chain of the paper trail.
  In this paper, we propose \emph{OpenVoting}, a novel polling booth voting
protocol that publicly demonstrates a one-to-one correspondence between the
cryptographically-secured electronic vote records and the easily understandable
paper records, while protecting individual voter secrecy and polling
booth-level voting statistics. This one-to-one correspondence helps each
provide a check for the other, improves overall transparency, and also enables
efficient and principled recovery in case of tally mismatches, by pinpointing
mismatching votes and their associated polling booths. We propose a novel
distributed zero-knowledge proof (ZKP) that facilitates the above.
  To an ordinary voter \emph{OpenVoting} looks just like an old fashioned paper
based voting system, with minimal additional cognitive overload.",1908.09557v6,cs.CR,2019-08-26 09:32:54+00:00,"[arxiv.Result.Author('Prashant Agrawal'), arxiv.Result.Author('Kabir Tomer'), arxiv.Result.Author('Abhinav Nakarmi'), arxiv.Result.Author('Mahabir Prasad Jhanwar'), arxiv.Result.Author('Subodh Sharma'), arxiv.Result.Author('Subhashis Banerjee')]",
86,"D-DEMOS: A distributed, end-to-end verifiable, internet voting system","E-voting systems have emerged as a powerful technology for improving
democracy by reducing election cost, increasing voter participation, and even
allowing voters to directly verify the entire election procedure. Prior
internet voting systems have single points of failure, which may result in the
compromise of availability, voter secrecy, or integrity of the election
results. In this paper, we present the design, implementation, security
analysis, and evaluation of D-DEMOS, a complete e-voting system that is
distributed, privacy-preserving and end-to-end verifiable. Our system includes
a fully asynchronous vote collection subsystem that provides immediate
assurance to the voter her vote was recorded as cast, without requiring
cryptographic operations on behalf of the voter. We also include a distributed,
replicated and fault-tolerant Bulletin Board component, that stores all
necessary election-related information, and allows any party to read and verify
the complete election process. Finally, we also incorporate trustees, i.e.,
individuals who control election result production while guaranteeing privacy
and end-to-end-verifiability as long as their strong majority is honest. Our
system is the first e-voting system whose voting operation is human verifiable,
i.e., a voter can vote over the web, even when her web client stack is
potentially unsafe, without sacrificing her privacy, and still be assured her
vote was recorded as cast. Additionally, a voter can outsource election
auditing to third parties, still without sacrificing privacy. Finally, as the
number of auditors increases, the probability of election fraud going
undetected is diminished exponentially. We provide a model and security
analysis of the system. We implement a prototype of the complete system, we
measure its performance experimentally, and we demonstrate its ability to
handle large-scale elections.",1507.06812v2,cs.CR,2015-07-24 11:29:12+00:00,"[arxiv.Result.Author('Nikos Chondros'), arxiv.Result.Author('Bingsheng Zhang'), arxiv.Result.Author('Thomas Zacharias'), arxiv.Result.Author('Panos Diamantopoulos'), arxiv.Result.Author('Stathis Maneas'), arxiv.Result.Author('Christos Patsonakis'), arxiv.Result.Author('Alex Delis'), arxiv.Result.Author('Aggelos Kiayias'), arxiv.Result.Author('Mema Roussopoulos')]",
87,Non(c)esuch Ballot-Level Risk-Limiting Audits for Precinct-Count Voting Systems,"Risk-limiting audits (RLAs) guarantee a high probability of correcting
incorrect reported outcomes before the outcomes are certified. The most
efficient use ballot-level comparison, comparing the voting system's
interpretation of individual ballot cards sampled at random (cast-vote records,
CVRs) from a trustworthy paper trail to a human interpretation of the same
cards. Such comparisons require the voting system to create and export CVRs in
a way that can be linked to the individual ballots the CVRs purport to
represent. Such links can be created by keeping the ballots in the order in
which they are scanned or by printing a unique serial number on each ballot.
But for precinct-count systems (PCOS), these strategies may compromise vote
anonymity: the order in which ballots are cast may identify the voters who cast
them. Printing a unique pseudo-random number (""cryptographic nonce"") on each
ballot card after the voter last touches it could reduce such privacy risks.
But what if the system does not in fact print a unique number on each ballot or
does not accurately report the numbers it printed? This paper gives two ways to
conduct an RLA so that even if the system does not print a genuine nonce on
each ballot or misreports the nonces it used, the audit's risk limit is not
compromised (however, the anonymity of votes might be compromised). One method
allows untrusted technology to be used to imprint and to retrieve ballot cards.
The method is adaptive: if the technology behaves properly, this protection
does not increase the audit workload. But if the imprinting or retrieval system
misbehaves, the sample size the RLA requires to confirm the reported results
when the results are correct is generally larger than if the imprinting and
retrieval were accurate.",2207.01362v1,cs.CR,2022-07-04 12:35:42+00:00,[arxiv.Result.Author('Philip B. Stark')],
88,Verifiable Elections with Commitment Consistent Encryption -- A Primer,"This note provides an introduction to the PPATS Commitment Consistent
Encryption (CCE) scheme proposed by Cuvelier, Pereira and Peters and its use in
the design of end-to-end verifiable elections with a perfectly private audit
trail. These elections can be verified using audit data that will never leak
any information about the vote, even if all the private keys of the elections
are compromised, or if the cryptographic assumptions are broken.",1412.7358v1,cs.CR,2014-12-23 13:35:21+00:00,[arxiv.Result.Author('Olivier Pereira')],
89,Towards Privacy-assured and Lightweight On-chain Auditing of Decentralized Storage,"How to audit outsourced data in centralized storage like cloud is
well-studied, but it is largely under-explored for the rising decentralized
storage network (DSN) that bodes well for a billion-dollar market. To realize
DSN as a usable service in a truly decentralized manner, the blockchain comes
in handy -- to record and verify audit trails in forms of proof of storage, and
based on that, to handle fair payments with necessary dispute resolution.
  Leaving the audit trails on the blockchain offers transparency and fairness,
yet it 1) sacrifices privacy, as they may leak information about the data under
audit, and 2) overwhelms on-chain resources, as they may be practically large
in size and expensive to verify. Prior auditing designs in centralized settings
are not directly applicable here. A handful of proposals targeting DSN cannot
satisfactorily address these issues either.
  We present an auditing solution that addresses on-chain privacy and
efficiency, from a synergy of homomorphic linear authenticators with polynomial
commitments for succinct proofs, and the sigma protocol for provable privacy.
The solution results in, per audit, 288-byte proof written to the blockchain,
and constant verification cost. It can sustain long-term operation and easily
scale to thousands of users on Ethereum.",2005.05531v3,cs.CR,2020-05-12 03:14:09+00:00,"[arxiv.Result.Author('Yuefeng Du'), arxiv.Result.Author('Huayi Duan'), arxiv.Result.Author('Anxin Zhou'), arxiv.Result.Author('Cong Wang'), arxiv.Result.Author('Man Ho Au'), arxiv.Result.Author('Qian Wang')]",
90,Electt: running auditable and verifiable elections in untrusted environments,"We present a system for running auditable and verifiable elections in
untrusted environments. Votes are anonymous since the order of candidates on a
ballot sheet is random. Tellers see only the position of the candidate. Voters
can check their vote. An election is auditable using blockchain log.
Threshold-encryption, which is used to implement the quorum, prevents a
deadlock from occurring if a minority of candidates or observers tries to
sabotage the election. Candidates and observers can indicate that the election
was free and fair by exposing their keys, which are used by the system to
decrypt each vote. Ballot sheets are encrypted by onion-routing, which has a
layer with the key of the election instance, so it's impossible for a quorum to
decode the results before they have announced their decision by exposing their
keys. A register of voters ensures that only verified voters can vote without
compromising their identity. If there any doubts about the identity of a voter,
their vote can be excluded from the election, if a quorum agrees. This system
is designed to scale from one instance to a distributed system that runs over
an unlimited number of instances, which can be achieved using cloud instances
or smartphones belonging to voters or tellers.",2011.10902v2,cs.CR,2020-11-22 00:58:34+00:00,[arxiv.Result.Author('Kirill A. Korinsky')],
91,Boardroom Voting: Verifiable Voting with Ballot Privacy Using Low-Tech Cryptography in a Single Room,"A boardroom election is an election that takes place in a single room -- the
boardroom -- in which all voters can see and hear each other. We present an
initial exploration of boardroom elections with ballot privacy and voter
verifiability that use only ""low-tech cryptography"" without using computers to
mark or collect ballots. Specifically, we define the problem, introduce several
building blocks, and propose a new protocol that combines these blocks in novel
ways. Our new building blocks include ""foldable ballots"" that can be rotated to
hide the alignment of ballot choices with voting marks, and ""visual secrets""
that are easy to remember and use but hard to describe. Although closely seated
participants in a boardroom election have limited privacy, the protocol ensures
that no one can determine how others voted. Moreover, each voter can verify
that their ballot was correctly cast, collected, and counted, without being
able to prove how they voted, providing assurance against undue influence.
Low-tech cryptography is useful in situations where constituents do not trust
computer technology, and it avoids the complex auditing requirements of
end-to-end cryptographic voting systems such as Pr\^{e}t-\`{a}-Voter. This
paper's building blocks and protocol are meant to be a proof of concept that
might be tested for usability and improved.",2007.14916v2,cs.CR,2020-07-29 15:40:51+00:00,"[arxiv.Result.Author('Enka Blanchard'), arxiv.Result.Author('Ted Selker'), arxiv.Result.Author('Alan T. Sherman')]",
92,Explicit Auditing,"The Calculus of Audited Units (CAU) is a typed lambda calculus resulting from
a computational interpretation of Artemov's Justification Logic under the
Curry-Howard isomorphism; it extends the simply typed lambda calculus by
providing audited types, inhabited by expressions carrying a trail of their
past computation history. Unlike most other auditing techniques, CAU allows the
inspection of trails at runtime as a first-class operation, with applications
in security, debugging, and transparency of scientific computation.
  An efficient implementation of CAU is challenging: not only do the sizes of
trails grow rapidly, but they also need to be normalized after every beta
reduction. In this paper, we study how to reduce terms more efficiently in an
untyped variant of CAU by means of explicit substitutions and explicit auditing
operations, finally deriving a call-by-value abstract machine.",1808.00486v1,cs.LO,2018-08-01 18:03:02+00:00,"[arxiv.Result.Author('Wilmer Ricciotti'), arxiv.Result.Author('James Cheney')]",
93,Auditing for Core Stability in Participatory Budgeting,"We consider the participatory budgeting problem where each of $n$ voters
specifies additive utilities over $m$ candidate projects with given sizes, and
the goal is to choose a subset of projects (i.e., a committee) with total size
at most $k$. Participatory budgeting mathematically generalizes multiwinner
elections, and both have received great attention in computational social
choice recently. A well-studied notion of group fairness in this setting is
core stability: Each voter is assigned an ""entitlement"" of $\frac{k}{n}$, so
that a subset $S$ of voters can pay for a committee of size at most $|S| \cdot
\frac{k}{n}$. A given committee is in the core if no subset of voters can pay
for another committee that provides each of them strictly larger utility. This
provides proportional representation to all voters in a strong sense.
  In this paper, we study the following auditing question: Given a committee
computed by some preference aggregation method, how close is it to the core?
Concretely, how much does the entitlement of each voter need to be scaled down
by, so that the core property subsequently holds? As our main contribution, we
present computational hardness results for this problem, as well as a
logarithmic approximation algorithm via linear program rounding. We show that
our analysis is tight against the linear programming bound. Additionally, we
consider two related notions of group fairness that have similar audit
properties. The first is Lindahl priceability, which audits the closeness of a
committee to a market clearing solution. We show that this is related to the
linear programming relaxation of auditing the core, leading to efficient exact
and approximation algorithms for auditing. The second is a novel weakening of
the core that we term the sub-core, and we present computational results for
auditing this notion as well.",2209.14468v1,cs.GT,2022-09-28 23:13:06+00:00,"[arxiv.Result.Author('Kamesh Munagala'), arxiv.Result.Author('Yiheng Shen'), arxiv.Result.Author('Kangning Wang')]",
94,Risk-Limiting Tallies,"Many voter-verifiable, coercion-resistant schemes have been proposed, but
even the most carefully designed systems necessarily leak information via the
announced result. In corner cases, this may be problematic. For example, if all
the votes go to one candidate then all vote privacy evaporates. The mere
possibility of candidates getting no or few votes could have implications for
security in practice: if a coercer demands that a voter cast a vote for such an
unpopular candidate, then the voter may feel obliged to obey, even if she is
confident that the voting system satisfies the standard coercion resistance
definitions. With complex ballots, there may also be a danger of ""Italian""
style (aka ""signature"") attacks: the coercer demands the voter cast a ballot
with a specific, identifying pattern. Here we propose an approach to tallying
end-to-end verifiable schemes that avoids revealing all the votes but still
achieves whatever confidence level in the announced result is desired. Now a
coerced voter can claim that the required vote must be amongst those that
remained shrouded. Our approach is based on the well-established notion of
Risk-Limiting Audits, but here applied to the tally rather than to the audit.
We show that this approach counters coercion threats arising in extreme tallies
and ""Italian"" attacks. We illustrate our approach by applying it to the Selene
scheme, and we extend the approach to Risk-Limiting Verification, where not all
vote trackers are revealed, thereby enhancing the coercion mitigation
properties of Selene.",1908.04947v1,cs.CR,2019-08-14 04:23:10+00:00,"[arxiv.Result.Author('Wojciech Jamroga'), arxiv.Result.Author('Peter B. Roenne'), arxiv.Result.Author('Peter Y. A. Ryan'), arxiv.Result.Author('Philip B. Stark')]",
95,On the security of ballot marking devices,"A recent debate among election experts has considered whether electronic
ballot marking devices (BMDs) have adequate security against the risks of
malware. A malicious BMD might produce a printed ballot that disagrees with a
voter's actual intent, with the hope that voters would be unlikely to detect
this subterfuge. This essay considers how an election administrator can create
reasonable auditing procedures to gain confidence that their fleet of BMDs is
operating correctly, allowing voters to benefit from the usability and
accessibility features of BMDs while the overall election still benefits from
the same security and reliability properties we expect from hand-marked paper
ballots.",1908.01897v2,cs.CR,2019-08-05 23:04:16+00:00,[arxiv.Result.Author('Dan S. Wallach')],
96,EMA: Auditing Data Removal from Trained Models,"Data auditing is a process to verify whether certain data have been removed
from a trained model. A recently proposed method (Liu et al. 20) uses
Kolmogorov-Smirnov (KS) distance for such data auditing. However, it fails
under certain practical conditions. In this paper, we propose a new method
called Ensembled Membership Auditing (EMA) for auditing data removal to
overcome these limitations. We compare both methods using benchmark datasets
(MNIST and SVHN) and Chest X-ray datasets with multi-layer perceptrons (MLP)
and convolutional neural networks (CNN). Our experiments show that EMA is
robust under various conditions, including the failure cases of the previously
proposed method. Our code is available at: https://github.com/Hazelsuko07/EMA.",2109.03675v2,cs.LG,2021-09-08 14:22:02+00:00,"[arxiv.Result.Author('Yangsibo Huang'), arxiv.Result.Author('Xiaoxiao Li'), arxiv.Result.Author('Kai Li')]",
97,Reversibility and Composition of Rewriting in Hierarchies,"In this paper, we study how graph transformations based on sesqui-pushout
rewriting can be reversed and how the composition of rewrites can be
constructed. We illustrate how such reversibility and composition can be used
to design an audit trail system for individual graphs and graph hierarchies.
This provides us with a compact way to maintain the history of updates of an
object, including its multiple versions. The main application of the designed
framework is an audit trail of updates to knowledge represented by hierarchies
of graphs. Therefore, we introduce the notion of rule hierarchy that represents
a transformation of the entire hierarchy, study how rule hierarchies can be
applied to hierarchies and analyse the conditions under which this application
is reversible. We then present a theory for constructing the composition of
consecutive hierarchy rewrites. The prototype audit trail system for
transformations in hierarchies of simple graphs with attributes is implemented
as part of the ReGraph Python library.",2012.01661v1,cs.LO,2020-12-03 02:29:28+00:00,"[arxiv.Result.Author('Russ Harmer'), arxiv.Result.Author('Eugenia Oshurko')]","EPTCS 330, 2020, pp. 145-162"
98,Phrase-Verified Voting: Verifiable Low-Tech Remote Boardroom Voting,"We present Phrase-Verified Voting, a voter-verifiable remote voting system
assembled from commercial off-the-shelf software for small private elections.
The system is transparent and enables each voter to verify that the tally
includes their ballot selection without requiring any understanding of
cryptography. This paper describes the system and its use in fall 2020, to vote
remotely in promotion committees in a university. Each voter fills out a form
in the cloud with their vote V (YES, NO, ABSTAIN) and a passphrase P-two words
entered by the voter. The system generates a verification prompt of the (P,V)
pairs and a tally of the votes, organized to help visualize how the votes add
up. After the polls close, each voter verifies that this table lists their
(P,V) pair and that the tally is computed correctly. The system is especially
appropriate for any small group making sensitive decisions. Because the system
would not prevent a coercer from demanding that their victim use a specified
passphrase, it is not designed for applications where such malfeasance would be
likely or go undetected. Results from 43 voters show that the system was
well-accepted, performed effectively for its intended purpose, and introduced
users to the concept of voter-verified elections. Compared to the commonly-used
alternatives of paper ballots or voting by email, voters found the system
easier to use, and that it provided greater privacy and outcome integrity.",2103.07180v1,cs.CR,2021-03-12 09:59:55+00:00,"[arxiv.Result.Author('Enka Blanchard'), arxiv.Result.Author('Ryan Robucci'), arxiv.Result.Author('Ted Selker'), arxiv.Result.Author('Alan Sherman')]",
99,Voter Perceptions of Trust in Risk-Limiting Audits,"Risk-limiting audits (RLAs) are expected to strengthen the public confidence
in the correctness of an election outcome. We hypothesize that this is not
always the case, in part because for large margins between the winner and the
runner-up, the number of ballots to be drawn can be so small that voters lose
confidence. We conduct a user study with 105 participants resident in the US.
Our findings confirm the hypothesis, showing that our study participants felt
less confident when they were told the number of ballots audited for RLAs. We
elaborate on our findings and propose recommendations for future use of RLAs.",2109.07918v1,cs.CY,2021-09-15 07:36:17+00:00,"[arxiv.Result.Author('Asmita Dalela'), arxiv.Result.Author('Oksana Kulyk'), arxiv.Result.Author('Carsten Schürmann')]",
100,"Voter Verification of BMD Ballots Is a Two-Part Question: Can They? Mostly, They Can. Do They? Mostly, They Don't","The question of whether or not voters actually verify ballots produced by
ballot marking devices (BMDs) is presently the subject of some controversy.
Recent studies (e.g., Bernhard, et al. 2020) suggest the verification rate is
low. What is not clear from previous research is whether this is more a result
of voters being unable to do so accurately or whether this is because voters
simply choose not to attempt verification in the first place. In order to
understand this problem, we conducted an experiment in which 108 participants
participated in a mock election where the BMD displayed the voters' true
choices, but then changed a subset of those choices on the printed ballot. The
design of the printed ballot, the length of the ballot, the number of changes
that were made to the ballot, the location of those changes, and the
instructions provided to the voters were manipulated as part of the experiment.
Results indicated that of those voters who chose to examine the printed ballot,
76% detected anomalies, indicating that voters can reliably detect errors on
their ballot if they will simply review it. This suggests that administrative
remedies, rather than attempts to alter fundamental human perceptual
capabilities, could be employed to encourage voters to check their ballots,
which could prove as an effective countermeasure.",2003.04997v1,cs.HC,2020-03-10 21:06:42+00:00,"[arxiv.Result.Author('Philip Kortum'), arxiv.Result.Author('Michael D. Byrne'), arxiv.Result.Author('Julie Whitmore')]",
101,On cryptological schemes for r-person secret vote and r-person authentication,"We introduce a scheme for the membership verification, a scheme for a secret
ballot, a scheme for the unanimity rule which can hide the number of voter
using some partition number identities.",2008.06224v1,math.CO,2020-08-14 07:43:22+00:00,[arxiv.Result.Author('BongJu Kim')],
102,Information-Theoretically Secure Voting Without an Honest Majority,"We present three voting protocols with unconditional privacy and
information-theoretic correctness, without assuming any bound on the number of
corrupt voters or voting authorities. All protocols have polynomial complexity
and require private channels and a simultaneous broadcast channel. Our first
protocol is a basic voting scheme which allows voters to interact in order to
compute the tally. Privacy of the ballot is unconditional, but any voter can
cause the protocol to fail, in which case information about the tally may
nevertheless transpire. Our second protocol introduces voting authorities which
allow the implementation of the first protocol, while reducing the interaction
and limiting it to be only between voters and authorities and among the
authorities themselves. The simultaneous broadcast is also limited to the
authorities. As long as a single authority is honest, the privacy is
unconditional, however, a single corrupt authority or a single corrupt voter
can cause the protocol to fail. Our final protocol provides a safeguard against
corrupt voters by enabling a verification technique to allow the authorities to
revoke incorrect votes. We also discuss the implementation of a simultaneous
broadcast channel with the use of temporary computational assumptions, yielding
versions of our protocols achieving everlasting security.",0806.1931v1,cs.CR,2008-06-11 18:51:04+00:00,"[arxiv.Result.Author('Anne Broadbent'), arxiv.Result.Author('Alain Tapp')]",
103,Phrase-Verified Voting: Verifiable Low-Tech Remote Boardroom Voting,"We present Phrase-Verified Voting, a voter-verifiable remote voting system
assembled from commercial off-the-shelf software for small private elections.
The system is transparent and enables each voter to verify that the tally
includes their ballot selection without requiring any understanding of
cryptography. This paper describes the system and its use in fall 2020, to vote
remotely in promotion committees in a university. Each voter fills out a form
in the cloud with their vote V (YES, NO, ABSTAIN) and a passphrase P-two words
entered by the voter. The system generates a verification prompt of the (P,V)
pairs and a tally of the votes, organized to help visualize how the votes add
up. After the polls close, each voter verifies that this table lists their
(P,V) pair and that the tally is computed correctly. The system is especially
appropriate for any small group making sensitive decisions. Because the system
would not prevent a coercer from demanding that their victim use a specified
passphrase, it is not designed for applications where such malfeasance would be
likely or go undetected. Results from 43 voters show that the system was
well-accepted, performed effectively for its intended purpose, and introduced
users to the concept of voter-verified elections. Compared to the commonly-used
alternatives of paper ballots or voting by email, voters found the system
easier to use, and that it provided greater privacy and outcome integrity.",2103.07180v1,cs.CR,2021-03-12 09:59:55+00:00,"[arxiv.Result.Author('Enka Blanchard'), arxiv.Result.Author('Ryan Robucci'), arxiv.Result.Author('Ted Selker'), arxiv.Result.Author('Alan Sherman')]",
104,Estonian Voting Verification Mechanism Revisited,"After the Estonian Parliamentary Elections held in 2011, an additional
verification mechanism was integrated into the i-voting system in order to
resist corrupted voting devices, including the so called Student's Attack where
a student practically showed that the voting system is indeed not verifiable by
developing several versions of malware capable of blocking or even changing the
vote. This mechanism gives voters the opportunity to verify whether the vote
they cast is stored in the central system correctly. However, the verification
phase ends by displaying the cast vote in plain form on the verification
device. In other words, the device on which the verification is done learns the
voter's choice. In this work, our aim is to investigate this verification phase
in detail and to point out that leaking the voter's choice to the verification
application may harm the voter privacy. Additionally, when applied in a wide
range, this would even compromise the fairness and the overall secrecy of the
elections. In this respect, we propose an alternative verification mechanism
for the Estonian i-voting system to overcome this vulnerability. Not only is
the proposed mechanism secure and resistant against corrupted verification
devices, so does it successfully verify whether the vote is correctly stored in
the system. We also highlight that our proposed mechanism brings only symmetric
encryptions and hash functions on the verification device, thereby mitigating
these weaknesses in an efficient way with a negligible cost. More concretely,
it brings only $m$ additional symmetric key decryptions to the verification
device, where $m$ denoting the number of candidates. Finally, we prove the
security of the proposed verification mechanism and compare the cost complexity
of the proposed method with that of the current mechanism.",1612.00668v2,cs.CR,2016-12-02 13:08:16+00:00,"[arxiv.Result.Author('K\x7foksal Mus'), arxiv.Result.Author('Mehmet Sabir Kiraz'), arxiv.Result.Author('Murat Cenk'), arxiv.Result.Author('Isa Sertkaya')]",
105,"Electryo, In-person Voting with Transparent Voter Verifiability and Eligibility Verifiability","Selene is an e-voting protocol that allows voters to directly check their
individual vote, in cleartext, in the final tally via a tracker system, while
providing good coercion mitigation. This is in contrast to conventional,
end-to-end verifiable schemes in which the voter verifies the presence of an
encryption of her vote on the bulletin board. The Selene mechanism can be
applied to many e-voting schemes, but here we present an application to the
polling station context, resulting in a voter-verifiable electronic tally with
a paper audit trail. The system uses a smartcard-based public key system to
provide the individual verification and universal eligibility verifiability.
The paper record contains an encrypted link to the voter's identity, requiring
stronger assumptions on ballot privacy than normal paper voting, but with the
benefit of providing good auditability and dispute resolution as well as
supporting (comparison) risk limiting audits.",2105.14783v1,cs.CR,2021-05-31 08:22:29+00:00,"[arxiv.Result.Author('Peter B. Roenne'), arxiv.Result.Author('Peter Y. A Ryan'), arxiv.Result.Author('Marie-Laure Zollinger')]",
106,An AI-Powered VVPAT Counter for Elections in India,"The Election Commission of India has introduced Voter Verified Paper Audit
Trail since 2019. This mechanism has increased voter confidence at the time of
casting the votes. However, physical verification of the VVPATs against the
party level counts from the EVMs is done only in 5 (randomly selected) machines
per constituency. The time required to conduct physical verification becomes a
bottleneck in scaling this activity for 100% of machines in all constituencies.
We proposed an automated counter powered by image processing and machine
learning algorithms to speed up the process and address this issue.",2212.11124v1,cs.CV,2022-12-09 14:59:40+00:00,"[arxiv.Result.Author('Prasath Murugesan'), arxiv.Result.Author('Shamshu Dharwez Saganvali')]",
107,Zero Knowledge Identification and Verification of Voting Systems,"Current methods of voter identification, especially in India, are highly
primitive and error-prone, depending on verification by (mostly) sight, by
highly trusted election officials. This paper attempts to provide a trustless
and zero-knowledge method of voter identification, while simultaneously
reducing error. It also proposes a method for vote verification, that is,
ensuring that the vote cast by a legal voter is registered as cast and tallied
as registered. While numerous methods of zero-knowledge identification are
available in the literature, very few of those are implementable on a large
scale and subject to the type of constraints that are present, eg., in India.
This paper attempts to provide a solution which, while preserving the integrity
of the available methods, will also be more scalable and cost-effective.",2212.06388v1,cs.CR,2022-12-13 05:57:11+00:00,"[arxiv.Result.Author('Arunava Gantait'), arxiv.Result.Author('Rajit Goyal'), arxiv.Result.Author('Syed Sajid Husain Rizvi'), arxiv.Result.Author('Zaira Haram')]",
108,Dispute Resolution in Voting,"In voting, disputes arise when a voter claims that the voting authority is
dishonest and did not correctly process his ballot while the authority claims
to have followed the protocol. A dispute can be resolved if any third party can
unambiguously determine who is right. We systematically characterize all
relevant disputes for a generic, practically relevant, class of voting
protocols. Based on our characterization, we propose a new definition of
dispute resolution for voting that accounts for the possibility that both
voters and the voting authority can make false claims and that voters may
abstain from voting.
  A central aspect of our work is timeliness: a voter should possess the
evidence required to resolve disputes no later than the election's end. We
characterize what assumptions are necessary and sufficient for timeliness in
terms of a communication topology for our voting protocol class. We formalize
the dispute resolution properties and communication topologies symbolically.
This provides the basis for verification of dispute resolution for a broad
class of protocols. To demonstrate the utility of our model, we analyze a
mixnet-based voting protocol and prove that it satisfies dispute resolution as
well as verifiability and receipt-freeness. To prove our claims, we combine
machine-checked proofs with traditional pen-and-paper proofs.",2005.03749v2,cs.CR,2020-05-07 20:51:24+00:00,"[arxiv.Result.Author('David Basin'), arxiv.Result.Author('Sasa Radomirovic'), arxiv.Result.Author('Lara Schmid')]",
109,RemoteVote and SAFE Vote: Towards Usable End-to-End Verification for Vote-by-Mail,"Postal voting is growing rapidly in the U.S., with 43% of voters casting
ballots by mail in 2020, yet until recently there has been little research
about extending the protections of end-to-end verifiable (E2E-V) election
schemes to vote-by-mail contexts. The first - and to date, only - framework to
focus on this setting is STROBE, which has important usability limitations. In
this work, we present two approaches, RemoteVote and SAFE Vote, that allow
mail-in voters to benefit from E2E-V without changing the voter experience for
those who choose not to participate in verification. To evaluate these systems
and compare them with STROBE, we consider an expansive set of properties,
including novel attributes of usability and verifiability, several of which
have applicability beyond vote-by-mail contexts. We hope that our work will
help catalyze further progress towards universal applicability of E2E-V for
real-world elections.",2111.08662v3,cs.CR,2021-11-16 17:49:54+00:00,"[arxiv.Result.Author('Braden L. Crimmins'), arxiv.Result.Author('Marshall Rhea'), arxiv.Result.Author('J. Alex Halderman')]",
110,Risk-Limiting Tallies,"Many voter-verifiable, coercion-resistant schemes have been proposed, but
even the most carefully designed systems necessarily leak information via the
announced result. In corner cases, this may be problematic. For example, if all
the votes go to one candidate then all vote privacy evaporates. The mere
possibility of candidates getting no or few votes could have implications for
security in practice: if a coercer demands that a voter cast a vote for such an
unpopular candidate, then the voter may feel obliged to obey, even if she is
confident that the voting system satisfies the standard coercion resistance
definitions. With complex ballots, there may also be a danger of ""Italian""
style (aka ""signature"") attacks: the coercer demands the voter cast a ballot
with a specific, identifying pattern. Here we propose an approach to tallying
end-to-end verifiable schemes that avoids revealing all the votes but still
achieves whatever confidence level in the announced result is desired. Now a
coerced voter can claim that the required vote must be amongst those that
remained shrouded. Our approach is based on the well-established notion of
Risk-Limiting Audits, but here applied to the tally rather than to the audit.
We show that this approach counters coercion threats arising in extreme tallies
and ""Italian"" attacks. We illustrate our approach by applying it to the Selene
scheme, and we extend the approach to Risk-Limiting Verification, where not all
vote trackers are revealed, thereby enhancing the coercion mitigation
properties of Selene.",1908.04947v1,cs.CR,2019-08-14 04:23:10+00:00,"[arxiv.Result.Author('Wojciech Jamroga'), arxiv.Result.Author('Peter B. Roenne'), arxiv.Result.Author('Peter Y. A. Ryan'), arxiv.Result.Author('Philip B. Stark')]",
111,"Exact, Efficient and Information-Theoretically Secure Voting with an Arbitrary Number of Cheaters","We present three voting protocols with unconditional privacy and correctness,
without assuming any bound on the number of corrupt participants. All protocols
have polynomial complexity and require private channels and a simultaneous
broadcast channel. Unlike previously proposed protocols in this model, the
protocols that we present deterministically output the exact tally. Our first
protocol is a basic voting scheme which allows voters to interact in order to
compute the tally. Privacy of the ballot is unconditional in the sense that
regardless of the behavior of the dishonest participants nothing can be learned
through the protocol that could not be learned in an ideal realisation.
Unfortunately, a single dishonest participant can make the protocol abort, in
which case the dishonest participants can nevertheless learn the outcome of the
tally. Our second protocol introduces voting authorities which improves the
communication complexity by limiting interaction to be only between voters and
authorities and among the authorities themselves; the simultaneous broadcast is
also limited to the authorities. In the second protocol, as long as a single
authority is honest, the privacy is unconditional, however, a single corrupt
authority or a single corrupt voter can cause the protocol to abort. Our final
protocol provides a safeguard against corrupt voters by enabling a verification
technique to allow the authorities to revoke incorrect votes without aborting
the protocol. Finally, we discuss the implementation of a simultaneous
broadcast channel with the use of temporary computational assumptions, yielding
versions of our protocols that achieve everlasting security.",1011.5242v1,cs.CR,2010-11-23 21:33:50+00:00,"[arxiv.Result.Author('Anne Broadbent'), arxiv.Result.Author('Stacey Jeffery'), arxiv.Result.Author('Alain Tapp')]",
112,A Novel Hybrid Biometric Electronic Voting System: Integrating Finger Print and Face Recognition,"A novel hybrid design based electronic voting system is proposed, implemented
and analyzed. The proposed system uses two voter verification techniques to
give better results in comparison to single identification based systems.
Finger print and facial recognition based methods are used for voter
identification. Cross verification of a voter during an election process
provides better accuracy than single parameter identification method. The
facial recognition system uses Viola-Jones algorithm along with rectangular
Haar feature selection method for detection and extraction of features to
develop a biometric template and for feature extraction during the voting
process. Cascaded machine learning based classifiers are used for comparing the
features for identity verification using GPCA (Generalized Principle Component
Analysis) and K-NN (K-Nearest Neighbor). It is accomplished through comparing
the Eigen-vectors of the extracted features with the biometric template
pre-stored in the election regulatory body database. The results of the
proposed system show that the proposed cascaded design based system performs
better than the systems using other classifiers or separate schemes i.e. facial
or finger print based schemes. The proposed system will be highly useful for
real time applications due to the reason that it has 91% accuracy under nominal
light in terms of facial recognition. with bags of paper votes. The central
station compiles and publishes the names of winners and losers through
television and radio stations. This method is useful only if the whole process
is completed in a transparent way. However, there are some drawbacks to this
system. These include higher expenses, longer time to complete the voting
process, fraudulent practices by the authorities administering elections as
well as malpractices by the voters [1]. These challenges result in manipulated
election results.",1801.02430v1,cs.CR,2018-01-05 07:57:29+00:00,"[arxiv.Result.Author('Shahram Najam Syed'), arxiv.Result.Author('Aamir Zeb Shaikh'), arxiv.Result.Author('Shabbar Naqvi')]","Mehran University Research Journal of Engineering and Technology,
  Mehran University Research Journal of Engineering and Technology, 2018, 37
  (1), pp.59-68.
  http://publications.muet.edu.pk/index.php/muetrj/article/view/100/50"
113,SOBA: Secrecy-preserving Observable Ballot-level Audit,"SOBA is an approach to election verification that provides observers with
justifiably high confidence that the reported results of an election are
consistent with an audit trail (""ballots""), which can be paper or electronic.
SOBA combines three ideas: (1) publishing cast vote records (CVRs) separately
for each contest, so that anyone can verify that each reported contest outcome
is correct, if the CVRs reflect voters' intentions with sufficient accuracy;
(2) shrouding a mapping between ballots and the CVRs for those ballots to
prevent the loss of privacy that could occur otherwise; (3) assessing the
accuracy with which the CVRs reflect voters' intentions for a collection of
contests while simultaneously assessing the integrity of the shrouded mapping
between ballots and CVRs by comparing randomly selected ballots to the CVRs
that purport to represent them. Step (1) is related to work by the Humboldt
County Election Transparency Project, but publishing CVRs separately for
individual contests rather than images of entire ballots preserves privacy.
Step (2) requires a cryptographic commitment from elections officials.
Observers participate in step (3), which relies on the ""super-simple
simultaneous single-ballot risk-limiting audit."" Step (3) is designed to reveal
relatively few ballots if the shrouded mapping is proper and the CVRs
accurately reflect voter intent. But if the reported outcomes of the contests
differ from the outcomes that a full hand count would show, step (3) is
guaranteed to have a large chance of requiring all the ballots to be counted by
hand, thereby limiting the risk that an incorrect outcome will become official
and final.",1105.5803v2,stat.AP,2011-05-29 17:00:20+00:00,"[arxiv.Result.Author('Josh Benaloh'), arxiv.Result.Author('Douglas Jones'), arxiv.Result.Author('Eric Lazarus'), arxiv.Result.Author('Mark Lindeman'), arxiv.Result.Author('Philip B. Stark')]",
114,Towards Verifiable Remote Voting with Paper Assurance,"We propose a protocol for verifiable remote voting with paper assurance. It
is intended to augment existing postal voting procedures, allowing a ballot to
be electronically constructed, printed on paper, then returned in the post. It
allows each voter to verify that their vote has been correctly cast, recorded
and tallied by the Electoral Commission. The system is not end-to-end
verifiable, but does allow voters to detect manipulation by an adversary who
controls either the voting device, or (the postal service and electoral
commission) but not both. The protocol is not receipt-free, but if the client
honestly follows the protocol (including possibly remembering everything), they
cannot subsequently prove how they voted. Our proposal is the first to combine
plain paper assurance with cryptographic verification in a (passively)
receipt-free manner.",2111.04210v2,cs.CR,2021-11-08 00:35:07+00:00,"[arxiv.Result.Author('Eleanor McMurtry'), arxiv.Result.Author('Xavier Boyen'), arxiv.Result.Author('Chris Culnane'), arxiv.Result.Author('Kristian Gjøsteen'), arxiv.Result.Author('Thomas Haines'), arxiv.Result.Author('Vanessa Teague')]",
115,The New South Wales iVote System: Security Failures and Verification Flaws in a Live Online Election,"In the world's largest-ever deployment of online voting, the iVote Internet
voting system was trusted for the return of 280,000 ballots in the 2015 state
election in New South Wales, Australia. During the election, we performed an
independent security analysis of parts of the live iVote system and uncovered
severe vulnerabilities that could be leveraged to manipulate votes, violate
ballot privacy, and subvert the verification mechanism. These vulnerabilities
do not seem to have been detected by the election authorities before we
disclosed them, despite a pre-election security review and despite the system
having run in a live state election for five days. One vulnerability, the
result of including analytics software from an insecure external server,
exposed some votes to complete compromise of privacy and integrity. At least
one parliamentary seat was decided by a margin much smaller than the number of
votes taken while the system was vulnerable. We also found protocol flaws,
including vote verification that was itself susceptible to manipulation. This
incident underscores the difficulty of conducting secure elections online and
carries lessons for voters, election officials, and the e-voting research
community.",1504.05646v2,cs.CR,2015-04-22 03:42:36+00:00,"[arxiv.Result.Author('J. Alex Halderman'), arxiv.Result.Author('Vanessa Teague')]",
116,Voting Theory in the Lean Theorem Prover,"There is a long tradition of fruitful interaction between logic and social
choice theory. In recent years, much of this interaction has focused on
computer-aided methods such as SAT solving and interactive theorem proving. In
this paper, we report on the development of a framework for formalizing voting
theory in the Lean theorem prover, which we have applied to verify properties
of a recently studied voting method. While previous applications of interactive
theorem proving to social choice (using Isabelle/HOL and Mizar) have focused on
the verification of impossibility theorems, we aim to cover a variety of
results ranging from impossibility theorems to the verification of properties
of specific voting methods (e.g., Condorcet consistency, independence of
clones, etc.). In order to formalize voting theoretic axioms concerning adding
or removing candidates and voters, we work in a variable-election setting whose
formalization makes use of dependent types in Lean.",2110.08453v1,cs.LO,2021-10-16 03:10:22+00:00,"[arxiv.Result.Author('Wesley H. Holliday'), arxiv.Result.Author('Chase Norman'), arxiv.Result.Author('Eric Pacuit')]",
117,Bisimulations for Verifying Strategic Abilities with an Application to the ThreeBallot Voting Protocol,"We propose a notion of alternating bisimulation for strategic abilities under
imperfect information. The bisimulation preserves formulas of ATL$^*$ for both
the {\em objective} and {\em subjective} variants of the state-based semantics
with imperfect information, which are commonly used in the modeling and
verification of multi-agent systems. Furthermore, we apply the theoretical
result to the verification of coercion-resistance in the ThreeBallot voting
system, a voting protocol that does not use cryptography. In particular, we
show that natural simplifications of an initial model of the protocol are in
fact bisimulations of the original model, and therefore satisfy the same
ATL$^*$ properties, including coercion-resistance. These simplifications allow
the model-checking tool MCMAS to terminate on models with a larger number of
voters and candidates, compared with the initial model.",2203.13692v1,cs.MA,2022-03-25 14:56:20+00:00,"[arxiv.Result.Author('Francesco Belardinelli'), arxiv.Result.Author('Rodica Condurache'), arxiv.Result.Author('Catalin Dima'), arxiv.Result.Author('Wojciech Jamroga'), arxiv.Result.Author('Michal Knapik')]",
118,"Polarization, Abstention, and the Median Voter Theorem","The median voter theorem has long been the default model of voter behavior
and candidate choice. While contemporary work on the distribution of political
opinion has emphasized polarization and an increasing gap between the ""left""
and the ""right"" in democracies, the median voter theorem presents a model of
anti-polarization: competing candidates move to the center of the ideological
distribution to maximize vote share, regardless of the underlying ideological
distribution of voters. These anti-polar results, however, largely depend on
the ""singled-peakedness"" of voter preferences, an assumption that is rapidly
loosing relevance in the age of polarization. This article presents a model of
voter choice that examines three potential mechanisms that can undermine this
finding: a relative cost of voting that deters voters who are sufficiently
indifferent to both candidates, ideologically motivated third-party
alternatives that attract extreme voters, and a bimodal distribution of voter
ideology. Under reasonable sets of conditions and empirically observed voter
opinion distributions, these mechanisms can be sufficient to cause
strategically-minded candidates to fail to converge to the center, or to even
become more polarized than their electorate.",2103.12847v1,physics.soc-ph,2021-03-23 21:14:22+00:00,"[arxiv.Result.Author('Matthew I. Jones'), arxiv.Result.Author('Antonio D. Sirianni'), arxiv.Result.Author('Feng Fu')]",
119,A Fault Tolerance Improved Majority Voter for TMR System Architectures,"For digital system designs, triple modular redundancy (TMR), which is a
3-tuple version of N-modular redundancy is widely preferred for many
mission-control and safety-critical applications. The TMR scheme involves
two-times duplication of the simplex system hardware, with a majority voter
ensuring correctness provided at least two out of three copies of the system
remain operational. Thus the majority voter plays a pivotal role in ensuring
the correct operation of the system. The fundamental assumption implicit in the
TMR scheme is that the majority voter does not become faulty, which may not
hold well for implementations based on latest technology nodes with dimensions
of the order of just tens of nanometers. To overcome the drawbacks of the
classical majority voter some new voter designs were put forward in the
literature with the aim of enhancing the fault tolerance. However, these voter
designs generally ensure the correct system operation in the presence of either
a faulty function module or the faulty voter, considered only in isolation.
Since multiple faults may no longer be excluded in the nanoelectronics regime,
simultaneous fault occurrences on both the function module and the voter should
be considered, and the fault tolerance of the voters have to be analyzed under
such a scenario. In this context, this article proposes a new fault-tolerant
majority voter which is found to be more robust to faults than the existing
voters in the presence of faults occurring internally and/or externally to the
voter. Moreover, the proposed voter features less power dissipation, delay, and
area metrics based on the simulation results obtained by using a 32/28nm CMOS
process.",1605.03771v2,cs.AR,2016-05-12 11:54:44+00:00,"[arxiv.Result.Author('P Balasubramanian'), arxiv.Result.Author('K Prasad')]","WSEAS Transactions on Circuits and Systems, vol. 15, Article #14,
  pp. 108-122, 2016"
120,Explicit Auditing,"The Calculus of Audited Units (CAU) is a typed lambda calculus resulting from
a computational interpretation of Artemov's Justification Logic under the
Curry-Howard isomorphism; it extends the simply typed lambda calculus by
providing audited types, inhabited by expressions carrying a trail of their
past computation history. Unlike most other auditing techniques, CAU allows the
inspection of trails at runtime as a first-class operation, with applications
in security, debugging, and transparency of scientific computation.
  An efficient implementation of CAU is challenging: not only do the sizes of
trails grow rapidly, but they also need to be normalized after every beta
reduction. In this paper, we study how to reduce terms more efficiently in an
untyped variant of CAU by means of explicit substitutions and explicit auditing
operations, finally deriving a call-by-value abstract machine.",1808.00486v1,cs.LO,2018-08-01 18:03:02+00:00,"[arxiv.Result.Author('Wilmer Ricciotti'), arxiv.Result.Author('James Cheney')]",
121,Reversibility and Composition of Rewriting in Hierarchies,"In this paper, we study how graph transformations based on sesqui-pushout
rewriting can be reversed and how the composition of rewrites can be
constructed. We illustrate how such reversibility and composition can be used
to design an audit trail system for individual graphs and graph hierarchies.
This provides us with a compact way to maintain the history of updates of an
object, including its multiple versions. The main application of the designed
framework is an audit trail of updates to knowledge represented by hierarchies
of graphs. Therefore, we introduce the notion of rule hierarchy that represents
a transformation of the entire hierarchy, study how rule hierarchies can be
applied to hierarchies and analyse the conditions under which this application
is reversible. We then present a theory for constructing the composition of
consecutive hierarchy rewrites. The prototype audit trail system for
transformations in hierarchies of simple graphs with attributes is implemented
as part of the ReGraph Python library.",2012.01661v1,cs.LO,2020-12-03 02:29:28+00:00,"[arxiv.Result.Author('Russ Harmer'), arxiv.Result.Author('Eugenia Oshurko')]","EPTCS 330, 2020, pp. 145-162"
122,"Electryo, In-person Voting with Transparent Voter Verifiability and Eligibility Verifiability","Selene is an e-voting protocol that allows voters to directly check their
individual vote, in cleartext, in the final tally via a tracker system, while
providing good coercion mitigation. This is in contrast to conventional,
end-to-end verifiable schemes in which the voter verifies the presence of an
encryption of her vote on the bulletin board. The Selene mechanism can be
applied to many e-voting schemes, but here we present an application to the
polling station context, resulting in a voter-verifiable electronic tally with
a paper audit trail. The system uses a smartcard-based public key system to
provide the individual verification and universal eligibility verifiability.
The paper record contains an encrypted link to the voter's identity, requiring
stronger assumptions on ballot privacy than normal paper voting, but with the
benefit of providing good auditability and dispute resolution as well as
supporting (comparison) risk limiting audits.",2105.14783v1,cs.CR,2021-05-31 08:22:29+00:00,"[arxiv.Result.Author('Peter B. Roenne'), arxiv.Result.Author('Peter Y. A Ryan'), arxiv.Result.Author('Marie-Laure Zollinger')]",
123,"STAR-Vote: A Secure, Transparent, Auditable, and Reliable Voting System","In her 2011 EVT/WOTE keynote, Travis County, Texas County Clerk Dana
DeBeauvoir described the qualities she wanted in her ideal election system to
replace their existing DREs. In response, in April of 2012, the authors,
working with DeBeauvoir and her staff, jointly architected STAR-Vote, a voting
system with a DRE-style human interface and a ""belt and suspenders"" approach to
verifiability. It provides both a paper trail and end-to-end cryptography using
COTS hardware. It is designed to support both ballot-level risk-limiting
audits, and auditing by individual voters and observers. The human interface
and process flow is based on modern usability research. This paper describes
the STAR-Vote architecture, which could well be the next-generation voting
system for Travis County and perhaps elsewhere.",1211.1904v1,cs.CR,2012-11-08 17:06:33+00:00,"[arxiv.Result.Author('Josh Benaloh'), arxiv.Result.Author('Mike Byrne'), arxiv.Result.Author('Philip Kortum'), arxiv.Result.Author('Neal McBurnett'), arxiv.Result.Author('Olivier Pereira'), arxiv.Result.Author('Philip B. Stark'), arxiv.Result.Author('Dan S. Wallach')]",
124,Auditing Indian Elections,"Indian Electronic Voting Machines (EVMs) will be fitted with printers that
produce Voter-Verifiable Paper Audit Trails (VVPATs) in time for the 2019
general election. VVPATs provide evidence that each vote was recorded as the
voter intended, without having to trust the perfection or security of the EVMs.
  However, confidence in election results requires more: VVPATs must be
preserved inviolate and then actually used to check the reported election
result in a trustworthy way that the public can verify. A full manual tally
from the VVPATs could be prohibitively expensive and time-consuming; moreover,
it is difficult for the public to determine whether a full hand count was
conducted accurately. We show how Risk-Limiting Audits (RLAs) could provide
high confidence in Indian election results. Compared to full hand recounts,
RLAs typically require manually inspecting far fewer VVPATs when the outcome is
correct, and are much easier for the electorate to observe in adequate detail
to determine whether the result is trustworthy.",1901.03108v2,cs.CR,2019-01-10 11:38:41+00:00,"[arxiv.Result.Author('Vishal Mohanty'), arxiv.Result.Author('Nicholas Akinyokun'), arxiv.Result.Author('Andrew Conway'), arxiv.Result.Author('Chris Culnane'), arxiv.Result.Author('Philip B. Stark'), arxiv.Result.Author('Vanessa Teague')]",
125,Effcient logging and querying for Blockchain-based cross-site genomic dataset access audit,"Background: Genomic data have been collected by different institutions and
companies and need to be shared for broader use. In a cross-site genomic data
sharing system, a secure and transparent access control audit module plays an
essential role in ensuring the accountability. The 2018 iDASH competition first
track provides us with an opportunity to design efficient logging and querying
system for cross-site genomic dataset access audit. We designed a
blockchain-based log system which can provide a light-weight and widely
compatible module for existing blockchain platforms. The submitted solution won
the third place of the competition. In this paper, we report the technical
details in our system. Methods: We present two methods: baseline method and
enhanced method. We started with the baseline method and then adjusted our
implementation based on the competition evaluation criteria and characteristics
of the log system. To overcome obstacles of indexing on the immutable
Blockchain system, we designed a hierarchical timestamp structure which
supports efficient range queries on the timestamp field. Results: We
implemented our methods in Python3, tested the scalability, and compared the
performance using the test data supplied by competition organizer. We
successfully boosted the log retrieval speed for complex AND queries that
contain multiple predicates. For the range query, we boosted the speed for at
least one order of magnitude. The storage usage is reduced by 25%. Conclusion:
We demonstrate that Blockchain can be used to build a time and space efficient
log and query genomic dataset audit trail. Therefore, it provides a promising
solution for sharing genomic data with accountability requirement across
multiple sites.",1907.07303v2,cs.DB,2019-07-17 02:11:53+00:00,"[arxiv.Result.Author('Shuaicheng Ma'), arxiv.Result.Author('Yang Cao'), arxiv.Result.Author('Li Xiong')]",
126,A Forensic Audit of the Tor Browser Bundle,"The increasing use of encrypted data within file storage and in network
communications leaves investigators with many challenges. One of the most
challenging is the Tor protocol, as its main focus is to protect the privacy of
the user, in both its local footprint within a host and over a network
connection. The Tor browser, though, can leave behind digital artefacts which
can be used by an investigator. This paper outlines an experimental methodology
and provides results for evidence trails which can be used within real-life
investigations.",1907.10279v1,cs.CR,2019-07-24 07:39:16+00:00,"[arxiv.Result.Author('Matt Muir'), arxiv.Result.Author('Petra Leimich'), arxiv.Result.Author('William J Buchanan')]","Digital Investigation, Volume 29, June 2019, Pages 118-128"
127,Continuous Design Control for Machine Learning in Certified Medical Systems,"Continuous software engineering has become commonplace in numerous fields.
However, in regulating intensive sectors, where additional concerns needs to be
taken into account, it is often considered difficult to apply continuous
development approaches, such as devops. In this paper, we present an approach
for using pull requests as design controls, and apply this approach to machine
learning in certified medical systems leveraging model cards, a novel technique
developed to add explainability to machine learning systems, as a regulatory
audit trail. The approach is demonstrated with an industrial system that we
have used previously to show how medical systems can be developed in a
continuous fashion.",2209.05843v1,cs.SE,2022-09-13 09:42:04+00:00,"[arxiv.Result.Author('Vlad Stirbu'), arxiv.Result.Author('Tuomas Granlund'), arxiv.Result.Author('Tommi Mikkonen')]",
128,Is Decentralized AI Safer?,"Artificial Intelligence (AI) has the potential to significantly benefit or
harm humanity. At present, a few for-profit companies largely control the
development and use of this technology, and therefore determine its outcomes.
In an effort to diversify and democratize work on AI, various groups are
building open AI systems, investigating their risks, and discussing their
ethics. In this paper, we demonstrate how blockchain technology can facilitate
and formalize these efforts. Concretely, we analyze multiple use-cases for
blockchain in AI research and development, including decentralized governance,
the creation of immutable audit trails, and access to more diverse and
representative datasets. We argue that decentralizing AI can help mitigate AI
risks and ethical concerns, while also introducing new issues that should be
considered in future work.",2211.05828v1,cs.CY,2022-11-04 01:01:31+00:00,"[arxiv.Result.Author('Casey Clifton'), arxiv.Result.Author('Richard Blythman'), arxiv.Result.Author('Kartika Tulusan')]",
129,An AI-Powered VVPAT Counter for Elections in India,"The Election Commission of India has introduced Voter Verified Paper Audit
Trail since 2019. This mechanism has increased voter confidence at the time of
casting the votes. However, physical verification of the VVPATs against the
party level counts from the EVMs is done only in 5 (randomly selected) machines
per constituency. The time required to conduct physical verification becomes a
bottleneck in scaling this activity for 100% of machines in all constituencies.
We proposed an automated counter powered by image processing and machine
learning algorithms to speed up the process and address this issue.",2212.11124v1,cs.CV,2022-12-09 14:59:40+00:00,"[arxiv.Result.Author('Prasath Murugesan'), arxiv.Result.Author('Shamshu Dharwez Saganvali')]",
130,"Algorithmic audits of algorithms, and the law","Algorithmic decision making is now widespread, ranging from health care
allocation to more common actions such as recommendation or information
ranking. The aim to audit these algorithms has grown alongside. In this paper,
we focus on external audits that are conducted by interacting with the user
side of the target algorithm, hence considered as a black box. Yet, the legal
framework in which these audits take place is mostly ambiguous to researchers
developing them: on the one hand, the legal value of the audit outcome is
uncertain; on the other hand the auditors' rights and obligations are unclear.
The contribution of this paper is to articulate two canonical audit forms to
law, to shed light on these aspects: 1) the first audit form (we coin the Bobby
audit form) checks a predicate against the algorithm, while the second
(Sherlock) is more loose and opens up to multiple investigations. We find that:
Bobby audits are more amenable to prosecution, yet are delicate as operating on
real user data. This can lead to reject by a court (notion of admissibility).
Sherlock audits craft data for their operation, most notably to build
surrogates of the audited algorithm. It is mostly used for acts for
whistleblowing, as even if accepted as a proof, the evidential value will be
low in practice. 2) these two forms require the prior respect of a proper right
to audit, granted by law or by the platform being audited; otherwise the
auditor will be also prone to prosecutions regardless of the audit outcome.
This article thus highlights the relation of current audits with law, in order
to structure the growing field of algorithm auditing.",2203.03711v1,cs.CY,2022-02-15 14:20:53+00:00,"[arxiv.Result.Author('Erwan Le Merrer'), arxiv.Result.Author('Ronan Pons'), arxiv.Result.Author('Gilles Trédan')]",
131,The Minimum Hybrid Contract (MHC): Combining legal and blockchain smart contracts,"Corruption is a major global financial problem with billions of dollars
rendered lost or unaccountable annually. Corruption through contract fraud is
often conducted by withholding and/or altering financial information. When such
scandals are investigated by authorities, financial and legal documents are
usually altered to conceal the paper trail.
  Smart contracts have emerged in recent years and appear promising for
applications such as legal contracts where transparency is critical and of
public interest. Transparency and auditability are inherent because smart
contracts execute operations on the blockchain, a distributed public ledger.
  In this paper, we propose the Minimum Hybrid Contract (MHC), with the aim of
introducing 1) auditability, 2) transparency, and 3) immutability to the
contract's financial transactions. The MHC comprises an online smart contract
and an offline traditional legal contract. where the two are immutably linked.
  Secure peer-to-peer financial transactions, transparency, and cost accounting
are automated by the smart contract, and legal issues or disputes are carried
out by civil courts. The reliance on established legal processes facilitates an
appropriate adoption of smart contracts in traditional contracts.",2002.06850v1,cs.CY,2020-02-17 09:19:20+00:00,"[arxiv.Result.Author('Jørgen Svennevik Notland'), arxiv.Result.Author('Jakob Svennevik Notland'), arxiv.Result.Author('Donn Morrison')]",
132,Towards Privacy-assured and Lightweight On-chain Auditing of Decentralized Storage,"How to audit outsourced data in centralized storage like cloud is
well-studied, but it is largely under-explored for the rising decentralized
storage network (DSN) that bodes well for a billion-dollar market. To realize
DSN as a usable service in a truly decentralized manner, the blockchain comes
in handy -- to record and verify audit trails in forms of proof of storage, and
based on that, to handle fair payments with necessary dispute resolution.
  Leaving the audit trails on the blockchain offers transparency and fairness,
yet it 1) sacrifices privacy, as they may leak information about the data under
audit, and 2) overwhelms on-chain resources, as they may be practically large
in size and expensive to verify. Prior auditing designs in centralized settings
are not directly applicable here. A handful of proposals targeting DSN cannot
satisfactorily address these issues either.
  We present an auditing solution that addresses on-chain privacy and
efficiency, from a synergy of homomorphic linear authenticators with polynomial
commitments for succinct proofs, and the sigma protocol for provable privacy.
The solution results in, per audit, 288-byte proof written to the blockchain,
and constant verification cost. It can sustain long-term operation and easily
scale to thousands of users on Ethereum.",2005.05531v3,cs.CR,2020-05-12 03:14:09+00:00,"[arxiv.Result.Author('Yuefeng Du'), arxiv.Result.Author('Huayi Duan'), arxiv.Result.Author('Anxin Zhou'), arxiv.Result.Author('Cong Wang'), arxiv.Result.Author('Man Ho Au'), arxiv.Result.Author('Qian Wang')]",
133,SOBA: Secrecy-preserving Observable Ballot-level Audit,"SOBA is an approach to election verification that provides observers with
justifiably high confidence that the reported results of an election are
consistent with an audit trail (""ballots""), which can be paper or electronic.
SOBA combines three ideas: (1) publishing cast vote records (CVRs) separately
for each contest, so that anyone can verify that each reported contest outcome
is correct, if the CVRs reflect voters' intentions with sufficient accuracy;
(2) shrouding a mapping between ballots and the CVRs for those ballots to
prevent the loss of privacy that could occur otherwise; (3) assessing the
accuracy with which the CVRs reflect voters' intentions for a collection of
contests while simultaneously assessing the integrity of the shrouded mapping
between ballots and CVRs by comparing randomly selected ballots to the CVRs
that purport to represent them. Step (1) is related to work by the Humboldt
County Election Transparency Project, but publishing CVRs separately for
individual contests rather than images of entire ballots preserves privacy.
Step (2) requires a cryptographic commitment from elections officials.
Observers participate in step (3), which relies on the ""super-simple
simultaneous single-ballot risk-limiting audit."" Step (3) is designed to reveal
relatively few ballots if the shrouded mapping is proper and the CVRs
accurately reflect voter intent. But if the reported outcomes of the contests
differ from the outcomes that a full hand count would show, step (3) is
guaranteed to have a large chance of requiring all the ballots to be counted by
hand, thereby limiting the risk that an incorrect outcome will become official
and final.",1105.5803v2,stat.AP,2011-05-29 17:00:20+00:00,"[arxiv.Result.Author('Josh Benaloh'), arxiv.Result.Author('Douglas Jones'), arxiv.Result.Author('Eric Lazarus'), arxiv.Result.Author('Mark Lindeman'), arxiv.Result.Author('Philip B. Stark')]",
134,Methods and Techniques of Quality Management for ICT Audit Processes,"In modern organizations, Information and Communication Technologies are used
to support the organizations' activities. To manage the quality of the
organization processes, audit processes are implemented. Also, the audit
processes can aim the quality of ICT systems themselves because their
involvement in organization processes. The paper investigates the ways in which
a quality management can be applied for audit processes in order to obtain a
high level of quality for the audit recommendations.",1201.0395v1,cs.OH,2012-01-01 22:40:23+00:00,[arxiv.Result.Author('Marius Popa')],
135,Verifiable Elections with Commitment Consistent Encryption -- A Primer,"This note provides an introduction to the PPATS Commitment Consistent
Encryption (CCE) scheme proposed by Cuvelier, Pereira and Peters and its use in
the design of end-to-end verifiable elections with a perfectly private audit
trail. These elections can be verified using audit data that will never leak
any information about the vote, even if all the private keys of the elections
are compromised, or if the cryptographic assumptions are broken.",1412.7358v1,cs.CR,2014-12-23 13:35:21+00:00,[arxiv.Result.Author('Olivier Pereira')],
136,Auditable Restoration of Distributed Programs,"We focus on a protocol for auditable restoration of distributed systems. The
need for such protocol arises due to conflicting requirements (e.g., access to
the system should be restricted but emergency access should be provided). One
can design such systems with a tamper detection approach (based on the
intuition of ""break the glass door""). However, in a distributed system, such
tampering, which are denoted as auditable events, is visible only for a single
node. This is unacceptable since the actions they take in these situations can
be different than those in the normal mode. Moreover, eventually, the auditable
event needs to be cleared so that system resumes the normal operation.
  With this motivation, in this paper, we present a protocol for auditable
restoration, where any process can potentially identify an auditable event.
Whenever a new auditable event occurs, the system must reach an ""auditable
state"" where every process is aware of the auditable event. Only after the
system reaches an auditable state, it can begin the operation of restoration.
Although any process can observe an auditable event, we require that only
""authorized"" processes can begin the task of restoration. Moreover, these
processes can begin the restoration only when the system is in an auditable
state. Our protocol is self-stabilizing and has bounded state space. It can
effectively handle the case where faults or auditable events occur during the
restoration protocol. Moreover, it can be used to provide auditable restoration
to other distributed protocol.",1506.07957v1,cs.DC,2015-06-26 04:52:24+00:00,"[arxiv.Result.Author('Reza Hajisheykhi'), arxiv.Result.Author('Mohammad Roohitavaf'), arxiv.Result.Author('Sandeep Kulkarni')]",
137,Non(c)esuch Ballot-Level Risk-Limiting Audits for Precinct-Count Voting Systems,"Risk-limiting audits (RLAs) guarantee a high probability of correcting
incorrect reported outcomes before the outcomes are certified. The most
efficient use ballot-level comparison, comparing the voting system's
interpretation of individual ballot cards sampled at random (cast-vote records,
CVRs) from a trustworthy paper trail to a human interpretation of the same
cards. Such comparisons require the voting system to create and export CVRs in
a way that can be linked to the individual ballots the CVRs purport to
represent. Such links can be created by keeping the ballots in the order in
which they are scanned or by printing a unique serial number on each ballot.
But for precinct-count systems (PCOS), these strategies may compromise vote
anonymity: the order in which ballots are cast may identify the voters who cast
them. Printing a unique pseudo-random number (""cryptographic nonce"") on each
ballot card after the voter last touches it could reduce such privacy risks.
But what if the system does not in fact print a unique number on each ballot or
does not accurately report the numbers it printed? This paper gives two ways to
conduct an RLA so that even if the system does not print a genuine nonce on
each ballot or misreports the nonces it used, the audit's risk limit is not
compromised (however, the anonymity of votes might be compromised). One method
allows untrusted technology to be used to imprint and to retrieve ballot cards.
The method is adaptive: if the technology behaves properly, this protection
does not increase the audit workload. But if the imprinting or retrieval system
misbehaves, the sample size the RLA requires to confirm the reported results
when the results are correct is generally larger than if the imprinting and
retrieval were accurate.",2207.01362v1,cs.CR,2022-07-04 12:35:42+00:00,[arxiv.Result.Author('Philip B. Stark')],
138,Next Steps for the Colorado Risk-Limiting Audit (CORLA) Program,"Colorado conducted risk-limiting tabulation audits (RLAs) across the state in
2017, including both ballot-level comparison audits and ballot-polling audits.
Those audits only covered contests restricted to a single county; methods to
efficiently audit contests that cross county boundaries and combine ballot
polling and ballot-level comparisons have not been available.
  Colorado's current audit software (RLATool) needs to be improved to audit
these contests that cross county lines and to audit small contests efficiently.
  This paper addresses these needs. It presents extremely simple but
inefficient methods, more efficient methods that combine ballot polling and
ballot-level comparisons using stratified samples, and methods that combine
ballot-level comparison and variable-size batch comparison audits in a way that
does not require stratified sampling.
  We conclude with some recommendations, and illustrate our recommended method
using examples that compare them to existing approaches. Exemplar open-source
code and interactive Jupyter notebooks are provided that implement the methods
and allow further exploration.",1803.00698v1,stat.AP,2018-03-02 03:46:37+00:00,"[arxiv.Result.Author('Mark Lindeman'), arxiv.Result.Author('Neal McBurnett'), arxiv.Result.Author('Kellie Ottoboni'), arxiv.Result.Author('Philip B. Stark')]",
139,A note on efficient audit sample selection,"Auditing is a widely used method for quality improvement, and many guidelines
are available advising on how to draw samples for auditing. However,
researchers or auditors sometimes find themselves in situations that are not
straightforward and the standard sampling techniques are not sufficient, for
example when a selective sample has initially been audited and the auditor
desires to re-use as many cases as possible from this initial audit in a new
audit sample that is representative with respect to some background
characteristics. In this paper, we introduce a method that selects an audit
sample that re-uses initially audited cases by considering the selection of a
representative audit sample as a constrained minimization problem. In addition,
we evaluate the performance of this method by means of a simulation study and
we apply the method to draw an audit sample of establishments to evaluate the
quality of an establishment registry used to produce statistics on energy
consumption per type of economic activity.",2105.10737v1,stat.ME,2021-05-22 14:23:36+00:00,"[arxiv.Result.Author('Laura Boeschoten'), arxiv.Result.Author('Sander Scholtus'), arxiv.Result.Author('Arnout van Delden')]",
140,Can Voters Detect Errors on Their Printed Ballots? Absolutely,"There is still debate on whether voters can detect malicious changes in their
printed ballot after making their selections on a Ballot Marking Device (BMD).
In this study, we altered votes on a voter's ballot after they had made their
selections on a BMD. We then required them to examine their ballots for any
changes from the slate they used to vote. Overall accuracy was exceptionally
high. Participants saw 1440 total contests, and of those 1440, there were a
total of 4 errors, so total accuracy was 99.8%. Participants were able to
perform with near-perfect accuracy regardless of ballot length, ballot type,
number of altered races, and location of altered races. Detection performance
was extremely robust. We conclude that with proper direction and resources,
voters can be near-perfect detectors of ballot changes on printed paper ballots
after voting with a BMD. This finding has significant implications for the
voting community as BMD use continues to grow. Research should now focus on
identifying administrative and behavioral methods that will prompt and
encourage voters to check their BMD-generated ballots before they drop them in
the ballot box.",2204.09780v1,cs.HC,2022-04-20 20:40:32+00:00,"[arxiv.Result.Author('Philip Kortum'), arxiv.Result.Author('Michael D. Byrne'), arxiv.Result.Author('Chidera O. Azubike'), arxiv.Result.Author('Laura E. Roty')]",
141,SOBA: Secrecy-preserving Observable Ballot-level Audit,"SOBA is an approach to election verification that provides observers with
justifiably high confidence that the reported results of an election are
consistent with an audit trail (""ballots""), which can be paper or electronic.
SOBA combines three ideas: (1) publishing cast vote records (CVRs) separately
for each contest, so that anyone can verify that each reported contest outcome
is correct, if the CVRs reflect voters' intentions with sufficient accuracy;
(2) shrouding a mapping between ballots and the CVRs for those ballots to
prevent the loss of privacy that could occur otherwise; (3) assessing the
accuracy with which the CVRs reflect voters' intentions for a collection of
contests while simultaneously assessing the integrity of the shrouded mapping
between ballots and CVRs by comparing randomly selected ballots to the CVRs
that purport to represent them. Step (1) is related to work by the Humboldt
County Election Transparency Project, but publishing CVRs separately for
individual contests rather than images of entire ballots preserves privacy.
Step (2) requires a cryptographic commitment from elections officials.
Observers participate in step (3), which relies on the ""super-simple
simultaneous single-ballot risk-limiting audit."" Step (3) is designed to reveal
relatively few ballots if the shrouded mapping is proper and the CVRs
accurately reflect voter intent. But if the reported outcomes of the contests
differ from the outcomes that a full hand count would show, step (3) is
guaranteed to have a large chance of requiring all the ballots to be counted by
hand, thereby limiting the risk that an incorrect outcome will become official
and final.",1105.5803v2,stat.AP,2011-05-29 17:00:20+00:00,"[arxiv.Result.Author('Josh Benaloh'), arxiv.Result.Author('Douglas Jones'), arxiv.Result.Author('Eric Lazarus'), arxiv.Result.Author('Mark Lindeman'), arxiv.Result.Author('Philip B. Stark')]",
142,Next Steps for the Colorado Risk-Limiting Audit (CORLA) Program,"Colorado conducted risk-limiting tabulation audits (RLAs) across the state in
2017, including both ballot-level comparison audits and ballot-polling audits.
Those audits only covered contests restricted to a single county; methods to
efficiently audit contests that cross county boundaries and combine ballot
polling and ballot-level comparisons have not been available.
  Colorado's current audit software (RLATool) needs to be improved to audit
these contests that cross county lines and to audit small contests efficiently.
  This paper addresses these needs. It presents extremely simple but
inefficient methods, more efficient methods that combine ballot polling and
ballot-level comparisons using stratified samples, and methods that combine
ballot-level comparison and variable-size batch comparison audits in a way that
does not require stratified sampling.
  We conclude with some recommendations, and illustrate our recommended method
using examples that compare them to existing approaches. Exemplar open-source
code and interactive Jupyter notebooks are provided that implement the methods
and allow further exploration.",1803.00698v1,stat.AP,2018-03-02 03:46:37+00:00,"[arxiv.Result.Author('Mark Lindeman'), arxiv.Result.Author('Neal McBurnett'), arxiv.Result.Author('Kellie Ottoboni'), arxiv.Result.Author('Philip B. Stark')]",
143,RAIRE: Risk-Limiting Audits for IRV Elections,"Risk-limiting post election audits guarantee a high probability of correcting
incorrect election results, independent of why the result was incorrect.
Ballot-polling audits select ballots at random and interpret those ballots as
evidence for and against the reported result, continuing this process until
either they support the recorded result, or they fall back to a full manual
recount. For elections with digitised scanning and counting of ballots, a
comparison audit compares randomly selected digital ballots with their paper
versions. Discrepancies are referred to as errors, and are used to build
evidence against or in support of the recorded result. Risk-limiting audits for
first-past-the-post elections are well understood, and used in some US
elections. We define a number of approaches to ballot-polling and comparison
risk-limiting audits for Instant Runoff Voting (IRV) elections. We show that
for almost all real elections we found, we can perform a risk-limiting audit by
looking at only a small fraction of the total ballots (assuming no errors were
made in the tallying and distribution of votes).",1903.08804v2,cs.DS,2019-03-20 06:19:10+00:00,"[arxiv.Result.Author('Michelle Blom'), arxiv.Result.Author('Peter J. Stuckey'), arxiv.Result.Author('Vanessa Teague')]",
144,"A decomposition of ballot permutations, pattern avoidance and Gessel walks","A permutation whose any prefix has no more descents than ascents is called a
ballot permutation. In this paper, we present a decomposition of ballot
permutations that enables us to construct a bijection between ballot
permutations and odd order permutations, which proves a set-valued extension of
a conjecture due to Spiro using the statistic of peak values. This bijection
also preserves the neighbors of the largest letter in permutations and thus
resolves a refinement of Spiro' s conjecture proposed by Wang and Zhang. Our
decomposition can be extended to well-labelled positive paths, a class of
generalized ballot permutations arising from polytope theory, that were
enumerated by Bernardi, Duplantier and Nadeau. We will also investigate the
enumerative aspect of ballot permutations avoiding a single pattern of length 3
and establish a connection between 213-avoiding ballot permutations and Gessel
walks.",2103.04599v1,math.CO,2021-03-08 08:37:08+00:00,"[arxiv.Result.Author('Zhicong Lin'), arxiv.Result.Author('David G. L. Wang'), arxiv.Result.Author('Tongyuan Zhao')]",
145,Ballot Length in Instant Runoff Voting,"Instant runoff voting (IRV) is an increasingly-popular alternative to
traditional plurality voting in which voters submit rankings over the
candidates rather than single votes. In practice, elections using IRV often
restrict the ballot length, the number of candidates a voter is allowed to rank
on their ballot. We theoretically and empirically analyze how ballot length can
influence the outcome of an election, given fixed voter preferences. We show
that there exist preference profiles over $k$ candidates such that up to $k-1$
different candidates win at different ballot lengths. We derive exact lower
bounds on the number of voters required for such profiles and provide a
construction matching the lower bound for unrestricted voter preferences.
Additionally, we characterize which sequences of winners are possible over
ballot lengths and provide explicit profile constructions achieving any
feasible winner sequence. We also examine how classic preference restrictions
influence our results--for instance, single-peakedness makes $k-1$ different
winners impossible but still allows at least $\Omega(\sqrt k)$. Finally, we
analyze a collection of 168 real-world elections, where we truncate rankings to
simulate shorter ballots. We find that shorter ballots could have changed the
outcome in one quarter of these elections. Our results highlight ballot length
as a consequential degree of freedom in the design of IRV elections.",2207.08958v3,cs.MA,2022-07-18 22:01:13+00:00,"[arxiv.Result.Author('Kiran Tomlinson'), arxiv.Result.Author('Johan Ugander'), arxiv.Result.Author('Jon Kleinberg')]",
146,Ballot Paths Avoiding Depth Zero Patterns,"In a paper by Sapounakis, Tasoulas, and Tsikouras \cite{stt}, the authors
count the number of occurrences of patterns of length four in Dyck paths. In
this paper we specify in one direction and generalize in another. We only count
ballot paths that avoid a given pattern, where a ballot path stays weakly above
the diagonal $y=x$, starts at the origin, and takes steps from the set
$\{\uparrow ,\to \}=\{u,r\}$. A pattern is a finite string made from the same
step set; it is also a path. Notice that a ballot path ending at a point along
the diagonal is a Dyck path.",1004.2710v1,math.CO,2010-04-15 20:23:45+00:00,"[arxiv.Result.Author('Heinrich Niederhausen'), arxiv.Result.Author('Shaun Sullivan')]","Journal OF Combinatorial Mathematics and Combinatorial Computing,
  August 2010"
147,Blind proxy voting,"A secret ballot mechanism that enables voting in absence is proposed. It
amends standard vote collection methods that use ballot box as anonymizer,
adding the option for absent voters to vote by a proxy blinded to the content
of the ballot paper. Votes are cast under unique and hidden identification
numbers generated solely for the purpose of that election. Voters prepare their
ballot papers from scratch and submit them to the tallying authority in two
parts via separate routes, each part being meaningless without the other.",1812.11128v1,cs.CR,2018-12-28 17:41:13+00:00,[arxiv.Result.Author('Zuzana Haniková')],
148,Nice Bounds for the Generalized Ballot Problem,"This paper gives two sharp bounds for the generalized ballot problem with
candidate A receiving at least \mu times as candidate B for an arbitrary real
number \mu.",0912.1999v1,math.CO,2009-12-10 14:11:52+00:00,[arxiv.Result.Author('Delong Meng')],
149,A quantum secret ballot,"The paper concerns the protection of the secrecy of ballots, so that the
identity of the voters cannot be matched with their vote. To achieve this we
use an entangled quantum state to represent the ballots. Each ballot includes
the identity of the voter, explicitly marked on the ""envelope"" containing it.
Measuring the content of the envelope yields a random number which reveals no
information about the vote. However, the outcome of the elections can be
unambiguously decided after adding the random numbers from all envelopes. We
consider a few versions of the protocol and their complexity of implementation.",quant-ph/0602087v2,quant-ph,2006-02-09 21:03:35+00:00,"[arxiv.Result.Author('Shahar Dolev'), arxiv.Result.Author('Itamar Pitowsky'), arxiv.Result.Author('Boaz Tamir')]",
150,On the security of ballot marking devices,"A recent debate among election experts has considered whether electronic
ballot marking devices (BMDs) have adequate security against the risks of
malware. A malicious BMD might produce a printed ballot that disagrees with a
voter's actual intent, with the hope that voters would be unlikely to detect
this subterfuge. This essay considers how an election administrator can create
reasonable auditing procedures to gain confidence that their fleet of BMDs is
operating correctly, allowing voters to benefit from the usability and
accessibility features of BMDs while the overall election still benefits from
the same security and reliability properties we expect from hand-marked paper
ballots.",1908.01897v2,cs.CR,2019-08-05 23:04:16+00:00,[arxiv.Result.Author('Dan S. Wallach')],
151,Non(c)esuch Ballot-Level Risk-Limiting Audits for Precinct-Count Voting Systems,"Risk-limiting audits (RLAs) guarantee a high probability of correcting
incorrect reported outcomes before the outcomes are certified. The most
efficient use ballot-level comparison, comparing the voting system's
interpretation of individual ballot cards sampled at random (cast-vote records,
CVRs) from a trustworthy paper trail to a human interpretation of the same
cards. Such comparisons require the voting system to create and export CVRs in
a way that can be linked to the individual ballots the CVRs purport to
represent. Such links can be created by keeping the ballots in the order in
which they are scanned or by printing a unique serial number on each ballot.
But for precinct-count systems (PCOS), these strategies may compromise vote
anonymity: the order in which ballots are cast may identify the voters who cast
them. Printing a unique pseudo-random number (""cryptographic nonce"") on each
ballot card after the voter last touches it could reduce such privacy risks.
But what if the system does not in fact print a unique number on each ballot or
does not accurately report the numbers it printed? This paper gives two ways to
conduct an RLA so that even if the system does not print a genuine nonce on
each ballot or misreports the nonces it used, the audit's risk limit is not
compromised (however, the anonymity of votes might be compromised). One method
allows untrusted technology to be used to imprint and to retrieve ballot cards.
The method is adaptive: if the technology behaves properly, this protection
does not increase the audit workload. But if the imprinting or retrieval system
misbehaves, the sample size the RLA requires to confirm the reported results
when the results are correct is generally larger than if the imprinting and
retrieval were accurate.",2207.01362v1,cs.CR,2022-07-04 12:35:42+00:00,[arxiv.Result.Author('Philip B. Stark')],
152,Adaptive Risk-Limiting Ballot Comparison Audits,"Risk-limiting audits (RLAs) are rigorous statistical procedures meant to
detect invalid election results. RLAs examine paper ballots cast during the
election to statistically assess the possibility of a disagreement between the
winner determined by the ballots and the winner reported by tabulation. The
most ballot efficient approaches proceed by ""ballot comparison."" However,
ballot comparison requires an untrusted declaration of the contents of each
cast ballot, rather than a simple tabulation of vote totals. This ""cast-vote
record table"" (CVR) is then spot-checked against ballots for consistency. In
many practical settings, the cost of generating a suitable CVR dominates the
cost of conducting the audit, preventing widespread adoption of these
sample-efficient techniques.
  We introduce a new RLA procedure: an ""adaptive ballot comparison"" audit. In
this audit, a global CVR is never produced; instead, a three-stage procedure is
iterated:
  1) a batch is selected,
  2) a CVR is produced for that batch, and
  3) a ballot within the batch is sampled, inspected by auditors, and compared
with the CVR.
  We prove that such an audit can achieve risk commensurate with standard
comparison audits while generating a fraction of the CVR. We present three main
contributions:
  1) a formal adversarial model for RLAs;
  2) definition and analysis of an adaptive audit procedure with rigorous risk
limits and an associated correctness analysis accounting for the incidental
errors arising in typical audits; and
  3) an analysis of practical efficiency.
  This method can be organized in rounds (as is typical for comparison audits)
where sampled CVRs are produced in parallel. Using data from Florida's 2020
presidential election with 5% risk and 1% margin, only 22% of the CVR is
generated; at 10% margin, only 2% is generated.",2202.02607v5,cs.CR,2022-02-05 18:04:27+00:00,"[arxiv.Result.Author('Benjamin Fuller'), arxiv.Result.Author('Abigail Harrison'), arxiv.Result.Author('Alexander Russell')]",
153,On the Significance of Consecutive Ballots in Paxos,"In this paper we examine the Paxos protocol and demonstrate how the discrete
numbering of ballots can be leveraged to weaken the conditions for learning.
Specifically, we define the notion of consecutive ballots and use this to
define Consecutive Quorums. Consecutive Quorums weakens the learning criterion
such that a learner does not need matching $accept$ messages sent in the $same
\; ballot$ from a majority of acceptors to learn a value. We prove that this
modification preserves the original safety and liveness guarantees of Paxos. We
define $Consecutive \; Paxos$ which encapsulates the properties of discrete
consecutive ballots. To establish the correctness of these results, we, in
addition to a paper proof, formally verify the correctness of a State Machine
Replication Library built on top of an optimized version of Multi-Paxos
modified to reflect $Consecutive \; Paxos$.",2006.01885v1,cs.DC,2020-06-02 19:08:46+00:00,"[arxiv.Result.Author('Eli Goldweber'), arxiv.Result.Author('Nuda Zhang'), arxiv.Result.Author('Manos Kapritsos')]",
154,Identifying Possible Winners in Ranked Choice Voting Elections with Outstanding Ballots,"Several election districts in the US have recently moved to ranked-choice
voting (RCV) to decide the results of local elections. RCV allows voters to
rank their choices, and the results are computed in rounds, eliminating one
candidate at a time. RCV ensures fairer elections and has been shown to
increase elected representation of women and people of color. A main drawback
of RCV is that the round-by-round process requires all the ballots to be
tallied before the results of an election can be calculated. With increasingly
large portions of ballots coming from absentee voters, RCV election outcomes
are not always apparent on election night, and can take several weeks to be
published, leading to a loss of trust in the electoral process from the public.
In this paper, we present an algorithm for efficiently computing possible
winners of RCV elections from partially known ballots and evaluate it on data
from the recent New York City Primary elections. We show that our techniques
allow to significantly narrow down the field of possible election winners, and
in some case identify the winner as soon as election night despite a number of
yet-unaccounted absentee ballots, providing more transparency in the electoral
process.",2206.12741v1,cs.CY,2022-06-25 22:08:15+00:00,"[arxiv.Result.Author('Alborz Jelvani'), arxiv.Result.Author('Amélie Marian')]",
155,Risk-Limiting Audits by Stratified Union-Intersection Tests of Elections (SUITE),"Risk-limiting audits (RLAs) offer a statistical guarantee: if a full manual
tally of the paper ballots would show that the reported election outcome is
wrong, an RLA has a known minimum chance of leading to a full manual tally.
RLAs generally rely on random samples. Stratified sampling--partitioning the
population of ballots into disjoint strata and sampling independently from the
strata--may simplify logistics or increase efficiency compared to simpler
sampling designs, but makes risk calculations harder. We present SUITE, a new
method for conducting RLAs using stratified samples. SUITE considers all
possible partitions of outcome-changing error across strata. For each
partition, it combines P-values from stratum-level tests into a combined
P-value; there is no restriction on the tests used in different strata. SUITE
maximizes the combined P-value over all partitions of outcome-changing error.
The audit can stop if that maximum is less than the risk limit. Voting systems
in some Colorado counties (comprising 98.2% of voters) allow auditors to check
how the system interpreted each ballot, which allows ballot-level comparison
RLAs. Other counties use ballot polling, which is less efficient. Extant
approaches to conducting an RLA of a statewide contest would require major
changes to Colorado's procedures and software, or would sacrifice the
efficiency of ballot-level comparison. SUITE does not. It divides ballots into
two strata: those cast in counties that can conduct ballot-level comparisons,
and the rest. Stratum-level P-values are found by methods derived here. The
resulting audit is substantially more efficient than statewide ballot polling.
SUITE is useful in any state with a mix of voting systems or that uses
stratified sampling for other reasons. We provide an open-source reference
implementation and exemplar calculations in Jupyter notebooks.",1809.04235v1,stat.AP,2018-09-12 02:56:22+00:00,"[arxiv.Result.Author('Kellie Ottoboni'), arxiv.Result.Author('Philip B. Stark'), arxiv.Result.Author('Mark Lindeman'), arxiv.Result.Author('Neal McBurnett')]",
156,Physical Cryptographic Signatures for Absentee Ballots,"Physical signature verification on absentee ballots became a major flashpoint
in the 2018 midterm elections in the United States, especially in states like
Georgia, Florida, and Arizona, where close election margins resulted in
heightened attention to the counting of absentee ballots. As vote-by-mail
solutions are becoming more prevalent across the U.S., these issues are sure to
continue affecting elections in the United States. Signature verification is an
inexact science; often times guidelines can vary widely from jurisdiction to
jurisdiction. In this paper we provide a cryptographic remedy to this solution
that is usable, secure, and easily integrated into existing election
infrastructure.",1812.09423v1,cs.CR,2018-12-22 00:43:31+00:00,[arxiv.Result.Author('Matthew Bernhard')],
157,Boardroom Voting: Verifiable Voting with Ballot Privacy Using Low-Tech Cryptography in a Single Room,"A boardroom election is an election that takes place in a single room -- the
boardroom -- in which all voters can see and hear each other. We present an
initial exploration of boardroom elections with ballot privacy and voter
verifiability that use only ""low-tech cryptography"" without using computers to
mark or collect ballots. Specifically, we define the problem, introduce several
building blocks, and propose a new protocol that combines these blocks in novel
ways. Our new building blocks include ""foldable ballots"" that can be rotated to
hide the alignment of ballot choices with voting marks, and ""visual secrets""
that are easy to remember and use but hard to describe. Although closely seated
participants in a boardroom election have limited privacy, the protocol ensures
that no one can determine how others voted. Moreover, each voter can verify
that their ballot was correctly cast, collected, and counted, without being
able to prove how they voted, providing assurance against undue influence.
Low-tech cryptography is useful in situations where constituents do not trust
computer technology, and it avoids the complex auditing requirements of
end-to-end cryptographic voting systems such as Pr\^{e}t-\`{a}-Voter. This
paper's building blocks and protocol are meant to be a proof of concept that
might be tested for usability and improved.",2007.14916v2,cs.CR,2020-07-29 15:40:51+00:00,"[arxiv.Result.Author('Enka Blanchard'), arxiv.Result.Author('Ted Selker'), arxiv.Result.Author('Alan T. Sherman')]",
158,A multi-candidate electronic voting scheme with unlimited participants,"In this paper a new multi-candidate electronic voting scheme is constructed
with unlimited participants. The main idea is to express a ballot to allow
voting for up to k out of the m candidates and unlimited participants. The
purpose of vote is to select more than one winner among $m$ candidates. Our
result is complementary to the result by Sun peiyong$'$ s scheme, in the sense,
their scheme is not amenable for large-scale electronic voting due to flaw of
ballot structure. In our scheme the vote is split and hidden, and tallying is
made for $G\ddot{o}del$ encoding in decimal base without any trusted third
party, and the result does not rely on any traditional cryptography or
computational intractable assumption. Thus the proposed scheme not only solves
the problem of ballot structure, but also achieves the security including
perfect ballot secrecy, receipt-free, robustness, fairness and
dispute-freeness.",1712.10193v1,cs.CR,2017-12-29 11:52:23+00:00,"[arxiv.Result.Author('Xi Zhao'), arxiv.Result.Author('Yong Ding'), arxiv.Result.Author('Quanyu Zhao')]",
159,The Ranking Problem of Alternatives as a Cooperative Game,"This paper considers the ranking problem of candidates for a certain position
based on ballot papers filled by voters. We suggest a ranking procedure of
alternatives using cooperative game theory methods. For this, it is necessary
to construct a characteristic function via the filled ballot paper profile of
voters. The Shapley value serves as the ranking method. The winner is the
candidate having the maximum Shapley value. And finally, we explore the
properties of the designed ranking procedure.",1511.04437v1,cs.GT,2015-11-09 09:01:43+00:00,"[arxiv.Result.Author('Aleksei Kondratev'), arxiv.Result.Author('Vladimir Mazalov')]","International Journal of Game Theory, 49(1), 119-145 (2020)"
160,Formal Verification of Access Control Model for My Health Record System,"My Health Record system is the Australian Government's digital health record
system that holds My Health Record. My Health Record is a secure online health
record containing consumers' health information. The system aims to provide
health care professionals with access to key health information, e.g. listing
medicines, allergies and key diagnoses; radiology and pathology test results.
The system (previously named Personally Controlled Electronic Health Record)
enables consumers to decide how to share information with any of their health
care providers who are registered and connected to the system. The My Health
Record system operates under the Australian legislative framework My Health
Records Act 2012. The Act establishes, inter alia, a privacy framework
specifying which entities can collect, use and disclose certain information in
the system and the penalties that can be imposed on improper collection, use
and disclosure of this information. This paper presents the formal
specification (from the legislation) and verification of the My Health Record
regarding how consumers can control who access the information, and how the
system adheres to such access. We rely on the correct-by-construction Event-B
method to prove control and access properties of the system.",2006.06933v1,cs.SE,2020-06-12 03:47:59+00:00,[arxiv.Result.Author('Victor Rivera')],
161,Records from partial comparisons and discrete approximations,"In this paper we study records obtained from partial comparisons within a
sequence of independent and identically distributed (i.i.d.) random variables,
indexed by positive integers, with a common density~\(f.\) Our main result is
that if the comparison sets along a subsequence of the indices satisfy a
certain compatibility property, then the corresponding record events are
independent. Moreover, the record event probabilities do not depend on the
density~\(f\) and we obtain closed form expressions for the distribution
of~\(r^{th}\) record value for any integer~\(r \geq 1.\)
  Our proof techniques extend to the discrete case as well and we estimate the
difference in record event probabilities associated with a continuous random
variable~\(X\) and its discrete approximations.",1806.07357v1,math.PR,2018-06-19 17:40:07+00:00,[arxiv.Result.Author('Ghurumuruhan Ganesan')],
162,Finite record sets of chip-firing games,"A finite graph with an assignment of non-negative integers to vertices gives
chip-firing games. Chip-firing games determine languages (sets of words) called
the record sets of legal games. Bj\""orner, Lov\'asz and Shor found several
properties that are satisfied by record sets. In this paper, we will find two
more properties of record sets. Under the assumption that the record set is
finite and the game fires only two vertices, these properties characterize the
record sets of graphs.",2005.00822v1,math.CO,2020-05-02 12:56:15+00:00,"[arxiv.Result.Author('Kentaro Akasaka'), arxiv.Result.Author('Suguru Ishibashi'), arxiv.Result.Author('Masahiko Yoshinaga')]",
163,Records in the classical and quantum standard map,"Record statistics is the study of how new highs or lows are created and
sustained in any dynamical process. The study of the highest or lowest records
constitute the study of extreme values. This paper represents an exploration of
record statistics for certain aspects of the classical and quantum standard
map. For instance the momentum square or energy records is shown to behave like
that of records in random walks when the classical standard map is in a regime
of hard chaos. However different power laws is observed for the mixed phase
space regimes. The presence of accelerator modes are well-known to create
anomalous diffusion and we notice here that the record statistics is very
sensitive to their presence. We also discuss records in random vectors and use
it to analyze the {\it quantum} standard map via records in their eigenfunction
intensities, reviewing some recent results along the way.",1503.07823v1,cond-mat.stat-mech,2015-03-26 18:55:23+00:00,"[arxiv.Result.Author('Shashi C. L. Srivastava'), arxiv.Result.Author('Arul Lakshminarayan')]","Chaos, Solitons & Fractals, Volume 74, May 2015, Pages 67-78"
164,Efficient Entity Resolution on Heterogeneous Records,"Entity resolution (ER) is the problem of identifying and merging records that
refer to the same real-world entity. In many scenarios, raw records are stored
under heterogeneous environment. Specifically, the schemas of records may
differ from each other. To leverage such records better, most existing work
assume that schema matching and data exchange have been done to convert records
under different schemas to those under a predefined schema. However, we observe
that schema matching would lose information in some cases, which could be
useful or even crucial to ER.
  To leverage sufficient information from heterogeneous sources, in this paper,
we address several challenges of ER on heterogeneous records and show that none
of existing similarity metrics or their transformations could be applied to
find similar records under heterogeneous settings. Motivated by this, we design
the similarity function and propose a novel framework to iteratively find
records which refer to the same entity. Regarding efficiency, we build an index
to generate candidates and accelerate similarity computation. Evaluations on
real-world datasets show the effectiveness and efficiency of our methods.",1610.09500v1,cs.DB,2016-10-29 12:51:52+00:00,"[arxiv.Result.Author('Yiming Lin'), arxiv.Result.Author('Hongzhi Wang'), arxiv.Result.Author('Jianzhong Li'), arxiv.Result.Author('Hong Gao')]",
165,The Paper Pile at Home: Adopting Personal Electronic Records,"Research has found that if respondents do not manage their personal records
such as bills, receipts and tax-related documents efficiently, they risk not
being able to re-find them when needed, resulting in significant problems. A
significant gap in understanding and addressing this problem stems from a lack
of knowledge of the format of these records, particularly in the context of the
COVID-19 pandemic, that may have caused an increase in managing personal
records in an electronic format, rather than by hardcopy. This paper provides
results of quantitative research conducted in 2018, thereby providing a
valuable benchmark for future research on the same and related topics. This
measurement was achieved by means of an online survey distributed via social
media amongst 205 respondents. The results revealed that nearly all respondents
(97%) retained at least some records, and more than 80% kept some of those
records in an electronic format, particularly travel reservations and payslips.
Conversely, only 10% of respondents kept receipts and warranties for appliances
or medical records in an electronic format. The reason for these differences in
propensity of keeping various records in an electronic or hardcopy format will
require further research.",2204.13282v2,cs.CY,2022-04-28 04:33:39+00:00,[arxiv.Result.Author('Matt Balogh')],
166,Feasibility Study on Intra-Grid Location Estimation Using Power ENF Signals,"The Electric Network Frequency (ENF) is a signature of power distribution
networks that can be captured by multimedia recordings made in areas where
there is electrical activity. This has led to an emergence of several forensic
applications based on the use of the ENF signature. Examples of such
applications include estimating or verifying the time-of-recording of a media
signal and inferring the power grid associated with the location in which the
media signal was recorded. In this paper, we carry out a feasibility study to
examine the possibility of using embedded ENF traces to pinpoint the
location-of-recording of a signal within a power grid. In this study, we
demonstrate that it is possible to pinpoint the location-of-recording to a
certain geographical resolution using power signal recordings containing strong
ENF traces. To this purpose, a high-passed version of an ENF signal is
extracted and it is demonstrated that the correlation between two such signals,
extracted from recordings made in different geographical locations within the
same grid, decreases as the distance between the recording locations increases.
We harness this property of correlation in the ENF signals to propose
trilateration based localization methods, which pinpoint the unknown location
of a recording while using some known recording locations as anchor locations.
We also discuss the challenges that need to be overcome in order to extend this
work to using ENF traces in noisier audio/video recordings for such fine
localization purposes.",2105.00668v1,eess.SP,2021-05-03 07:56:54+00:00,"[arxiv.Result.Author('Ravi Garg'), arxiv.Result.Author('Adi Hajj-Ahmad'), arxiv.Result.Author('Min Wu')]",
167,Application of Advanced Record Linkage Techniques for Complex Population Reconstruction,"Record linkage is the process of identifying records that refer to the same
entities from several databases. This process is challenging because commonly
no unique entity identifiers are available. Linkage therefore has to rely on
partially identifying attributes, such as names and addresses of people. Recent
years have seen the development of novel techniques for linking data from
diverse application areas, where a major focus has been on linking complex data
that contain records about different types of entities. Advanced approaches
that exploit both the similarities between record attributes as well as the
relationships between entities to identify clusters of matching records have
been developed.
  In this application paper we study the novel problem where rather than
different types of entities we have databases where the same entity can have
different roles, and where these roles change over time. We specifically
develop novel techniques for linking historical birth, death, marriage and
census records with the aim to reconstruct the population covered by these
records over a period of several decades. Our experimental evaluation on real
Scottish data shows that even with advanced linkage techniques that consider
group, relationship, and temporal aspects it is challenging to achieve high
quality linkage from such complex data.",1612.04286v1,cs.DB,2016-12-13 17:10:11+00:00,[arxiv.Result.Author('Peter Christen')],
168,Increasing Transparent and Accountable Use of Data by Quantifying the Actual Privacy Risk in Interactive Record Linkage,"Record linkage refers to the task of integrating data from two or more
databases without a common identifier. MINDFIRL (MInimum Necessary Disclosure
For Interactive Record Linkage) is a software system that demonstrates the
tradeoff between utility and privacy in interactive record linkage. Due to the
need to access personally identifiable information (PII) to accurately assess
whether different records refer to the same person in heterogeneous databases,
privacy is a major concern in interactive record linkage. MINDFIRL supports
interactive record linkage while minimizing the privacy risk by (1) using
pseudonyms to separate the identifying information from the sensitive
information, (2) dynamically disclosing only the minimum necessary information
incrementally, as needed on-demand at the point of decision, and (3) quantifies
the risk due to the needed information disclosure to support transparency, the
reasoning, communication, and decisions on the privacy and utility trade off.
In this paper we present an overview of the MINDFIRL system and the
k-Anonymized Privacy Risk (KAPR) score used to measure the privacy risk based
on the disclosed information. We prove that KAPR score is a norm meeting all
the desirable properties for a risk score for interactive record linkage.",1906.03345v1,cs.DB,2019-06-07 22:00:08+00:00,"[arxiv.Result.Author('Qinbo Li'), arxiv.Result.Author(""Adam G. D'Souza""), arxiv.Result.Author('Cason Schmit'), arxiv.Result.Author('Hye-Chung Kum')]",
169,Finiteness of Record values and Alternative Asymptotic Theory of Records with Atom Endpoints,"Asymptotic theories on record values and times, including central limit
theorems, make sense only if the sequence of records values (and of record
times) is infinite. If not, such theories could not even be an option. In this
paper, we give necessary and/or sufficient conditions for the finiteness of the
number of records. We prove, for example for \textsl{iid} real valued random
variable, that strong upper record values are finite if and only if the upper
endpoint is finite and is an atom of the common cumulative distribution
function. The only asymptotic study left to us concerns the infinite sequence
of hitting times of that upper endpoints, which by the way, is the sequence of
weak record times. The asymptotic characterizations are made using negative
binomial random variables and the dimensional multinomial random variables.
Asymptotic comparison in terms of consistency bounds and confidence intervals
on the different sequences of hitting times are provide. The example of a
binomial random variable is given",1909.08163v1,math.PR,2019-09-18 01:46:38+00:00,"[arxiv.Result.Author('Gane Samb Lo'), arxiv.Result.Author('Harouna Sangaré'), arxiv.Result.Author('Mamadou Cherif Traoré'), arxiv.Result.Author('Mohammad Ahsanullah')]",
170,On the records,"World record setting has long attracted public interest and scientific
investigation. Extremal records summarize the limits of the space explored by a
process, and the historical progression of a record sheds light on the
underlying dynamics of the process. Existing analyses of prediction,
statistical properties, and ultimate limits of record progressions have focused
on particular domains. However, a broad perspective on how record progressions
vary across different spheres of activity needs further development. Here we
employ cross-cutting metrics to compare records across a variety of domains,
including sports, games, biological evolution, and technological development.
We find that these domains exhibit characteristic statistical signatures in
terms of rates of improvement, ""burstiness"" of record-breaking time series, and
the acceleration of the record breaking process. Specifically, sports and games
exhibit the slowest rate of improvement and a wide range of rates of
""burstiness."" Technology improves at a much faster rate and, unlike other
domains, tends to show acceleration in records. Many biological and
technological processes are characterized by constant rates of improvement,
showing less burstiness than sports and games. It is important to understand
how these statistical properties of record progression emerge from the
underlying dynamics. Towards this end, we conduct a detailed analysis of a
particular record-setting event: elite marathon running. In this domain, we
find that studying record-setting data alone can obscure many of the structural
properties of the underlying process. The marathon study also illustrates how
some of the standard statistical assumptions underlying record progression
models may be inappropriate or commonly violated in real-world datasets.",1705.04353v2,physics.soc-ph,2017-05-11 18:59:43+00:00,"[arxiv.Result.Author('Andrew Berdahl'), arxiv.Result.Author('Uttam Bhat'), arxiv.Result.Author('Vanessa Ferdinand'), arxiv.Result.Author('Joshua Garland'), arxiv.Result.Author('Keyan Ghazi-Zahedi'), arxiv.Result.Author('Justin Grana'), arxiv.Result.Author('Joshua A. Grochow'), arxiv.Result.Author('Elizabeth Hobson'), arxiv.Result.Author('Yoav Kallus'), arxiv.Result.Author('Christopher P. Kempes'), arxiv.Result.Author('Artemy Kolchinsky'), arxiv.Result.Author('Daniel B. Larremore'), arxiv.Result.Author('Eric Libby'), arxiv.Result.Author('Eleanor A. Power'), arxiv.Result.Author('Brendan D. Tracey')]",
171,End-to-end Recording Device Identification Based on Deep Representation Learning,"Deep learning techniques have achieved specific results in recording device
source identification. The recording device source features include spatial
information and certain temporal information. However, most recording device
source identification methods based on deep learning only use spatial
representation learning from recording device source features, which cannot
make full use of recording device source information. Therefore, in this paper,
to fully explore the spatial information and temporal information of recording
device source, we propose a new method for recording device source
identification based on the fusion of spatial feature information and temporal
feature information by using an end-to-end framework. From a feature
perspective, we designed two kinds of networks to extract recording device
source spatial and temporal information. Afterward, we use the attention
mechanism to adaptively assign the weight of spatial information and temporal
information to obtain fusion features. From a model perspective, our model uses
an end-to-end framework to learn the deep representation from spatial feature
and temporal feature and train using deep and shallow loss to joint optimize
our network. This method is compared with our previous work and baseline
system. The results show that the proposed method is better than our previous
work and baseline system under general conditions.",2212.02084v1,cs.SD,2022-12-05 07:56:04+00:00,"[arxiv.Result.Author('Chunyan Zeng'), arxiv.Result.Author('Dongliang Zhu'), arxiv.Result.Author('Zhifeng Wang'), arxiv.Result.Author('Minghu Wu'), arxiv.Result.Author('Wei Xiong'), arxiv.Result.Author('Nan Zhao')]",
172,Multi-label Ferns for Efficient Recognition of Musical Instruments in Recordings,"In this paper we introduce multi-label ferns, and apply this technique for
automatic classification of musical instruments in audio recordings. We compare
the performance of our proposed method to a set of binary random ferns, using
jazz recordings as input data. Our main result is obtaining much faster
classification and higher F-score. We also achieve substantial reduction of the
model size.",1403.7746v1,cs.LG,2014-03-30 12:22:36+00:00,"[arxiv.Result.Author('Miron B. Kursa'), arxiv.Result.Author('Alicja A. Wieczorkowska')]",
173,Sum of weighted records in set partitions,"The purpose of this paper is to find an explicit formula and asymptotic
estimate for the total number of sum of weighted records over set partitions of
$[n]$
  in terms of Bell numbers. For that we study the generating function for the
number of set partitions of $[n]$ according to the statistic sum of weighted
records.",1906.00680v2,math.CO,2019-06-03 10:04:50+00:00,[arxiv.Result.Author('Walaa Asakly')],
174,Large and moderate deviations of weak record numbers in random walks,"Record numbers are basic statistics in random walks, whose deviation
principles are not very clear so far. In this paper, the asymptotic
probabilities of large and moderate deviations for numbers of weak records in
right continuous or left continuous random walks are proved.",2101.03582v1,math.PR,2021-01-10 17:27:12+00:00,"[arxiv.Result.Author('Yuqiang Li'), arxiv.Result.Author('Qiang Yao')]",
175,Transformation of low-quality device-recorded speech to high-quality speech using improved SEGAN model,"Nowadays vast amounts of speech data are recorded from low-quality recorder
devices such as smartphones, tablets, laptops, and medium-quality microphones.
The objective of this research was to study the automatic generation of
high-quality speech from such low-quality device-recorded speech, which could
then be applied to many speech-generation tasks. In this paper, we first
introduce our new device-recorded speech dataset then propose an improved
end-to-end method for automatically transforming the low-quality
device-recorded speech into professional high-quality speech. Our method is an
extension of a generative adversarial network (GAN)-based speech enhancement
model called speech enhancement GAN (SEGAN), and we present two modifications
to make model training more robust and stable. Finally, from a large-scale
listening test, we show that our method can significantly enhance the quality
of device-recorded speech signals.",1911.03952v2,cs.SD,2019-11-10 16:05:21+00:00,"[arxiv.Result.Author('Seyyed Saeed Sarfjoo'), arxiv.Result.Author('Xin Wang'), arxiv.Result.Author('Gustav Eje Henter'), arxiv.Result.Author('Jaime Lorenzo-Trueba'), arxiv.Result.Author('Shinji Takaki'), arxiv.Result.Author('Junichi Yamagishi')]",
176,Durable Top-K Instant-Stamped Temporal Records with User-Specified Scoring Functions,"A way of finding interesting or exceptional records from instant-stamped
temporal data is to consider their ""durability,"" or, intuitively speaking, how
well they compare with other records that arrived earlier or later, and how
long they retain their supremacy. For example, people are naturally fascinated
by claims with long durability, such as: ""On January 22, 2006, Kobe Bryant
dropped 81 points against Toronto Raptors. Since then, this scoring record has
yet to be broken."" In general, given a sequence of instant-stamped records,
suppose that we can rank them by a user-specified scoring function $f$, which
may consider multiple attributes of a record to compute a single score for
ranking. This paper studies ""durable top-$k$ queries"", which find records whose
scores were within top-$k$ among those records within a ""durability window"" of
given length, e.g., a 10-year window starting/ending at the timestamp of the
record. The parameter $k$, the length of the durability window, and parameters
of the scoring function (which capture user preference) can all be given at the
query time. We illustrate why this problem formulation yields more meaningful
answers in some practical situations than other similar types of queries
considered previously. We propose new algorithms for solving this problem, and
provide a comprehensive theoretical analysis on the complexities of the problem
itself and of our algorithms. Our algorithms vastly outperform various
baselines (by up to two orders of magnitude on real and synthetic datasets).",2102.12072v2,cs.DB,2021-02-24 05:06:15+00:00,"[arxiv.Result.Author('Junyang Gao'), arxiv.Result.Author('Stavros Sintos'), arxiv.Result.Author('Pankaj K. Agarwal'), arxiv.Result.Author('Jun Yang')]",
177,Discovering Knowledge from Multi-modal Lecture Recordings,"Educational media mining is the process of converting raw media data from
educational systems to useful information that can be used to design learning
systems, answer research questions and allow personalized learning experiences.
Knowledge discovery encompasses a wide range of techniques ranging from
database queries to more recent developments in machine learning and language
technology. Educational media mining techniques are now being used in IT
Services research worldwide. Multi-modal Lecture Recordings is one of the
important types of educational media and this paper explores the research
challenges for mining lecture recordings for the efficient personalized
learning experiences. Keywords: Educational Media Mining; Lecture Recordings,
Multimodal Information System, Personalized Learning; Online Course Ware;
Skills and Competences;",1001.0443v1,cs.MM,2010-01-04 05:44:57+00:00,"[arxiv.Result.Author('Rajkumar Kannan'), arxiv.Result.Author('Christian Guetl')]",
178,Record-dependent measures on the symmetric groups,"A probability measure $P_n$ on the symmetric group ${\mathfrak S}_n$ is said
to be record-dependent if $P_n(\sigma)$ depends only on the set of records of a
permutation $\sigma\in{\mathfrak S}_n$. A sequence $P=(P_n)_{n\in{\mathbb N}}$
of consistent record-dependent measures determines a random order on $\mathbb
N$. In this paper we describe the extreme elements of the convex set of such
$P$. This problem turns out to be related to the study of asymptotic behavior
of permutation-valued growth processes, to random extensions of partial orders,
and to the measures on the Young-Fibonacci lattice.",1202.3680v2,math.PR,2012-02-16 19:44:03+00:00,"[arxiv.Result.Author('Alexander Gnedin'), arxiv.Result.Author('Vadim Gorin')]",
179,Pareto analysis based on records,"Estimation of the parameters of an exponential distribution based on record
data has been treated by Samaniego and Whitaker (1986) and Doostparast (2009).
Recently, Doostparast and Balakrishnan (2011) obtained optimal confidence
intervals as well as uniformly most powerful tests for one- and two-sided
hypotheses concerning location and scale parameters based on record data from a
two-parameter exponential model. In this paper, we derive optimal statistical
procedures including point and interval estimation as well as most powerful
tests based on record data from a two-parameter Pareto model. For illustrative
purpose, a data set on annual wages of a sample production-line workers in a
large industrial firm is analyzed using the proposed procedures.",1205.0638v1,math.ST,2012-05-03 07:20:45+00:00,"[arxiv.Result.Author('M. Doostparast'), arxiv.Result.Author('N. Balakrishnan')]",
180,TRIP: Trustless Coercion-Resistant In-Person Voter Registration,"Most existing remote electronic voting systems are vulnerable to voter
coercion and vote buying. While coercion-resistant voting systems address this
challenge, current schemes assume that the voter has access to an untappable,
incorruptible device during voter registration. We present TRIP, an in-person
voter registration scheme enabling voters to create verifiable and
indistinguishable real and fake credentials using an untrusted kiosk inside a
privacy booth at a supervised location, e.g., the registrar's office. TRIP
ensures the integrity of the voter's real credential while enabling the
creation of fake credentials using interactive zero-knowledge proofs between
the voter as the verifier and the kiosk as the prover, unbeknownst to the
average voter. TRIP ensures that even voters who are under extreme coercion,
and cannot leave the booth with a real credential, can delegate their vote to a
political party, with the caveat that they must then trust the kiosk. TRIP
optimizes the tallying process by limiting the number of credentials a voter
can receive and capping the number of votes that a credential can cast per
election. We conduct a preliminary usability study among 41 participants at a
university and found that 42.5% of participants rated TRIP a B or higher in
usability, a promising result for a voter registration scheme that
substantially reduces trust in the registrar.",2202.06692v1,cs.CR,2022-02-14 13:35:46+00:00,"[arxiv.Result.Author('Louis-Henri Merino'), arxiv.Result.Author('Simone Colombo'), arxiv.Result.Author('Jeff Allen'), arxiv.Result.Author('Vero Estrada-Galiñanes'), arxiv.Result.Author('Bryan Ford')]",
181,Coconut E-Petition Implementation,"In this dissertation project, we describe and implement a practical system
application based on a selective disclosure credential scheme, namely the
Coconut credential scheme\cite{sonnino_coconut:_2018}. The specific application
here is an electronic petition system with the distinctive added feature of
unlinkability as well as anonymity: such that no information about the
anonymous petition voter is linkable back to the individual. In other words,
there is no data leaked about who voted in the petition, just that the users
who did, were indeed eligible and authorized to vote. As for the
implementation, the client-side is done using JavaScript so that the client can
trustlessly compute the cryptographic constructions individually, whereas the
server-side is done using Node.js, but can easily be replaced by a more
sophisticated and secure structure such as a permissionless blockchain
platform.",1809.10956v1,cs.CR,2018-09-28 10:44:48+00:00,[arxiv.Result.Author('Jad Wahab')],
182,TEEvil: Identity Lease via Trusted Execution Environments,"We investigate identity lease, a new type of service in which users lease
their identities to third parties by providing them with full or restricted
access to their online accounts or credentials. We discuss how identity lease
could be abused to subvert the digital society, facilitating the spread of fake
news and subverting electronic voting by enabling the sale of votes. We show
that the emergence of Trusted Execution Environments and anonymous
cryptocurrencies, for the first time, allows the implementation of such a lease
service while guaranteeing fairness, plausible deniability and anonymity,
therefore shielding the users and account renters from prosecution. To show
that such a service can be practically implemented, we build an example service
that we call TEEvil leveraging Intel SGX and ZCash. Finally, we discuss defense
mechanisms and challenges in the mitigation of identity lease services.",1903.00449v2,cs.CR,2019-03-01 18:20:24+00:00,"[arxiv.Result.Author('Ivan Puddu'), arxiv.Result.Author('Daniele Lain'), arxiv.Result.Author('Moritz Schneider'), arxiv.Result.Author('Elizaveta Tretiakova'), arxiv.Result.Author('Sinisa Matetic'), arxiv.Result.Author('Srdjan Capkun')]",
183,Data-centric Misbehavior Detection in VANETs,"Detecting misbehavior (such as transmissions of false information) in
vehicular ad hoc networks (VANETs) is very important problem with wide range of
implications including safety related and congestion avoidance applications. We
discuss several limitations of existing misbehavior detection schemes (MDS)
designed for VANETs. Most MDS are concerned with detection of malicious nodes.
In most situations, vehicles would send wrong information because of selfish
reasons of their owners, e.g. for gaining access to a particular lane. Because
of this (\emph{rational behavior}), it is more important to detect false
information than to identify misbehaving nodes. We introduce the concept of
data-centric misbehavior detection and propose algorithms which detect false
alert messages and misbehaving nodes by observing their actions after sending
out the alert messages. With the data-centric MDS, each node can independently
decide whether an information received is correct or false. The decision is
based on the consistency of recent messages and new alert with reported and
estimated vehicle positions. No voting or majority decisions is needed, making
our MDS resilient to Sybil attacks. Instead of revoking all the secret
credentials of misbehaving nodes, as done in most schemes, we impose fines on
misbehaving nodes (administered by the certification authority), discouraging
them to act selfishly. This reduces the computation and communication costs
involved in revoking all the secret credentials of misbehaving nodes.",1103.2404v1,cs.NI,2011-03-12 00:51:18+00:00,"[arxiv.Result.Author('Sushmita Ruj'), arxiv.Result.Author('Marcos Antonio Cavenaghi'), arxiv.Result.Author('Zhen Huang'), arxiv.Result.Author('Amiya Nayak'), arxiv.Result.Author('Ivan Stojmenovic')]",
184,Twins: BFT Systems Made Robust,"This paper presents Twins, an automated unit test generator of Byzantine
attacks. Twins implements three types of Byzantine behaviors: (i) leader
equivocation, (ii) double voting, and (iii) losing internal state such as
forgetting 'locks' guarding voted values. To emulate interesting attacks by a
Byzantine node, it instantiates twin copies of the node instead of one, giving
both twins the same identities and network credentials. To the rest of the
system, the twins appear indistinguishable from a single node behaving in a
'questionable' manner. Twins can systematically generate Byzantine attack
scenarios at scale, execute them in a controlled manner, and examine their
behavior. Twins scenarios iterate over protocol rounds and vary the
communication patterns among nodes. Twins runs in a production setting within
DiemBFT where it can execute 44M Twins-generated scenarios daily. Whereas the
system at hand did not manifest errors, subtle safety bugs that were
deliberately injected for the purpose of validating the implementation of Twins
itself were exposed within minutes. Twins can prevent developers from
regressing correctness when updating the codebase, introducing new features, or
performing routine maintenance tasks. Twins only requires a thin wrapper over
DiemBFT, we thus envision other systems using it. Building on this idea, one
new attack and several known attacks against other BFT protocols were
materialized as Twins scenarios. In all cases, the target protocols break
within fewer than a dozen protocol rounds, hence it is realistic for the Twins
approach to expose the problems.",2004.10617v2,cs.CR,2020-04-22 14:59:04+00:00,"[arxiv.Result.Author('Shehar Bano'), arxiv.Result.Author('Alberto Sonnino'), arxiv.Result.Author('Andrey Chursin'), arxiv.Result.Author('Dmitri Perelman'), arxiv.Result.Author('Zekun Li'), arxiv.Result.Author('Avery Ching'), arxiv.Result.Author('Dahlia Malkhi')]",
185,Trust Implications of DDoS Protection in Online Elections,"Online elections make a natural target for distributed denial of service
attacks. Election agencies wary of disruptions to voting may procure DDoS
protection services from a cloud provider. However, current DDoS detection and
mitigation methods come at the cost of significantly increased trust in the
cloud provider. In this paper we examine the security implications of
denial-of-service prevention in the context of the 2017 state election in
Western Australia, revealing a complex interaction between actors and
infrastructure extending far beyond its borders.
  Based on the publicly observable properties of this deployment, we outline
several attack scenarios including one that could allow a nation state to
acquire the credentials necessary to man-in-the-middle a foreign election in
the context of an unrelated domestic law enforcement or national security
operation, and we argue that a fundamental tension currently exists between
trust and availability in online elections.",1708.00991v1,cs.CR,2017-08-03 04:19:32+00:00,"[arxiv.Result.Author('Chris Culnane'), arxiv.Result.Author('Mark Eldridge'), arxiv.Result.Author('Aleksander Essex'), arxiv.Result.Author('Vanessa Teague')]",
186,A new weighted ensemble model for phishing detection based on feature selection,"A phishing attack is a sort of cyber assault in which the attacker sends fake
communications to entice a human victim to provide personal information or
credentials. Phishing website identification can assist visitors in avoiding
becoming victims of these assaults. The phishing problem is increasing day by
day, and there is no single solution that can properly mitigate all
vulnerabilities, thus many techniques are used. In this paper, We have proposed
an ensemble model that combines multiple base models with a voting technique
based on the weights. Moreover, we applied feature selection methods and
standardization on the dataset effectively and compared the result before and
after applying any feature selection.",2212.11125v1,cs.CR,2022-12-15 23:15:36+00:00,"[arxiv.Result.Author('Farnoosh Shirani Bidabadi'), arxiv.Result.Author('Shuaifang Wang')]",
187,Cerberus: A Blockchain-Based Accreditation and Degree Verification System,"Credential fraud is a widespread practice that undermines investment and
confidence in higher education systems and bears significant economic and
social costs. Legacy credential verification systems are typically
time-consuming, costly, and bureaucratic, and struggle against certain classes
of credential fraud. In this paper, we propose a comprehensive blockchain-based
credential verification solution, Cerberus, which is considerably more
efficient, easy and intuitive to use, and effectively mitigates widespread
manifestations of credential fraud. Cerberus also improves significantly upon
other blockchain-based solutions in the research literature: it adheres closely
to the existing credential verification ecosystem, it addresses a threat model
informed by real-world fraud scenarios. Moreover, Cerberus uses on-chain smart
contracts for credential revocation, and it does not entail students or
employers to manage digital identities or cryptographic credentials to use the
system. We prototype our solution and describe our attempt to design an online
verification service with a rich feature set, including data privacy,
transcript verification, and selective disclosure of data. We hope this effort
contributes positively to towards alleviating the problem of fake credentials.",1912.06812v1,cs.CR,2019-12-14 09:29:41+00:00,"[arxiv.Result.Author('Aamna Tariq'), arxiv.Result.Author('Hina Binte Haq'), arxiv.Result.Author('Syed Taha Ali')]",
188,Blockchain for Academic Credentials,"Academic credentials are documents that attest to successful completion of
any test, exam or act as a validation of an individual's skill. Currently, the
domain of academic credential management suffers from large time consumption,
high cost, dependence on third-party and a lack of transparency. A blockchain
based solution tries to resolve these pain-points by allowing any recruiter or
company to verify the user credentials without dependence on any centralized
third party. Our decentralized application is based off of BlockCerts, an MIT
project that acts as an open standard for blockchain credentials. The project
talks about the implementation details of the decentralized application built
for BlockCerts Wallet. It is an attempt to leverage the power of the blockchain
technology as a global notary for the verification of digital records.",2006.12665v1,cs.CR,2020-06-22 23:42:20+00:00,[arxiv.Result.Author('Chaitanya Bapat')],
189,Full-text Search for Verifiable Credential Metadata on Distributed Ledgers,"Self-sovereign Identity (SSI) powered by distributed ledger technologies
enables more flexible and faster digital identification workflows, while at the
same time limiting the control and influence of central authorities. However, a
global identity solution must be able to handle myriad credential types from
millions of issuing organizations. As metadata about types of digital
credentials is readable by everyone on the public permissioned ledger with
Hyperledger Indy, anyone could find relevant and trusted credential types for
their use cases by looking at the records on the blockchain. To this date, no
efficient full-text search mechanism exists that would allow users to search
for credential types in a simple and efficient fashion tightly integrated into
their applications. In this work, we propose a full-text search framework based
on the publicly available metadata on the Hyperledger Indy ledger for
retrieving matching credential types. The proposed solution is able to find
credential types based on textual input from the user by using a full-text
search engine and maintaining a local copy of the ledger. Thus, we do not need
to rely on information about credentials coming from a very large candidate
pool of third parties we would need to trust, such as the website of a company
displaying its own identifier and a list of issued credentials. We have also
proven the feasiblity of the concept by implementing and evaluating a prototype
of the full-text credential metadata search service.",1909.02895v1,cs.CR,2019-09-06 13:31:13+00:00,"[arxiv.Result.Author('Zoltán András Lux'), arxiv.Result.Author('Felix Beierle'), arxiv.Result.Author('Sebastian Zickau'), arxiv.Result.Author('Sebastian Göndör')]",
190,Privatizing user credential information of Web services in a shared user environment,"User credentials security is one of the most important tasks in Web World.
Most Web sites on the Internet that support user accounts store the users
credentials in a database. Now a days, most of the web browsers offer auto
login feature for the favorite web sites such as yahoo, google, gmail etc.
using these credential information. This facilitates the misuse of user
credentials. Privatizing user credential information of web services in a
shared user environment provides a feature enhancement where the root user will
be able to privatize his stored credentials by enforcing some masking
techniques such that even a user logs on to the system with root user
credentials, he will not be able to access privatized data. In case of web
browsers auto login feature, a root user can disable the feature manually by
deleting entries from web browsers' saved password list. But this involves
spending a considerable amount of time and the biggest problem is that he has
to insert those credentials once again when he next visits these websites. This
application resumes auto login feature whenever root user disable the masked
mode. The application includes two parts: Masked Application Mode and Disabling
the Masked Application Mode. When the system goes for masked application mode,
the other user will not be able to use the credentials of the root user.If the
other user tries to access any of the web pages which have been masked, the
other user will have to authenticate with his own credentials. Disabling the
masked mode requires authentication from the root user. As long as this
credential is not shared, masked mode can be disabled only by the root user.",1308.3482v1,cs.CR,2013-08-15 19:28:00+00:00,"[arxiv.Result.Author('Pinaki Mitra'), arxiv.Result.Author('Rinku Das'), arxiv.Result.Author('Girish Sundaram')]",
191,A note on anonymous credentials using BLS signatures,"In this note, we remark that the aggregation property of the BLS signature
scheme yields an efficient Content Extraction Signature (CES). This
construction can be used to build digital credentials that support selective
disclosure in various settings. Interestingly, this construction is efficient
and well suited to build credential issuance schemes with various applications
in the client-server or in the distributed ledger models. Finally, we sketch a
protocol that combines the CES with the use of a NIZK which allows to prove
predicate satisfaction on claims extracted from a credential, while keeping the
data secret.",2006.05201v1,cs.CR,2020-06-09 11:54:41+00:00,[arxiv.Result.Author('Antoine Rondelet')],
192,Might I Get Pwned: A Second Generation Compromised Credential Checking Service,"Credential stuffing attacks use stolen passwords to log into victim accounts.
To defend against these attacks, recently deployed compromised credential
checking (C3) services provide APIs that help users and companies check whether
a username, password pair is exposed. These services however only check if the
exact password is leaked, and therefore do not mitigate credential tweaking
attacks - attempts to compromise a user account with variants of a user's
leaked passwords. Recent work has shown credential tweaking attacks can
compromise accounts quite effectively even when the credential stuffing
countermeasures are in place. We initiate work on C3 services that protect
users from credential tweaking attacks. The core underlying challenge is how to
identify passwords that are similar to their leaked passwords while preserving
honest clients' privacy and also preventing malicious clients from extracting
breach data from the service. We formalize the problem and explore ways to
measure password similarity that balance efficacy, performance, and security.
Based on this study, we design ""Might I Get Pwned"" (MIGP), a new kind of breach
alerting service. Our simulations show that MIGP reduces the efficacy of
state-of-the-art 1000-guess credential tweaking attacks by 94%. MIGP preserves
user privacy and limits potential exposure of sensitive breach entries. We show
that the protocol is fast, with response time close to existing C3 services. We
worked with Cloudflare to deploy MIGP in practice.",2109.14490v2,cs.CR,2021-09-29 15:16:59+00:00,"[arxiv.Result.Author('Bijeeta Pal'), arxiv.Result.Author('Mazharul Islam'), arxiv.Result.Author('Marina Sanusi'), arxiv.Result.Author('Nick Sullivan'), arxiv.Result.Author('Luke Valenta'), arxiv.Result.Author('Tara Whalen'), arxiv.Result.Author('Christopher Wood'), arxiv.Result.Author('Thomas Ristenpart'), arxiv.Result.Author('Rahul Chattejee')]",
193,Behavioural Analytics: Beyond Risk-based MFA,"This paper investigates how to effectively stop an attacker from using
compromised user credentials to gain authorized entry to systems that they are
otherwise not authorised to access. The proposed solution extends previous work
to move beyond a risk-based multi-factor authentication system. It adds a
behavioural analytics component that uses keystroke dynamics to grant or deny
users access. Given the increasing number of compromised user credential
stores, we make the assumption that criminals already know the user
credentials. Hence, to test our solution, users were given authentic user
credentials and asked to login to our proof-of-concept. Despite the fact that
all illegitimate users in our test cases were given the correct user
credentials for legitimate users, none of these were granted access by the
system. This demonstrates zero- tolerance to false positives. The results
demonstrate the uniqueness of keystroke dynamics and its use to prevent users
with stolen credentials from accessing systems they are not authorized to
access.",1801.02332v1,cs.CR,2018-01-08 08:06:30+00:00,[arxiv.Result.Author('Roy Henha Eyono')],
194,Generating Single Peaked Votes,"We discuss how to generate singled peaked votes uniformly from the Impartial
Culture model.",1503.02766v1,cs.GT,2015-03-10 04:15:23+00:00,[arxiv.Result.Author('Toby Walsh')],
195,Normalized Range Voting Broadly Resists Control,"We study the behavior of Range Voting and Normalized Range Voting with
respect to electoral control. Electoral control encompasses attempts from an
election chair to alter the structure of an election in order to change the
outcome. We show that a voting system resists a case of control by proving that
performing that case of control is computationally infeasible. Range Voting is
a natural extension of approval voting, and Normalized Range Voting is a simple
variant which alters each vote to maximize the potential impact of each voter.
We show that Normalized Range Voting has among the largest number of control
resistances among natural voting systems.",1005.5698v3,cs.GT,2010-05-31 15:34:11+00:00,[arxiv.Result.Author('Curtis Menton')],
196,Strategic Voting Under Uncertainty About the Voting Method,"Much of the theoretical work on strategic voting makes strong assumptions
about what voters know about the voting situation. A strategizing voter is
typically assumed to know how other voters will vote and to know the rules of
the voting method. A growing body of literature explores strategic voting when
there is uncertainty about how others will vote. In this paper, we study
strategic voting when there is uncertainty about the voting method. We
introduce three notions of manipulability for a set of voting methods: sure,
safe, and expected manipulability. With the help of a computer program, we
identify voting scenarios in which uncertainty about the voting method may
reduce or even eliminate a voter's incentive to misrepresent her preferences.
Thus, it may be in the interest of an election designer who wishes to reduce
strategic voting to leave voters uncertain about which of several reasonable
voting methods will be used to determine the winners of an election.",1907.09110v1,cs.MA,2019-07-22 03:17:19+00:00,"[arxiv.Result.Author('Wesley H. Holliday'), arxiv.Result.Author('Eric Pacuit')]","EPTCS 297, 2019, pp. 252-272"
197,Vote Delegation and Misbehavior,"We study vote delegation with ""well-behaving"" and ""misbehaving"" agents and
compare it with conventional voting. Typical examples for vote delegation are
validation or governance tasks on blockchains. There is a majority of
well-behaving agents, but they may abstain or delegate their vote to other
agents since voting is costly. Misbehaving agents always vote. We compare
conventional voting allowing for abstention with vote delegation. Preferences
of voters are private information and a positive outcome is achieved if
well-behaving agents win. We illustrate that vote delegation leads to quite
different outcomes than conventional voting with abstention. In particular, we
obtain three insights: First, if the number of misbehaving voters, denoted by f
, is high, both voting methods fail to deliver a positive outcome. Second, if f
takes an intermediate value, conventional voting delivers a positive outcome,
while vote delegation fails with probability one. Third, if f is low,
delegation delivers a positive outcome with higher probability than
conventional voting. Finally, our results characterize worst-case outcomes that
can happen in a liquid democracy.",2102.08823v2,cs.GT,2021-02-17 15:32:32+00:00,"[arxiv.Result.Author('Hans Gersbach'), arxiv.Result.Author('Akaki Mamageishvili'), arxiv.Result.Author('Manvir Schneider')]",
198,Optimal Strategic Mining Against Cryptographic Self-Selection in Proof-of-Stake,"Cryptographic Self-Selection is a subroutine used to select a leader for
modern proof-of-stake consensus protocols, such as Algorand. In cryptographic
self-selection, each round $r$ has a seed $Q_r$. In round $r$, each account
owner is asked to digitally sign $Q_r$, hash their digital signature to produce
a credential, and then broadcast this credential to the entire network. A
publicly-known function scores each credential in a manner so that the
distribution of the lowest scoring credential is identical to the distribution
of stake owned by each account. The user who broadcasts the lowest-scoring
credential is the leader for round $r$, and their credential becomes the seed
$Q_{r+1}$. Such protocols leave open the possibility of a selfish-mining style
attack: a user who owns multiple accounts that each produce low-scoring
credentials in round $r$ can selectively choose which ones to broadcast in
order to influence the seed for round $r+1$. Indeed, the user can pre-compute
their credentials for round $r+1$ for each potential seed, and broadcast only
the credential (among those with a low enough score to be the leader) that
produces the most favorable seed.
  We consider an adversary who wishes to maximize the expected fraction of
rounds in which an account they own is the leader. We show such an adversary
always benefits from deviating from the intended protocol, regardless of the
fraction of the stake controlled. We characterize the optimal strategy; first
by proving the existence of optimal positive recurrent strategies whenever the
adversary owns last than $38\%$ of the stake. Then, we provide a Markov
Decision Process formulation to compute the optimal strategy.",2207.07996v1,cs.CR,2022-07-16 18:28:07+00:00,"[arxiv.Result.Author('Matheus V. X. Ferreira'), arxiv.Result.Author('Ye Lin Sally Hahn'), arxiv.Result.Author('S. Matthew Weinberg'), arxiv.Result.Author('Catherine Yu')]","Proceedings of the 23rd ACM Conference on Economics and
  Computation, 2022, 89-114"
199,Authentication Without Identification using Anonymous Credential System,"Privacy and security are often intertwined. For example, identity theft is
rampant because we have become accustomed to authentication by identification.
To obtain some service, we provide enough information about our identity for an
unscrupulous person to steal it (for example, we give our credit card number to
Amazon.com). One of the consequences is that many people avoid e-commerce
entirely due to privacy and security concerns. The solution is to perform
authentication without identification. In fact, all on-line actions should be
as anonymous as possible, for this is the only way to guarantee security for
the overall system. A credential system is a system in which users can obtain
credentials from organizations and demonstrate possession of these credentials.
Such a system is anonymous when transactions carried out by the same user
cannot be linked. An anonymous credential system is of significant practical
relevance because it is the best means of providing privacy for users.",0908.0979v1,cs.CR,2009-08-07 04:02:05+00:00,"[arxiv.Result.Author('A. Damodaram'), arxiv.Result.Author('H. Jayasri')]","International Journal of Computer Science and Information
  Security, IJCSIS, Vol. 3, No. 1, July 2009, USA"
200,Measuring Congressional District Meandering,"In recent decades, state legislatures have often drawn U.S. Congressional
voting districts that look---to the human eye---to be rather twisted. In this
paper, we propose a method to measure how much districts ""meander"" via a
computation of the medial axis of the region. We then compare this to the
medial axis of the convex hull of the district to obtain the {\em medial-hull
ratio}: a dimensionless quantity that captures the district's irregularity. We
compute this quantity for many example Congressional districts.",1805.08208v1,math.HO,2018-05-20 02:21:07+00:00,"[arxiv.Result.Author('Eion Blanchard'), arxiv.Result.Author('Kevin Knudson')]",
201,A mathematical evaluation of vote transfer systems,"The paper builds a general model of vote transfer systems on the basis of the
current Hungarian electoral rules. It combines single-seat districts and list
mandates with three possible compensation rule for 'wasted' votes in
constituencies: no compensation (direct vote transfer, DVT), compensation for
votes cast for losing party candidates (positive vote transfer, PVT) and
compensation for all votes that are not necessary to win the district (negative
vote transfer, NVT).
  The model is studied in the case of two parties. When the number of votes for
the majority party follows a uniform distribution in each district, DVT results
in the greatest expected seat share, however, application of PVT, and,
especially, NVT increases the probability of winning the election. The
trade-off between vote transfer formulas and the number of list mandates
reveals that the majority party should use an appropriately calibrated NVT
system if it focuses on these two variables.",1607.02879v3,cs.GT,2016-07-11 09:58:58+00:00,[arxiv.Result.Author('László Csató')],
202,Minimizing Margin of Victory for Fair Political and Educational Districting,"In many practical scenarios, a population is divided into disjoint groups for
better administration, e.g., electorates into political districts, employees
into departments, students into school districts, and so on. However, grouping
people arbitrarily may lead to biased partitions, raising concerns of
gerrymandering in political districting, racial segregation in schools, etc. To
counter such issues, in this paper, we conceptualize such problems in a voting
scenario, and propose FAIR DISTRICTING problem to divide a given set of people
having preference over candidates into k groups such that the maximum margin of
victory of any group is minimized. We also propose the FAIR CONNECTED
DISTRICTING problem which additionally requires each group to be connected. We
show that the FAIR DISTRICTING problem is NP-complete for plurality voting even
if we have only 3 candidates but admits polynomial time algorithms if we assume
k to be some constant or everyone can be moved to any group. In contrast, we
show that the FAIR CONNECTED DISTRICTING problem is NP-complete for plurality
voting even if we have only 2 candidates and k = 2. Finally, we propose
heuristic algorithms for both the problems and show their effectiveness in UK
political districting and in lowering racial segregation in public schools in
the US.",1909.05583v1,cs.SI,2019-09-12 11:50:25+00:00,"[arxiv.Result.Author('Ana-Andreea Stoica'), arxiv.Result.Author('Abhijnan Chakraborty'), arxiv.Result.Author('Palash Dey'), arxiv.Result.Author('Krishna P. Gummadi')]",
203,Gerrymandering and fair districting in parallel voting systems,"Switching from one electoral system to another one is frequently criticized
by the opposition and is viewed as a means for the ruling party to stay in
power. In particular, when the new electoral system is a parallel voting (or a
single-member district) system, the ruling party is usually suspected of a
biased way of partitioning the state into electoral districts such that based
on a priori knowledge it has more chances to win in a maximum possible number
of districts. In this paper, we propose a new methodology for deciding whether
a particular party benefits from a given districting map under a parallel
voting system. As a part of our methodology, we formulate and solve several
gerrymandering problems. We showcased the application of our approach to the
Moldovan parliamentary elections of 2019. Our results suggest that contrary to
the arguments of previous studies, there is no clear evidence to consider that
the districting map used in those elections was unfair.",2002.06849v1,physics.soc-ph,2020-02-17 09:17:24+00:00,"[arxiv.Result.Author('Igor Mandric'), arxiv.Result.Author('Igor Roşca'), arxiv.Result.Author('Radu Buzatu')]",
204,Sampling-Based Winner Prediction in District-Based Elections,"In a district-based election, we apply a voting rule $r$ to decide the
winners in each district, and a candidate who wins in a maximum number of
districts is the winner of the election. We present efficient sampling-based
algorithms to predict the winner of such district-based election systems in
this paper. When $r$ is plurality and the margin of victory is known to be at
least $\varepsilon$ fraction of the total population, we present an algorithm
to predict the winner. The sample complexity of our algorithm is
$\mathcal{O}\left(\frac{1}{\varepsilon^4}\log
\frac{1}{\varepsilon}\log\frac{1}{\delta}\right)$. We complement this result by
proving that any algorithm, from a natural class of algorithms, for predicting
the winner in a district-based election when $r$ is plurality, must sample at
least $\Omega\left(\frac{1}{\varepsilon^4}\log\frac{1}{\delta}\right)$ votes.
We then extend this result to any voting rule $r$. Loosely speaking, we show
that we can predict the winner of a district-based election with an extra
overhead of
$\mathcal{O}\left(\frac{1}{\varepsilon^2}\log\frac{1}{\delta}\right)$ over the
sample complexity of predicting the single-district winner under $r$. We
further extend our algorithm for the case when the margin of victory is
unknown, but we have only two candidates. We then consider the median voting
rule when the set of preferences in each district is single-peaked. We show
that the winner of a district-based election can be predicted with
$\mathcal{O}\left(\frac{1}{\varepsilon^4}\log\frac{1}{\varepsilon}\log\frac{1}{\delta}\right)$
samples even when the harmonious order in different districts can be different
and even unknown. Finally, we also show some results for estimating the margin
of victory of a district-based election within both additive and multiplicative
error bounds.",2203.00083v1,cs.AI,2022-02-28 20:32:48+00:00,"[arxiv.Result.Author('Palash Dey'), arxiv.Result.Author('Debajyoti Kar'), arxiv.Result.Author('Swagato Sanyal')]",
205,Comparing Voting Districts with Uncertain Data Envelopment Analysis,"Gerrymandering voting districts is one of the most salient concerns of
contemporary American society, and the creation of new voting maps, along with
their subsequent legal challenges, speaks for much of our modern political
discourse. The legal, societal, and political debate over serviceable voting
districts demands a concept of fairness, which is a loosely characterized, but
amorphous, concept that has evaded precise definition. We advance a new
paradigm to compare voting maps that avoids the pitfalls associated with an a
priori metric being used to uniformly assess maps. Our evaluative method
instead shows how to use uncertain data envelopment analysis to assess maps on
a variety of metrics, a tactic that permits each district to be assessed
separately and optimally. We test our methodology on a collection of proposed
and publicly available maps to illustrate our assessment strategy.",2212.07779v1,physics.soc-ph,2022-09-02 20:12:27+00:00,"[arxiv.Result.Author('Casey Garner'), arxiv.Result.Author('Allen Holder')]",
206,How Segregation Patterns Affect the Availability of Fair District Plans,"We create 4200 synthetic cities which vary in percent minority population and
their residential segregation patterns. Of these, 1200 are modeled on existing
cities, and 3000 are rectangular grid cities. In each city, we consider
single-member voting district plans for a hypothetical city council election. A
fair district plan is defined as one where the number of minority-majority
districts is proportional to the city-wide minority population. Thus each city
is summarized by three traits: minority percent, a measure of segregation, and
availability of a fair district plan. We find that when the minority population
is around 25%-33%, there is a positive correlation between the degree of
segregation and the availability of proportional district plan. Consistently,
when the minority population lives in a more diffuse residential pattern, there
are fewer available proportional district plans. Finally, we develop a new
method to validate runtime and sample size of an ensemble of district plans
created by the GerryChain software program.",2208.13235v1,cs.CY,2022-08-28 15:38:20+00:00,"[arxiv.Result.Author('William Hager'), arxiv.Result.Author('Betseygail Rand')]",
207,"Voting Rights, Markov Chains, and Optimization by Short Bursts","Finding outlying elements in probability distributions can be a hard problem.
Taking a real example from Voting Rights Act enforcement, we consider the
problem of maximizing the number of simultaneous majority-minority districts in
a political districting plan. An unbiased random walk on districting plans is
unlikely to find plans that approach this maximum. A common search approach is
to use a biased random walk: preferentially select districting plans with more
majority-minority districts. Here, we present a third option, called short
bursts, in which an unbiased random walk is performed for a small number of
steps (called the burst length), then re-started from the most extreme plan
that was encountered in the last burst. We give empirical evidence that
short-burst runs outperform biased random walks for the problem of maximizing
the number of majority-minority districts, and that there are many values of
burst length for which we see this improvement. Abstracting from our use case,
we also consider short bursts where the underlying state space is a line with
various probability distributions, and then explore some features of more
complicated state spaces and how these impact the effectiveness of short
bursts.",2011.02288v2,physics.soc-ph,2020-10-23 17:41:30+00:00,"[arxiv.Result.Author('Sarah Cannon'), arxiv.Result.Author('Ari Goldbloom-Helzner'), arxiv.Result.Author('Varun Gupta'), arxiv.Result.Author('JN Matthews'), arxiv.Result.Author('Bhushan Suwal')]",
208,Political Geography and Representation: A Case Study of Districting in Pennsylvania,"This preprint offers a detailed look, both qualitative and quantitative, at
districting with respect to recent voting patterns in one state: Pennsylvania.
We investigate how much the partisan playing field is tilted by political
geography. In particular we closely examine the role of scale. We find that
partisan-neutral maps rarely give seats proportional to votes, and that making
the district size smaller tends to make it even harder to find a proportional
map. This preprint was prepared as a chapter in the forthcoming edited volume
Political Geometry, an interdisciplinary collection of essays on redistricting.
(mggg.org/gerrybook)",2010.14608v2,cs.CY,2020-10-27 21:01:10+00:00,"[arxiv.Result.Author('Jonathan Rodden'), arxiv.Result.Author('Thomas Weighill')]",
209,Quantifying Gerrymandering With Simulated Annealing,"Gerrymandering is the perversion of an election based on manipulation of
voting district boundaries, and has been a historically important yet difficult
task to analytically prove. We propose a Markov Chain Monte Carlo with
Simulated Annealing as a solution for measuring the extent to which a
districting plan is unfair. We put forth promising results in the successful
application of redistricting chains for the state of Texas, using an
implementation of a redistricting Markov Chain with Simulated Annealing to
produce accelerated results. This demonstrates strong evidence that Simulated
Annealing is effective in quickly generating representative voting
distributions for large elections, and furthermore capable of indicating unfair
bias in enacted districting plans.",2209.00624v1,stat.AP,2022-08-16 19:20:21+00:00,[arxiv.Result.Author('Stuart Wayland')],
210,District-Fair Participatory Budgeting,"Participatory budgeting is a method used by city governments to select public
projects to fund based on residents' votes. Many cities use participatory
budgeting at a district level. Typically, a budget is divided among districts
proportionally to their population, and each district holds an election over
local projects and then uses its budget to fund the projects most preferred by
its voters. However, district-level participatory budgeting can yield poor
social welfare because it does not necessarily fund projects supported across
multiple districts. On the other hand, decision making that only takes global
social welfare into account can be unfair to districts: A
social-welfare-maximizing solution might not fund any of the projects preferred
by a district, despite the fact that its constituents pay taxes to the city.
Thus, we study how to fairly maximize social welfare in a participatory
budgeting setting with a single city-wide election. We propose a notion of
fairness that guarantees each district at least as much welfare as it would
have received in a district-level election. We show that, although optimizing
social welfare subject to this notion of fairness is NP-hard, we can
efficiently construct a lottery over welfare-optimal outcomes that is fair in
expectation. Moreover, we show that, when we are allowed to slightly relax
fairness, we can efficiently compute a fair solution that is
welfare-maximizing, but which may overspend the budget.",2102.06115v1,cs.GT,2021-02-11 16:55:57+00:00,"[arxiv.Result.Author('D Ellis Hershkowitz'), arxiv.Result.Author('Anson Kahng'), arxiv.Result.Author('Dominik Peters'), arxiv.Result.Author('Ariel D. Procaccia')]",
211,Partisan gerrymandering with geographically compact districts,"Bizarrely shaped voting districts are frequently lambasted as likely
instances of gerrymandering. In order to systematically identify such
instances, researchers have devised several tests for so-called geographic
compactness (i.e., shape niceness). We demonstrate that under certain
conditions, a party can gerrymander a competitive state into geographically
compact districts to win an average of over 70% of the districts. Our results
suggest that geometric features alone may fail to adequately combat partisan
gerrymandering.",1712.05390v1,math.CO,2017-12-14 18:44:28+00:00,"[arxiv.Result.Author('Boris Alexeev'), arxiv.Result.Author('Dustin G. Mixon')]",J. Appl. Probab. 55 (2018) 1046-1059
212,The distortion of distributed voting,"Voting can abstractly model any decision-making scenario and as such it has
been extensively studied over the decades. Recently, the related literature has
focused on quantifying the impact of utilizing only limited information in the
voting process on the societal welfare for the outcome, by bounding the
distortion of voting rules. Even though there has been significant progress
towards this goal, all previous works have so far neglected the fact that in
many scenarios (like presidential elections) voting is actually a distributed
procedure. In this paper, we consider a setting in which the voters are
partitioned into disjoint districts and vote locally therein to elect local
winning alternatives using a voting rule; the final outcome is then chosen from
the set of these alternatives. We prove tight bounds on the distortion of
well-known voting rules for such distributed elections both from a worst-case
perspective as well as from a best-case one. Our results indicate that the
partition of voters into districts leads to considerably higher distortion, a
phenomenon which we also experimentally showcase using real-world data.",1905.01882v2,cs.GT,2019-05-06 08:42:46+00:00,"[arxiv.Result.Author('Aris Filos-Ratsikas'), arxiv.Result.Author('Evi Micha'), arxiv.Result.Author('Alexandros A. Voudouris')]",
213,Agent-based Simulation of District-based Elections,"In district-based elections, electors cast votes in their respective
districts. In each district, the party with maximum votes wins the
corresponding seat in the governing body. The election result is based on the
number of seats won by different parties. In this system, locations of electors
across the districts may severely affect the election result even if the total
number of votes obtained by different parties remains unchanged. A less popular
party may end up winning more seats if their supporters are suitably
distributed spatially. This happens due to various regional and social
influences on individual voters which modulate their voting choice. In this
paper, we explore agent-based models for district-based elections, where we
consider each elector as an agent, and try to represent their social and
geographical attributes and political inclinations using probability
distributions. This model can be used to simulate election results by Monte
Carlo sampling. The models allow us to explore the full space of possible
outcomes of an electoral setting, though they can also be calibrated to actual
election results for suitable values of parameters. We use Approximate Bayesian
Computation (ABC) framework to estimate model parameters. We show that our
model can reproduce the results of elections held in India and USA, and can
also produce counterfactual scenarios.",2205.14400v1,cs.MA,2022-05-28 11:19:04+00:00,[arxiv.Result.Author('Adway Mitra')],
214,Protecting Elections by Recounting Ballots,"Complexity of voting manipulation is a prominent topic in computational
social choice. In this work, we consider a two-stage voting manipulation
scenario. First, a malicious party (an attacker) attempts to manipulate the
election outcome in favor of a preferred candidate by changing the vote counts
in some of the voting districts. Afterwards, another party (a defender), which
cares about the voters' wishes, demands a recount in a subset of the
manipulated districts, restoring their vote counts to their original values. We
investigate the resulting Stackelberg game for the case where votes are
aggregated using two variants of the Plurality rule, and obtain an almost
complete picture of the complexity landscape, both from the attacker's and from
the defender's perspective.",1906.07071v1,cs.GT,2019-06-17 14:59:08+00:00,"[arxiv.Result.Author('Edith Elkind'), arxiv.Result.Author('Jiarui Gan'), arxiv.Result.Author('Svetlana Obraztsova'), arxiv.Result.Author('Zinovi Rabinovich'), arxiv.Result.Author('Alexandros A. Voudouris')]",
215,Measures of Partisan Bias for Legislating Fair Elections,"Several measures of partisan bias are reviewed for single member districts
with two dominant parties. These include variants of the simple bias that
considers only deviation of seats from 50% at statewide 50% vote. Also included
are equalization of losing votes and equalization of wasted votes, both of
which apply directly when the statewide vote is not 50% and which require, not
just partisan symmetry, but specific forms of the seats-votes curve. A new
measure of bias is introduced, based on the geometric area between the
seats-vote curve and the symmetrically inverted seats-votes curve. These
measures are applied to recent Pennsylvania congressional elections and to
abstract models of the seats-votes curves. The numerical values obtained from
the various measures of bias are compared and contrasted. Each bias measure has
merits for different seats-votes curves and for different elections, but all
essentially agree for most cases when applied to measure only partisan bias,
not conflated with competitiveness. This supports the inclusion of partisan
fairness as a fundamental element for election law reform, and some options are
discussed.",1505.06749v1,physics.soc-ph,2015-05-25 20:27:21+00:00,[arxiv.Result.Author('John F. Nagle')],Election Law Journal 14 (2015) 346-360
216,Combatting Gerrymandering with Social Choice: the Design of Multi-member Districts,"Every representative democracy must specify a mechanism under which voters
choose their representatives. The most common mechanism in the United States --
Winner takes all single-member districts -- both enables substantial partisan
gerrymandering and constrains `fair' redistricting, preventing proportional
representation in legislatures. We study the design of multi-member districts
(MMDs), in which each district elects multiple representatives, potentially
through a non-Winner takes all voting rule. We carry out large-scale empirical
analyses for the U.S. House of Representatives under MMDs with different social
choice functions, under algorithmically generated maps optimized for either
partisan benefit or proportionality. Doing so requires efficiently
incorporating predicted partisan outcomes -- under various multi-winner social
choice functions -- into an algorithm that optimizes over an ensemble of maps.
We find that with three-member districts using Single Transferable Vote,
fairness-minded independent commissions would be able to achieve proportional
outcomes in every state up to rounding, and advantage-seeking partisans would
have their power to gerrymander significantly curtailed. Simultaneously, such
districts would preserve geographic cohesion, an arguably important aspect of
representative democracies. In the process, we advance a rich research agenda
at the intersection of social choice and computational gerrymandering.",2107.07083v4,cs.GT,2021-07-15 02:29:46+00:00,"[arxiv.Result.Author('Nikhil Garg'), arxiv.Result.Author('Wes Gurnee'), arxiv.Result.Author('David Rothschild'), arxiv.Result.Author('David Shmoys')]",
217,Electoral David vs Goliath: How does the Spatial Concentration of Electors affect District-based Elections?,"Many democratic countries use district-based elections where there is a
""seat"" for each district in the governing body. In each district, the party
whose candidate gets the maximum number of votes wins the corresponding seat.
The result of the election is decided based on the number of seats won by the
different parties. The electors (voters) can cast their votes only in the
district of their residence. Thus, locations of the electors and boundaries of
the districts may severely affect the election result even if the proportion of
popular support (number of electors) of different parties remains unchanged.
This has led to significant amount of research on whether the districts may be
redrawn or electors may be moved to maximize seats for a particular party. In
this paper, we frame the spatial distribution of electors in a probabilistic
setting, and explore different models to capture the intra-district
polarization of electors in favour of a party, or the spatial concentration of
supporters of different parties. Our models are inspired by elections in India,
where supporters of different parties tend to be concentrated in certain
districts. We show with extensive simulations that our model can capture
different statistical properties of real elections held in India. We frame
parameter estimation problems to fit our models to the observed election
results. Since analytical calculation of the likelihood functions are
infeasible for our complex models, we use Likelihood-free Inference methods
under the Approximate Bayesian Computation framework. Since this approach is
highly time-consuming, we explore how supervised regression using Logistic
Regression or Deep Neural Networks can be used to speed it up. We also explore
how the election results can change by varying the spatial distributions of the
voters, even when the proportions of popular support of the parties remain
constant.",2006.11865v1,stat.AP,2020-06-21 18:17:57+00:00,[arxiv.Result.Author('Adway Mitra')],
218,Gerrymandering: A Briber's Perspective,"We initiate the study of bribery problem in the context of gerrymandering and
reverse gerrymandering. In our most general problem, the input is a set of
voters having votes over a set of alternatives, a graph on the voters, a
partition of voters into connected districts, cost of every voter for changing
her district, a budget for the briber, and a favorite alternative of the
briber. The briber needs to compute if the given partition can be modified so
that (i) the favorite alternative of the briber wins the resulting election,
(ii) the modification is budget feasible, and (iii) every new district is
connected. We study four natural variants of the above problem -- the graph on
voter being arbitrary vs complete graph (corresponds to removing connectedness
requirement for districts) and the cost of bribing every voter being uniform vs
non-uniform. We show that all the four problems are NP-complete even under
quite restrictive scenarios. Hence our results show that district based
elections are quite resistant under this new kind of electoral attack. We
complement our hardness results with polynomial time algorithms in some other
cases.",1909.01583v1,cs.GT,2019-09-04 07:21:15+00:00,[arxiv.Result.Author('Palash Dey')],
219,Mathematics of Nested Districts: The Case of Alaska,"In eight states, a ""nesting rule"" requires that each state Senate district be
exactly composed of two adjacent state House districts. In this paper we
investigate the potential impacts of these nesting rules with a focus on
Alaska, where Republicans have a 2/3 majority in the Senate while a
Democratic-led coalition controls the House. Treating the current House plan as
fixed and considering all possible pairings, we find that the choice of
pairings alone can create a swing of 4-5 seats out of 20 against recent voting
patterns, which is similar to the range observed when using a Markov chain
procedure to generate plans without the nesting constraint. The analysis
enables other insights into Alaska districting, including the partisan latitude
available to districters with and without strong rules about nesting and
contiguity.",2005.12732v1,cs.SI,2020-05-26 14:02:07+00:00,"[arxiv.Result.Author('Sophia Caldera'), arxiv.Result.Author('Daryl DeFord'), arxiv.Result.Author('Moon Duchin'), arxiv.Result.Author('Samuel C. Gutekunst'), arxiv.Result.Author('Cara Nix')]",
220,Electric Democracy: Proof of Work to secure Elections,"Electronic voting consistently fails to supplant conventional paper ballot
due to a plethora of security shortcomings. Not only are traditional voting
methods mediocre in terms of convenience and interface, they also encompass
principal-agent problem, where the state may have vested interest in the
outcome of the ballot. Electronic voting protocol using cryptography to deliver
in zero trust environments is long overdue. Here I propose using Proof of Work
algorithm at user devices in combination with other well-known security
primitives to build a zero-trust voting system. The state would only issue
single-use voting authorizations to its citizens, while zero trust design would
allow conducting elections by an open-source platform with enhanced
observability. It is also hypothesized that the heighten availability of
plebiscite by means of the proposed design may ultimately change the way our
society participates in the policy making.",2207.07446v1,cs.CR,2022-06-30 11:52:45+00:00,[arxiv.Result.Author('Vitaly Zuevsky')],
221,"STAR-Vote: A Secure, Transparent, Auditable, and Reliable Voting System","In her 2011 EVT/WOTE keynote, Travis County, Texas County Clerk Dana
DeBeauvoir described the qualities she wanted in her ideal election system to
replace their existing DREs. In response, in April of 2012, the authors,
working with DeBeauvoir and her staff, jointly architected STAR-Vote, a voting
system with a DRE-style human interface and a ""belt and suspenders"" approach to
verifiability. It provides both a paper trail and end-to-end cryptography using
COTS hardware. It is designed to support both ballot-level risk-limiting
audits, and auditing by individual voters and observers. The human interface
and process flow is based on modern usability research. This paper describes
the STAR-Vote architecture, which could well be the next-generation voting
system for Travis County and perhaps elsewhere.",1211.1904v1,cs.CR,2012-11-08 17:06:33+00:00,"[arxiv.Result.Author('Josh Benaloh'), arxiv.Result.Author('Mike Byrne'), arxiv.Result.Author('Philip Kortum'), arxiv.Result.Author('Neal McBurnett'), arxiv.Result.Author('Olivier Pereira'), arxiv.Result.Author('Philip B. Stark'), arxiv.Result.Author('Dan S. Wallach')]",
222,"Grafana plugin for visualising vote based consensus mechanisms, and network P2P overlay networks","In this paper, we present a plugin for visualising vote based consensus
mechanisms primarily aimed to help engineers understand and debug blockchain
and distributed ledger protocols. Both tools are built as Grafana plugins and
make no assumptions on the data storage implementation. The plugins can be
configured via Grafana plugin configuration interface to fit the specifics of
the protocol implementation.",2112.01082v1,cs.DC,2021-12-02 09:50:44+00:00,"[arxiv.Result.Author('Daniil Baldouski'), arxiv.Result.Author('Aleksandar Tošić')]",
223,Quantifying Voter Biases in Online Platforms: An Instrumental Variable Approach,"In content-based online platforms, use of aggregate user feedback (say, the
sum of votes) is commonplace as the ""gold standard"" for measuring content
quality. Use of vote aggregates, however, is at odds with the existing
empirical literature, which suggests that voters are susceptible to different
biases -- reputation (e.g., of the poster), social influence (e.g., votes thus
far), and position (e.g., answer position). Our goal is to quantify, in an
observational setting, the degree of these biases in online platforms.
Specifically, what are the causal effects of different impression signals --
such as the reputation of the contributing user, aggregate vote thus far, and
position of content -- on a participant's vote on content? We adopt an
instrumental variable (IV) framework to answer this question. We identify a set
of candidate instruments, carefully analyze their validity, and then use the
valid instruments to reveal the effects of the impression signals on votes. Our
empirical study using log data from Stack Exchange websites shows that the bias
estimates from our IV approach differ from the bias estimates from the ordinary
least squares (OLS) method. In particular, OLS underestimates reputation bias
(1.6--2.2x for gold badges) and position bias (up to 1.9x for the initial
position) and overestimates social influence bias (1.8--2.3x for initial
votes). The implications of our work include: redesigning user interface to
avoid voter biases; making changes to platforms' policy to mitigate voter
biases; detecting other forms of biases in online platforms.",1910.00757v1,cs.SI,2019-10-02 03:00:36+00:00,"[arxiv.Result.Author('Himel Dev'), arxiv.Result.Author('Karrie Karahalios'), arxiv.Result.Author('Hari Sundaram')]","Proceedings of the ACM on Human Computer Interaction, Vol. 3, No.
  CSCW, Article 120. Publication date: November 2019"
224,User Experience Design for E-Voting: How mental models align with security mechanisms,"This paper presents a mobile application for vote-casting and
vote-verification based on the Selene e-voting protocol and explains how it was
developed and implemented using the User Experience Design process. The
resulting interface was tested with 38 participants, and user experience data
was collected via questionnaires and semi-structured interviews on user
experience and perceived security. Results concerning the impact of displaying
security mechanisms on UX were presented in a complementary paper. Here we
expand on this analysis by studying the mental models revealed during the
interviews and compare them with theoretical security notions. Finally, we
propose a list of improvements for designs of future voting protocols.",2105.14901v2,cs.HC,2021-05-31 11:56:09+00:00,"[arxiv.Result.Author('Marie-Laure Zollinger'), arxiv.Result.Author('Verena Distler'), arxiv.Result.Author('Peter B. Roenne'), arxiv.Result.Author('Peter Y. A. Ryan'), arxiv.Result.Author('Carine Lallemand'), arxiv.Result.Author('Vincent Koenig')]","Fourth International Joint Conference on Electronic Voting
  E-Vote-ID 2019, p187--202"
225,Agreement Rate Initialized Maximum Likelihood Estimator for Ensemble Classifier Aggregation and Its Application in Brain-Computer Interface,"Ensemble learning is a powerful approach to construct a strong learner from
multiple base learners. The most popular way to aggregate an ensemble of
classifiers is majority voting, which assigns a sample to the class that most
base classifiers vote for. However, improved performance can be obtained by
assigning weights to the base classifiers according to their accuracy. This
paper proposes an agreement rate initialized maximum likelihood estimator
(ARIMLE) to optimally fuse the base classifiers. ARIMLE first uses a simplified
agreement rate method to estimate the classification accuracy of each base
classifier from the unlabeled samples, then employs the accuracies to
initialize a maximum likelihood estimator (MLE), and finally uses the
expectation-maximization algorithm to refine the MLE. Extensive experiments on
visually evoked potential classification in a brain-computer interface
application show that ARIMLE outperforms majority voting, and also achieves
better or comparable performance with several other state-of-the-art classifier
combination approaches.",1805.04740v1,cs.LG,2018-05-12 15:43:36+00:00,"[arxiv.Result.Author('Dongrui Wu'), arxiv.Result.Author('Vernon J. Lawhern'), arxiv.Result.Author('Stephen Gordon'), arxiv.Result.Author('Brent J. Lance'), arxiv.Result.Author('Chin-Teng Lin')]","IEEE Int'l. Conf. on Systems, Man and Cybernetics, pp. 724-729,
  Budapest, Hungary, 2016"
226,Exploring Crowd Co-creation Scenarios for Sketches,"As a first step towards studying the ability of human crowds and machines to
effectively co-create, we explore several human-only collaborative co-creation
scenarios. The goal in each scenario is to create a digital sketch using a
simple web interface. We find that settings in which multiple humans
iteratively add strokes and vote on the best additions result in the sketches
with highest perceived creativity (value + novelty). Lack of collaboration
leads to a higher variance in quality and lower novelty or surprise.
Collaboration without voting leads to high novelty but low quality.",2005.07328v2,cs.AI,2020-05-15 02:28:35+00:00,"[arxiv.Result.Author('Devi Parikh'), arxiv.Result.Author('C. Lawrence Zitnick')]",
227,The voter model chordal interface in two dimensions,"Consider the voter model on a box of side length $L$ (in the triangular
lattice) with boundary votes fixed forever as type 0 or type 1 on two different
halves of the boundary. Motivated by analogous questions in percolation, we
study several geometric objects at stationarity, as $L\rightarrow \infty$. One
is the interface between the (large -- i.e., boundary connected) 0-cluster and
1-cluster. Another is the set of large ""coalescing classes"" determined by the
coalescing walk process dual to the voter model.",1409.0136v1,math.PR,2014-08-30 17:39:23+00:00,"[arxiv.Result.Author('Mark Holmes'), arxiv.Result.Author('Yevhen Mohylevskyy'), arxiv.Result.Author('Charles M. Newman')]",
228,Generating Single Peaked Votes,"We discuss how to generate singled peaked votes uniformly from the Impartial
Culture model.",1503.02766v1,cs.GT,2015-03-10 04:15:23+00:00,[arxiv.Result.Author('Toby Walsh')],
229,Normalized Range Voting Broadly Resists Control,"We study the behavior of Range Voting and Normalized Range Voting with
respect to electoral control. Electoral control encompasses attempts from an
election chair to alter the structure of an election in order to change the
outcome. We show that a voting system resists a case of control by proving that
performing that case of control is computationally infeasible. Range Voting is
a natural extension of approval voting, and Normalized Range Voting is a simple
variant which alters each vote to maximize the potential impact of each voter.
We show that Normalized Range Voting has among the largest number of control
resistances among natural voting systems.",1005.5698v3,cs.GT,2010-05-31 15:34:11+00:00,[arxiv.Result.Author('Curtis Menton')],
230,Strategic Voting Under Uncertainty About the Voting Method,"Much of the theoretical work on strategic voting makes strong assumptions
about what voters know about the voting situation. A strategizing voter is
typically assumed to know how other voters will vote and to know the rules of
the voting method. A growing body of literature explores strategic voting when
there is uncertainty about how others will vote. In this paper, we study
strategic voting when there is uncertainty about the voting method. We
introduce three notions of manipulability for a set of voting methods: sure,
safe, and expected manipulability. With the help of a computer program, we
identify voting scenarios in which uncertainty about the voting method may
reduce or even eliminate a voter's incentive to misrepresent her preferences.
Thus, it may be in the interest of an election designer who wishes to reduce
strategic voting to leave voters uncertain about which of several reasonable
voting methods will be used to determine the winners of an election.",1907.09110v1,cs.MA,2019-07-22 03:17:19+00:00,"[arxiv.Result.Author('Wesley H. Holliday'), arxiv.Result.Author('Eric Pacuit')]","EPTCS 297, 2019, pp. 252-272"
231,Vote Delegation and Misbehavior,"We study vote delegation with ""well-behaving"" and ""misbehaving"" agents and
compare it with conventional voting. Typical examples for vote delegation are
validation or governance tasks on blockchains. There is a majority of
well-behaving agents, but they may abstain or delegate their vote to other
agents since voting is costly. Misbehaving agents always vote. We compare
conventional voting allowing for abstention with vote delegation. Preferences
of voters are private information and a positive outcome is achieved if
well-behaving agents win. We illustrate that vote delegation leads to quite
different outcomes than conventional voting with abstention. In particular, we
obtain three insights: First, if the number of misbehaving voters, denoted by f
, is high, both voting methods fail to deliver a positive outcome. Second, if f
takes an intermediate value, conventional voting delivers a positive outcome,
while vote delegation fails with probability one. Third, if f is low,
delegation delivers a positive outcome with higher probability than
conventional voting. Finally, our results characterize worst-case outcomes that
can happen in a liquid democracy.",2102.08823v2,cs.GT,2021-02-17 15:32:32+00:00,"[arxiv.Result.Author('Hans Gersbach'), arxiv.Result.Author('Akaki Mamageishvili'), arxiv.Result.Author('Manvir Schneider')]",
232,To Vote Before Decide: A Logless One-Phase Commit Protocol for Highly-Available Datastores,"Highly-available datastores are widely deployed for online applications.
However, many online applications are not contented with the simple data access
interface currently provided by highly-available datastores. Distributed
transaction support is demanded by applications such as large-scale online
payment used by Alipay or Paypal. Current solutions to distributed transaction
can spend more than half of the whole transaction processing time in
distributed commit. An efficient atomic commit protocol is highly desirable.
This paper presents the HACommit protocol, a logless one-phase commit protocol
for highly-available systems. HACommit has transaction participants vote for a
commit before the client decides to commit or abort the transaction; in
comparison, the state-of-the-art practice for distributed commit is to have the
client decide before participants vote. The change enables the removal of both
the participant logging and the coordinator logging steps in the distributed
commit process; it also makes possible that, after the client initiates the
transaction commit, the transaction data is visible to other transactions
within one communication roundtrip time (i.e., one phase). In the evaluation
with extensive experiments, HACommit outperforms recent atomic commit solutions
for highly-available datastores under different workloads. In the best case,
HACommit can commit in one fifth of the time 2PC does.",1701.02408v2,cs.DC,2017-01-10 02:05:49+00:00,"[arxiv.Result.Author('Yuqing Zhu'), arxiv.Result.Author('Philip S. Yu'), arxiv.Result.Author('Guolei Yi'), arxiv.Result.Author('Wenlong Ma'), arxiv.Result.Author('Mengying Guo'), arxiv.Result.Author('Jianxun Liu')]",
233,Voting Power of Teams Working Together,"Voting power determines the ""power"" of individuals who cast votes; their
power is based on their ability to influence the winning-ness of a coalition.
Usually each individual acts alone, casting either all or none of their votes
and is equally likely to do either. This paper extends this standard ""random
voting"" model to allow probabilistic voting, partial voting, and correlated
team voting. We extend the standard Banzhaf metric to account for these cases;
our generalization reduces to the standard metric under ""random voting"", This
new paradigm allows us to answer questions such as ""In the 2013 US Senate, how
much more unified would the Republicans have to be in order to have the same
power as the Democrats in attaining cloture?""",1312.3394v1,cs.GT,2013-12-12 03:19:17+00:00,[arxiv.Result.Author('Daniel Zwillinger')],
234,The Computational Impact of Partial Votes on Strategic Voting,"In many real world elections, agents are not required to rank all candidates.
We study three of the most common methods used to modify voting rules to deal
with such partial votes. These methods modify scoring rules (like the Borda
count), elimination style rules (like single transferable vote) and rules based
on the tournament graph (like Copeland) respectively. We argue that with an
elimination style voting rule like single transferable vote, partial voting
does not change the situations where strategic voting is possible. However,
with scoring rules and rules based on the tournament graph, partial voting can
increase the situations where strategic voting is possible. As a consequence,
the computational complexity of computing a strategic vote can change. For
example, with Borda count, the complexity of computing a strategic vote can
decrease or stay the same depending on how we score partial votes.",1405.7714v1,cs.GT,2014-05-28 12:13:52+00:00,"[arxiv.Result.Author('Nina Narodytska'), arxiv.Result.Author('Toby Walsh')]",
235,FASTEN: Fair and Secure Distributed Voting Using Smart Contracts,"Electing democratic representatives via voting has been a common mechanism
since the 17th century. However, these mechanisms raise concerns about
fairness, privacy, vote concealment, fair calculations of tally, and proxies
voting on their behalf for the voters. Ballot voting, and in recent times,
electronic voting via electronic voting machines (EVMs) improves fairness by
relying on centralized trust. Homomorphic encryption-based voting protocols
also assure fairness but cannot scale to large scale elections such as
presidential elections. In this paper, we leverage the blockchain technology of
distributing trust to propose a smart contract-based protocol, namely, \proto.
There are many existing protocols for voting using smart contracts. We observe
that these either are not scalable or leak the vote tally during the voting
stage, i.e., do not provide vote concealment. In contrast, we show that FASTEN
preserves voter's privacy ensures vote concealment, immutability, and avoids
double voting. We prove that the probability of privacy breaches is negligibly
small. Further, our cost analysis of executing FASTEN over Ethereum is
comparable to most of the existing cost of elections.",2102.10594v1,cs.CR,2021-02-21 12:29:23+00:00,"[arxiv.Result.Author('Sankarshan Damle'), arxiv.Result.Author('Sujit Gujar'), arxiv.Result.Author('Moin Hussain Moti')]",
236,Using a Model of Social Dynamics to Predict Popularity of News,"Popularity of content in social media is unequally distributed, with some
items receiving a disproportionate share of attention from users. Predicting
which newly-submitted items will become popular is critically important for
both companies that host social media sites and their users. Accurate and
timely prediction would enable the companies to maximize revenue through
differential pricing for access to content or ad placement. Prediction would
also give consumers an important tool for filtering the ever-growing amount of
content. Predicting popularity of content in social media, however, is
challenging due to the complex interactions among content quality, how the
social media site chooses to highlight content, and influence among users.
While these factors make it difficult to predict popularity \emph{a priori}, we
show that stochastic models of user behavior on these sites allows predicting
popularity based on early user reactions to new content. By incorporating
aspects of the web site design, such models improve on predictions based on
simply extrapolating from the early votes. We validate this claim on the social
news portal Digg using a previously-developed model of social voting based on
the Digg user interface.",1004.5354v1,cs.CY,2010-04-29 18:11:47+00:00,"[arxiv.Result.Author('Kristina Lerman'), arxiv.Result.Author('Tad Hogg')]","In Proceedings of 19th International World Wide Web Conference
  (WWW10), 2010"
237,Approval Voting and Incentives in Crowdsourcing,"The growing need for labeled training data has made crowdsourcing an
important part of machine learning. The quality of crowdsourced labels is,
however, adversely affected by three factors: (1) the workers are not experts;
(2) the incentives of the workers are not aligned with those of the requesters;
and (3) the interface does not allow workers to convey their knowledge
accurately, by forcing them to make a single choice among a set of options. In
this paper, we address these issues by introducing approval voting to utilize
the expertise of workers who have partial knowledge of the true answer, and
coupling it with a (""strictly proper"") incentive-compatible compensation
mechanism. We show rigorous theoretical guarantees of optimality of our
mechanism together with a simple axiomatic characterization. We also conduct
preliminary empirical studies on Amazon Mechanical Turk which validate our
approach.",1502.05696v3,cs.GT,2015-02-19 20:42:55+00:00,"[arxiv.Result.Author('Nihar B. Shah'), arxiv.Result.Author('Dengyong Zhou'), arxiv.Result.Author('Yuval Peres')]",
238,"Time Majority Voting, a PC-based EEG Classifier for Non-expert Users","Using Machine Learning and Deep Learning to predict cognitive tasks from
electroencephalography (EEG) signals is a rapidly advancing field in
Brain-Computer Interfaces (BCI). In contrast to the fields of computer vision
and natural language processing, the data amount of these trials is still
rather tiny. Developing a PC-based machine learning technique to increase the
participation of non-expert end-users could help solve this data collection
issue. We created a novel algorithm for machine learning called Time Majority
Voting (TMV). In our experiment, TMV performed better than cutting-edge
algorithms. It can operate efficiently on personal computers for classification
tasks involving the BCI. These interpretable data also assisted end-users and
researchers in comprehending EEG tests better.",2207.12662v1,cs.LG,2022-07-26 05:43:54+00:00,"[arxiv.Result.Author('Guangyao Dou'), arxiv.Result.Author('Zheng Zhou'), arxiv.Result.Author('Xiaodong Qu')]",
239,"They may look and look, yet not see: BMDs cannot be tested adequately","Bugs, misconfiguration, and malware can cause ballot-marking devices (BMDs)
to print incorrect votes. Several approaches to testing BMDs have been
proposed. In logic and accuracy testing (LAT) and parallel or live testing,
auditors input known test votes into the BMD and check the printout. Passive
testing monitors the rate of ""spoiled"" BMD printout, on the theory that if BMDs
malfunction, the rate will increase noticeably. We show that these approaches
cannot reliably detect outcome-altering problems, because: (i) The number of
possible interactions with BMDs is enormous, so testing interactions uniformly
at random is hopeless. (ii) To probe the space of interactions intelligently
requires an accurate model of voter behavior, but because the space of
interactions is so large, building an accurate model requires observing a huge
number of voters in every jurisdiction in every election--more voters than
there are in most jurisdictions. (iii) Even with a perfect model of voter
behavior, the number of tests needed exceeds the number of voters in most
jurisdictions. (iv) An attacker can target interactions that are expensive to
test, e.g., because they involve voting slowly; or interactions for which
tampering is less likely to be noticed, e.g., because the voter uses the audio
interface. (v) Whether BMDs misbehave or not, the distribution of spoiled
ballots is unknown and varies by election and possibly by ballot style:
historical data do not help much. Hence, there is no way to calibrate a
threshold for passive testing, e.g., to guarantee at least a 95% chance of
noticing that 5% of the votes were altered, with at most a 5% false alarm rate.
(vi) Even if the distribution of spoiled ballots were known to be Poisson, the
vast majority of jurisdictions do not have enough voters for passive testing to
have a large chance of detecting problems but only a small chance of false
alarms.",1908.08144v4,stat.AP,2019-08-21 23:39:04+00:00,"[arxiv.Result.Author('Philip B. Stark'), arxiv.Result.Author('Ran Xie')]",
240,Party Matters: Enhancing Legislative Embeddings with Author Attributes for Vote Prediction,"Predicting how Congressional legislators will vote is important for
understanding their past and future behavior. However, previous work on
roll-call prediction has been limited to single session settings, thus did not
consider generalization across sessions. In this paper, we show that metadata
is crucial for modeling voting outcomes in new contexts, as changes between
sessions lead to changes in the underlying data generation process. We show how
augmenting bill text with the sponsors' ideologies in a neural network model
can achieve an average of a 4% boost in accuracy over the previous
state-of-the-art.",1805.08182v1,cs.CL,2018-05-21 17:03:13+00:00,"[arxiv.Result.Author('Anastassia Kornilova'), arxiv.Result.Author('Daniel Argyle'), arxiv.Result.Author('Vlad Eidelman')]",
241,Community Structure in the United Nations General Assembly,"We study the community structure of networks representing voting on
resolutions in the United Nations General Assembly. We construct networks from
the voting records of the separate annual sessions between 1946 and 2008 in
three different ways: (1) by considering voting similarities as weighted
unipartite networks; (2) by considering voting similarities as weighted, signed
unipartite networks; and (3) by examining signed bipartite networks in which
countries are connected to resolutions. For each formulation, we detect
communities by optimizing network modularity using an appropriate null model.
We compare and contrast the results that we obtain for these three different
network representations. In so doing, we illustrate the need to consider
multiple resolution parameters and explore the effectiveness of each network
representation for identifying voting groups amidst the large amount of
agreement typical in General Assembly votes.",1010.3757v1,physics.soc-ph,2010-10-18 23:40:16+00:00,"[arxiv.Result.Author('Kevin T. Macon'), arxiv.Result.Author('Peter J. Mucha'), arxiv.Result.Author('Mason A. Porter')]","Physica A 391, 343-361 (2012)"
242,Complexity of Manipulating Elections with Few Candidates,"In multiagent settings where the agents have different preferences,
preference aggregation is a central issue. Voting is a general method for
preference aggregation, but seminal results have shown that all general voting
protocols are manipulable. One could try to avoid manipulation by using voting
protocols where determining a beneficial manipulation is hard. Especially among
computational agents, it is reasonable to measure this hardness by
computational complexity. Some earlier work has been done in this area, but it
was assumed that the number of voters and candidates is unbounded. We derive
hardness results for practical multiagent settings where the number of
candidates is small but the number of voters can be large. We show that with
complete information about the others' votes, individual manipulation is easy,
and coalitional manipulation is easy with unweighted voters. However,
constructive coalitional manipulation with weighted voters is intractable for
all of the voting protocols under study, except for the nonrandomized Cup.
Destructive manipulation tends to be easier. Randomizing over instantiations of
the protocols (such as schedules of the Cup protocol) can be used to make
manipulation hard. Finally, we show that under weak assumptions, if weighted
coalitional manipulation with complete information about the others' votes is
hard in some voting protocol, then individual and unweighted manipulation is
hard when there is uncertainty about the others' votes.",cs/0205076v1,cs.GT,2002-05-29 20:27:58+00:00,"[arxiv.Result.Author('Vincent Conitzer'), arxiv.Result.Author('Tuomas Sandholm')]","Proceedings of the 18th National Conference on Artificial
  Intelligence (AAAI-02), Edmonton, Canada, 2002"
243,Brazilian Congress structural balance analysis,"In this work, we study the behavior of Brazilian politicians and political
parties with the help of clustering algorithms for signed social networks. For
this purpose, we extract and analyze a collection of signed networks
representing voting sessions of the lower house of Brazilian National Congress.
We process all available voting data for the period between 2011 and 2016, by
considering voting similarities between members of the Congress to define
weighted signed links. The solutions obtained by solving Correlation Clustering
(CC) problems are the basis for investigating deputies voting networks as well
as questions about loyalty, leadership, coalitions, political crisis, and
social phenomena such as mediation and polarization.",1609.00767v2,cs.SI,2016-09-02 23:29:25+00:00,"[arxiv.Result.Author('Mario Levorato'), arxiv.Result.Author('Yuri Frota')]",
244,Consumers and Curators: Browsing and Voting Patterns on Reddit,"As crowd-sourced curation of news and information become the norm, it is
important to understand not only how individuals consume information through
social news Web sites, but also how they contribute to their ranking systems.
In the present work, we introduce and make available a new dataset containing
the activity logs that recorded all activity for 309 Reddit users for one year.
Using this newly collected data, we present findings that highlight the
browsing and voting behavior of the study's participants. We find that most
users do not read the article that they vote on, and that, in total, 73% of
posts were rated (ie, upvoted or downvoted) without first viewing the content.
We also show evidence of cognitive fatigue in the browsing sessions of users
that are most likely to vote.",1703.05267v1,cs.SI,2017-03-15 17:06:31+00:00,"[arxiv.Result.Author('Maria Glenski'), arxiv.Result.Author('Corey Pennycuff'), arxiv.Result.Author('Tim Weninger')]",
245,Relevance of Negative Links in Graph Partitioning: A Case Study Using Votes From the European Parliament,"In this paper, we want to study the informative value of negative links in
signed complex networks. For this purpose, we extract and analyze a collection
of signed networks representing voting sessions of the European Parliament
(EP). We first process some data collected by the VoteWatch Europe Website for
the whole 7 th term (2009-2014), by considering voting similarities between
Members of the EP to define weighted signed links. We then apply a selection of
community detection algorithms, designed to process only positive links, to
these data. We also apply Parallel Iterative Local Search (Parallel ILS), an
algorithm recently proposed to identify balanced partitions in signed networks.
Our results show that, contrary to the conclusions of a previous study focusing
on other data, the partitions detected by ignoring or considering the negative
links are indeed remarkably different for these networks. The relevance of
negative links for graph partitioning therefore is an open question which
should be further explored.",1507.04215v2,cs.SI,2015-07-15 13:46:55+00:00,"[arxiv.Result.Author('Israel Mendonça'), arxiv.Result.Author('Rosa Figueiredo'), arxiv.Result.Author('Vincent Labatut'), arxiv.Result.Author('Philippe Michelon')]",
246,Automatic artifact removal of resting-state fMRI with Deep Neural Networks,"Functional Magnetic Resonance Imaging (fMRI) is a non-invasive technique for
studying brain activity. During an fMRI session, the subject executes a set of
tasks (task-related fMRI study) or no tasks (resting-state fMRI), and a
sequence of 3-D brain images is obtained for further analysis. In the course of
fMRI, some sources of activation are caused by noise and artifacts. The removal
of these sources is essential before the analysis of the brain activations.
Deep Neural Network (DNN) architectures can be used for denoising and artifact
removal. The main advantage of DNN models is the automatic learning of abstract
and meaningful features, given the raw data. This work presents advanced DNN
architectures for noise and artifact classification, using both spatial and
temporal information in resting-state fMRI sessions. The highest performance is
achieved by a voting schema using information from all the domains, with an
average accuracy of over 98% and a very good balance between the metrics of
sensitivity and specificity (98.5% and 97.5% respectively).",2011.12113v2,eess.IV,2020-11-15 16:57:58+00:00,"[arxiv.Result.Author('Christos Theodoropoulos'), arxiv.Result.Author('Christos Chatzichristos'), arxiv.Result.Author('Sabine Van Huffel')]",
247,Modelling Sequential Music Track Skips using a Multi-RNN Approach,"Modelling sequential music skips provides streaming companies the ability to
better understand the needs of the user base, resulting in a better user
experience by reducing the need to manually skip certain music tracks. This
paper describes the solution of the University of Copenhagen DIKU-IR team in
the 'Spotify Sequential Skip Prediction Challenge', where the task was to
predict the skip behaviour of the second half in a music listening session
conditioned on the first half. We model this task using a Multi-RNN approach
consisting of two distinct stacked recurrent neural networks, where one network
focuses on encoding the first half of the session and the other network focuses
on utilizing the encoding to make sequential skip predictions. The encoder
network is initialized by a learned session-wide music encoding, and both of
them utilize a learned track embedding. Our final model consists of a majority
voted ensemble of individually trained models, and ranked 2nd out of 45
participating teams in the competition with a mean average accuracy of 0.641
and an accuracy on the first skip prediction of 0.807. Our code is released at
https://github.com/Varyn/WSDM-challenge-2019-spotify.",1903.08408v1,cs.IR,2019-03-20 09:47:22+00:00,"[arxiv.Result.Author('Christian Hansen'), arxiv.Result.Author('Casper Hansen'), arxiv.Result.Author('Stephen Alstrup'), arxiv.Result.Author('Jakob Grue Simonsen'), arxiv.Result.Author('Christina Lioma')]","12th ACM International Conference on Web Search and Data Mining
  (WSDM) 2019, WSDM Cup"
248,"Voting on Cyclic Orders, Group Theory, and Ballots","A cyclic order may be thought of informally as a way to seat people around a
table, perhaps for a game of chance or for dinner. Given a set of agents such
as $\{A,B,C\}$, we can formalize this by defining a cyclic order as a
permutation or linear order on this finite set, under the equivalence relation
where $A\succ B\succ C$ is identified with both $B\succ C\succ A$ and $C\succ
A\succ B$. As with other collections of sets with some structure, we might want
to aggregate preferences of a (possibly different) set of voters on the set of
possible ways to choose a cyclic order.
  However, given the combinatorial explosion of the number of full rankings of
cyclic orders, one may not wish to use the usual voting machinery. This raises
the question of what sort of ballots may be appropriate; a single cyclic order,
a set of them, or some other ballot type? Further, there is a natural action of
the group of permutations on the set of agents. A reasonable requirement for a
choice procedure would be to respect this symmetry (the equivalent of
neutrality in normal voting theory).
  In this paper we will exploit the representation theory of the symmetric
group to analyze several natural types of ballots for voting on cyclic orders,
and points-based procedures using such ballots. We provide a full
characterization of such procedures for two quite different ballot types for
$n=4$, along with the most important observations for $n=5$.",2211.04545v1,cs.GT,2022-11-08 20:32:14+00:00,"[arxiv.Result.Author('Karl-Dieter Crisman'), arxiv.Result.Author('Abraham Holleran'), arxiv.Result.Author('Micah Martin'), arxiv.Result.Author('Josephine Noonan')]",
249,"""Read My Lips"": Using Automatic Text Analysis to Classify Politicians by Party and Ideology","The increasing digitization of political speech has opened the door to
studying a new dimension of political behavior using text analysis. This work
investigates the value of word-level statistical data from the US Congressional
Record--which contains the full text of all speeches made in the US
Congress--for studying the ideological positions and behavior of senators.
Applying machine learning techniques, we use this data to automatically
classify senators according to party, obtaining accuracy in the 70-95% range
depending on the specific method used. We also show that using text to predict
DW-NOMINATE scores, a common proxy for ideology, does not improve upon these
already-successful results. This classification deteriorates when applied to
text from sessions of Congress that are four or more years removed from the
training set, pointing to a need on the part of voters to dynamically update
the heuristics they use to evaluate party based on political speech. Text-based
predictions are less accurate than those based on voting behavior, supporting
the theory that roll-call votes represent greater commitment on the part of
politicians and are thus a more accurate reflection of their ideological
preferences. However, the overall success of the machine learning approaches
studied here demonstrates that political speeches are highly predictive of
partisan affiliation. In addition to these findings, this work also introduces
the computational tools and methods relevant to the use of political speech
data.",1809.00741v1,econ.GN,2018-09-03 23:13:00+00:00,[arxiv.Result.Author('Eitan Sapiro-Gheiler')],
250,Generating Single Peaked Votes,"We discuss how to generate singled peaked votes uniformly from the Impartial
Culture model.",1503.02766v1,cs.GT,2015-03-10 04:15:23+00:00,[arxiv.Result.Author('Toby Walsh')],
251,Normalized Range Voting Broadly Resists Control,"We study the behavior of Range Voting and Normalized Range Voting with
respect to electoral control. Electoral control encompasses attempts from an
election chair to alter the structure of an election in order to change the
outcome. We show that a voting system resists a case of control by proving that
performing that case of control is computationally infeasible. Range Voting is
a natural extension of approval voting, and Normalized Range Voting is a simple
variant which alters each vote to maximize the potential impact of each voter.
We show that Normalized Range Voting has among the largest number of control
resistances among natural voting systems.",1005.5698v3,cs.GT,2010-05-31 15:34:11+00:00,[arxiv.Result.Author('Curtis Menton')],
252,Strategic Voting Under Uncertainty About the Voting Method,"Much of the theoretical work on strategic voting makes strong assumptions
about what voters know about the voting situation. A strategizing voter is
typically assumed to know how other voters will vote and to know the rules of
the voting method. A growing body of literature explores strategic voting when
there is uncertainty about how others will vote. In this paper, we study
strategic voting when there is uncertainty about the voting method. We
introduce three notions of manipulability for a set of voting methods: sure,
safe, and expected manipulability. With the help of a computer program, we
identify voting scenarios in which uncertainty about the voting method may
reduce or even eliminate a voter's incentive to misrepresent her preferences.
Thus, it may be in the interest of an election designer who wishes to reduce
strategic voting to leave voters uncertain about which of several reasonable
voting methods will be used to determine the winners of an election.",1907.09110v1,cs.MA,2019-07-22 03:17:19+00:00,"[arxiv.Result.Author('Wesley H. Holliday'), arxiv.Result.Author('Eric Pacuit')]","EPTCS 297, 2019, pp. 252-272"
253,Vote Delegation and Misbehavior,"We study vote delegation with ""well-behaving"" and ""misbehaving"" agents and
compare it with conventional voting. Typical examples for vote delegation are
validation or governance tasks on blockchains. There is a majority of
well-behaving agents, but they may abstain or delegate their vote to other
agents since voting is costly. Misbehaving agents always vote. We compare
conventional voting allowing for abstention with vote delegation. Preferences
of voters are private information and a positive outcome is achieved if
well-behaving agents win. We illustrate that vote delegation leads to quite
different outcomes than conventional voting with abstention. In particular, we
obtain three insights: First, if the number of misbehaving voters, denoted by f
, is high, both voting methods fail to deliver a positive outcome. Second, if f
takes an intermediate value, conventional voting delivers a positive outcome,
while vote delegation fails with probability one. Third, if f is low,
delegation delivers a positive outcome with higher probability than
conventional voting. Finally, our results characterize worst-case outcomes that
can happen in a liquid democracy.",2102.08823v2,cs.GT,2021-02-17 15:32:32+00:00,"[arxiv.Result.Author('Hans Gersbach'), arxiv.Result.Author('Akaki Mamageishvili'), arxiv.Result.Author('Manvir Schneider')]",
254,Partial Order Reduction for Security Protocols,"Security protocols are concurrent processes that communicate using
cryptography with the aim of achieving various security properties. Recent work
on their formal verification has brought procedures and tools for deciding
trace equivalence properties (e.g., anonymity, unlinkability, vote secrecy) for
a bounded number of sessions. However, these procedures are based on a naive
symbolic exploration of all traces of the considered processes which,
unsurprisingly, greatly limits the scalability and practical impact of the
verification tools.
  In this paper, we overcome this difficulty by developing partial order
reduction techniques for the verification of security protocols. We provide
reduced transition systems that optimally eliminate redundant traces, and which
are adequate for model-checking trace equivalence properties of protocols by
means of symbolic execution. We have implemented our reductions in the tool
Apte, and demonstrated that it achieves the expected speedup on various
protocols.",1504.04768v3,cs.CR,2015-04-18 21:58:38+00:00,"[arxiv.Result.Author('David Baelde'), arxiv.Result.Author('Stéphanie Delaune'), arxiv.Result.Author('Lucca Hirschi')]",
255,The Microsoft 2017 Conversational Speech Recognition System,"We describe the 2017 version of Microsoft's conversational speech recognition
system, in which we update our 2016 system with recent developments in
neural-network-based acoustic and language modeling to further advance the
state of the art on the Switchboard speech recognition task. The system adds a
CNN-BLSTM acoustic model to the set of model architectures we combined
previously, and includes character-based and dialog session aware LSTM language
models in rescoring. For system combination we adopt a two-stage approach,
whereby subsets of acoustic models are first combined at the senone/frame
level, followed by a word-level voting via confusion networks. We also added a
confusion network rescoring step after system combination. The resulting system
yields a 5.1\% word error rate on the 2000 Switchboard evaluation set.",1708.06073v2,cs.CL,2017-08-21 03:17:23+00:00,"[arxiv.Result.Author('W. Xiong'), arxiv.Result.Author('L. Wu'), arxiv.Result.Author('F. Alleva'), arxiv.Result.Author('J. Droppo'), arxiv.Result.Author('X. Huang'), arxiv.Result.Author('A. Stolcke')]","Proc. IEEE ICASSP, April 2018, pp. 5934-5938"
256,"Risks for Academic Research Projects, An Empirical Study of Perceived Negative Risks and Possible Responses","Academic research projects receive hundreds of billions of dollars of
government investment each year. They complement business research projects by
focusing on the generation of new foundational knowledge and addressing
societal challenges. Despite the importance of academic research, the
management of it is often undisciplined and ad hoc. It has been postulated that
the inherent uncertainty and complexity of academic research projects make them
challenging to manage. However, based on this study's analysis of input and
voting from more than 500 academic research team members in facilitated risk
management sessions, the most important perceived risks are general, as opposed
to being research specific. Overall participants' top risks related to funding,
team instability, unreliable partners, study participant recruitment, and data
access. Many of these risks would require system- or organization-level
responses that are beyond the scope of individual academic research teams.",2103.08048v1,econ.GN,2021-03-14 21:50:48+00:00,[arxiv.Result.Author('P. Alison Paprica')],
257,Voting Power of Teams Working Together,"Voting power determines the ""power"" of individuals who cast votes; their
power is based on their ability to influence the winning-ness of a coalition.
Usually each individual acts alone, casting either all or none of their votes
and is equally likely to do either. This paper extends this standard ""random
voting"" model to allow probabilistic voting, partial voting, and correlated
team voting. We extend the standard Banzhaf metric to account for these cases;
our generalization reduces to the standard metric under ""random voting"", This
new paradigm allows us to answer questions such as ""In the 2013 US Senate, how
much more unified would the Republicans have to be in order to have the same
power as the Democrats in attaining cloture?""",1312.3394v1,cs.GT,2013-12-12 03:19:17+00:00,[arxiv.Result.Author('Daniel Zwillinger')],
258,The Computational Impact of Partial Votes on Strategic Voting,"In many real world elections, agents are not required to rank all candidates.
We study three of the most common methods used to modify voting rules to deal
with such partial votes. These methods modify scoring rules (like the Borda
count), elimination style rules (like single transferable vote) and rules based
on the tournament graph (like Copeland) respectively. We argue that with an
elimination style voting rule like single transferable vote, partial voting
does not change the situations where strategic voting is possible. However,
with scoring rules and rules based on the tournament graph, partial voting can
increase the situations where strategic voting is possible. As a consequence,
the computational complexity of computing a strategic vote can change. For
example, with Borda count, the complexity of computing a strategic vote can
decrease or stay the same depending on how we score partial votes.",1405.7714v1,cs.GT,2014-05-28 12:13:52+00:00,"[arxiv.Result.Author('Nina Narodytska'), arxiv.Result.Author('Toby Walsh')]",
259,FASTEN: Fair and Secure Distributed Voting Using Smart Contracts,"Electing democratic representatives via voting has been a common mechanism
since the 17th century. However, these mechanisms raise concerns about
fairness, privacy, vote concealment, fair calculations of tally, and proxies
voting on their behalf for the voters. Ballot voting, and in recent times,
electronic voting via electronic voting machines (EVMs) improves fairness by
relying on centralized trust. Homomorphic encryption-based voting protocols
also assure fairness but cannot scale to large scale elections such as
presidential elections. In this paper, we leverage the blockchain technology of
distributing trust to propose a smart contract-based protocol, namely, \proto.
There are many existing protocols for voting using smart contracts. We observe
that these either are not scalable or leak the vote tally during the voting
stage, i.e., do not provide vote concealment. In contrast, we show that FASTEN
preserves voter's privacy ensures vote concealment, immutability, and avoids
double voting. We prove that the probability of privacy breaches is negligibly
small. Further, our cost analysis of executing FASTEN over Ethereum is
comparable to most of the existing cost of elections.",2102.10594v1,cs.CR,2021-02-21 12:29:23+00:00,"[arxiv.Result.Author('Sankarshan Damle'), arxiv.Result.Author('Sujit Gujar'), arxiv.Result.Author('Moin Hussain Moti')]",
260,Equation of States for Elections,"In the US 2008 presidential election, Barack Obama was elected as the 44th
president of United States, winning the 53% of popular votes and 68% of
electoral votes; in the election of 2000, Al Gore lost the election receiving
49 % of electoral votes, although he had more popular votes. It is generally
believed that the electoral votes and the popular votes are correlated; however
the detailed quantitative relationship for these two quantities is unclear.
Here, we found an interesting relationship between fractions of electoral votes
and fractions of popular votes in the presidential elections of the United
States by examining the election results from 1932 to 2004. Moreover, this
curve could provide an interesting explanation for the results of other
elections that have taken place in Taiwan.",1211.1825v1,physics.soc-ph,2012-11-08 10:59:50+00:00,[arxiv.Result.Author('Bih-Yaw Jin')],
261,Distributed Voting/Ranking with Optimal Number of States per Node,"Considering a network with $n$ nodes, where each node initially votes for one
(or more) choices out of $K$ possible choices, we present a Distributed
Multi-choice Voting/Ranking (DMVR) algorithm to determine either the choice
with maximum vote (the voting problem) or to rank all the choices in terms of
their acquired votes (the ranking problem). The algorithm consolidates node
votes across the network by updating the states of interacting nodes using two
key operations, the union and the intersection. The proposed algorithm is
simple, independent from network size, and easily scalable in terms of the
number of choices $K$, using only $K\times 2^{K-1}$ nodal states for voting,
and $K\times K!$ nodal states for ranking. We prove the number of states to be
optimal in the ranking case, this optimality is conjectured to also apply to
the voting case. The time complexity of the algorithm is analyzed in complete
graphs. We show that the time complexity for both ranking and voting is
$O(\log(n))$ for given vote percentages, and is inversely proportional to the
minimum of the vote percentage differences among various choices.",1703.08838v1,cs.DC,2017-03-26 16:19:31+00:00,"[arxiv.Result.Author('Saber Salehkaleybar'), arxiv.Result.Author('Arsalan Sharif-Nassab'), arxiv.Result.Author('S. Jamaloddin Golestani')]",
262,"Voting power and Qualified Majority Voting with a ""no vote"" option","In recent years, enlargement of the European Union has led to increased
interest in the allocation of voting weights to member states with hugely
differing population numbers. While the eventually agreed voting scheme lacks
any strict mathematical basis, the Polish government suggested a voting scheme
based on the Penrose definition of voting power, leading to an allocation of
voting weights proportional to the square root of the population (the
""Jagiellonian Compromise""). The Penrose definition of voting power is derived
from the citizens' freedom to vote either ""yes"" or ""no"". This paper defines a
corresponding voting power based on ""yes"", ""no"" and ""abstain"" options, and it
is found that this definition also leads to a square root law, and to the same
optimal vote allocation as the Penrose scheme.",0805.3251v2,math.GM,2008-05-21 10:57:28+00:00,[arxiv.Result.Author('Martin Kurth')],
263,"Square root voting system, optimal threshold and π","The problem of designing an optimal weighted voting system for the two-tier
voting, applicable in the case of the Council of Ministers of the European
Union (EU), is investigated. Various arguments in favour of the square root
voting system, where the voting weights of member states are proportional to
the square root of their population are discussed and a link between this
solution and the random walk in the one-dimensional lattice is established. It
is known that the voting power of every member state is approximately equal to
its voting weight, if the threshold q for the qualified majority in the voting
body is optimally chosen. We analyze the square root voting system for a
generic 'union' of M states and derive in this case an explicit approximate
formula for the level of the optimal threshold: q \simeq 1/2+1/\sqrt{{\pi} M}.
The prefactor 1/\sqrt{{\pi}} appears here as a result of averaging over the
ensemble of unions with random populations.",1104.5213v2,physics.soc-ph,2011-04-27 18:51:41+00:00,"[arxiv.Result.Author('Karol Zyczkowski'), arxiv.Result.Author('Wojciech Slomczynski')]",
264,Penrose voting system and optimal quota,"Systems of indirect voting based on the principle of qualified majority can
be analysed using the methods of game theory. In particular, this applies to
the voting system in the Council of the European Union, which was recently a
subject of a vivid political discussion. The a priori voting power of a voter
measures his potential influence over the decisions of the voting body under a
given decision rule. We investigate a system based on the law of Penrose, in
which each representative in the voting body receives the number of votes (the
voting weight) proportional to the square root of the population he or she
represents. Here we demonstrate that for a generic distribution of the
population there exists an optimal quota for which the voting power of any
state is proportional to its weight. The optimal quota is shown to decrease
with the number of voting countries.",physics/0610271v1,physics.soc-ph,2006-10-30 12:49:18+00:00,"[arxiv.Result.Author('Wojciech Slomczynski'), arxiv.Result.Author('Karol Zyczkowski')]","Acta Physica Polonica B37, 3133-3143 (2006)"
265,Square root voting in the Council of the European Union: Rounding effects and the Jagiellonian Compromise,"In recent years, enlargement of the European Union has brought with it
renewed discussion of voting arrangements in the Council of the EU. During
these negotiations, the Polish government proposed a voting scheme that gives
each country a voting weight proportional to the square root of its population,
and sets a quota according to an optimality condition (""Jagiellonian
Compromise""). In this paper, the optimal quota is found exactly for the current
population data from the 27 EU member states, and it is found that rounding of
the voting weights can be used to improve the voting scheme.",0712.2699v2,math.GM,2007-12-17 11:57:53+00:00,[arxiv.Result.Author('Martin Kurth')],
266,Analyzing Voting Power in Decentralized Governance: Who controls DAOs?,"We empirically study the state of three prominent DAO governance systems on
the Ethereum blockchain: Compound, Uniswap and ENS. In particular, we examine
how the voting power is distributed in these systems. Using a comprehensive
dataset of all governance token holders, delegates, proposals and votes, we
analyze who holds the voting rights and how they are used to influence
governance decisions.",2204.01176v1,cs.CY,2022-04-03 22:58:01+00:00,"[arxiv.Result.Author('Robin Fritsch'), arxiv.Result.Author('Marino Müller'), arxiv.Result.Author('Roger Wattenhofer')]",
267,Self-tallying Quantum Anonymous Voting,"Anonymous voting is a voting method of hiding the link between a vote and a
voter, the context of which ranges from governmental elections to decision
making in small groups like councils or companies. In this paper, we propose a
quantum anonymous voting protocol assisted by two kinds of entangled quantum
states. Particularly, we provide a mechanism of opening and permuting the
ordered votes of all the voters in an anonymous manner; any party, who is
interested in the voting results, can acquire a permutation copy, and then
obtains the voting result through simple calculation. Unlike all previous
quantum works on anonymous voting, our quantum anonymous protocol firstly
possesses the properties of privacy, self-tallying, non-reusability,
verifiability and fairness at the same time. Besides, we demonstrate that the
entanglement of the novel quantum states used in our protocol makes the attack
from outside eavesdropper and inside dishonest voters impossible. We also
generalize our protocol to execute tasks of anonymous multi-party computation,
such as anonymous broadcast and anonymous ranking.",1605.07942v1,quant-ph,2016-05-25 15:48:21+00:00,"[arxiv.Result.Author('Qingle Wang'), arxiv.Result.Author('Chaohua Yu'), arxiv.Result.Author('Fei Gao'), arxiv.Result.Author('Haoyu Qi'), arxiv.Result.Author('Qiaoyan Wen')]","Phys. Rev. A 94, 022333 (2016)"
268,From a toy model to the double square root voting system,"We investigate systems of indirect voting based on the law of Penrose, in
which each representative in the voting body receives the number of votes
(voting weight) proportional to the square root of the population he or she
represents. For a generic population distribution the quota required for the
qualified majority can be set in such a way that the voting power of any state
is proportional to its weight. For a specific distribution of population the
optimal quota has to be computed numerically. We analyse a toy voting model for
which the optimal quota can be estimated analytically as a function of the
number of members of the voting body. This result, combined with the normal
approximation technique, allows us to design a simple, efficient, and flexible
voting system which can be easily adopted for varying weights and number of
players.",physics/0701338v3,physics.soc-ph,2007-01-30 14:19:45+00:00,"[arxiv.Result.Author('Wojciech Slomczynski'), arxiv.Result.Author('Karol Zyczkowski')]","Homo Oeconomicus 24, 381 - 399 (2007)"
269,Secure and Verifiable Electronic Voting in Practice: the use of vVote in the Victorian State Election,"The November 2014 Australian State of Victoria election was the first
statutory political election worldwide at State level which deployed an
end-to-end verifiable electronic voting system in polling places. This was the
first time blind voters have been able to cast a fully secret ballot in a
verifiable way, and the first time a verifiable voting system has been used to
collect remote votes in a political election. The code is open source, and the
output from the election is verifiable. The system took 1121 votes from these
particular groups, an increase on 2010 and with fewer polling places.",1504.07098v1,cs.CR,2015-04-27 14:08:24+00:00,"[arxiv.Result.Author('Craig Burton'), arxiv.Result.Author('Chris Culnane'), arxiv.Result.Author('Steve Schneider')]",
270,"Control Complexity in Bucklin, Fallback, and Plurality Voting: An Experimental Approach","Walsh [Wal10, Wal09], Davies et al. [DKNW10, DKNW11], and Narodytska et al.
[NWX11] studied various voting systems empirically and showed that they can
often be manipulated effectively, despite their manipulation problems being
NP-hard. Such an experimental approach is sorely missing for NP-hard control
problems, where control refers to attempts to tamper with the outcome of
elections by adding/deleting/partitioning either voters or candidates. We
experimentally tackle NP-hard control problems for Bucklin and fallback voting.
Among natural voting systems with efficient winner determination, fallback
voting is currently known to display the broadest resistance to control in
terms of NP-hardness, and Bucklin voting has been shown to behave almost as
well in terms of control resistance [ER10, EPR11, EFPR11]. We also investigate
control resistance experimentally for plurality voting, one of the first voting
systems analyzed with respect to electoral control [BTT92, HHR07]. Our findings
indicate that NP-hard control problems can often be solved effectively in
practice. Moreover, our experiments allow a more fine-grained analysis and
comparison-across various control scenarios, vote distribution models, and
voting systems-than merely stating NP-hardness for all these control problems.",1203.3967v2,cs.GT,2012-03-18 16:10:02+00:00,"[arxiv.Result.Author('Joerg Rothe'), arxiv.Result.Author('Lena Schend')]",
271,Tradeoffs in Hierarchical Voting Systems,"Condorcet's jury theorem states that the correct outcome is reached in direct
majority voting systems with sufficiently large electorates as long as each
voter's independent probability of voting for that outcome is greater than 0.5.
Yet, in situations where direct voting systems are infeasible, such as due to
high implementation and infrastructure costs, hierarchical voting systems
provide a reasonable alternative. We study differences in outcome precision
between hierarchical and direct voting systems for varying group sizes,
abstention rates, and voter competencies. Using asymptotic expansions of the
derivative of the reliability function (or Banzhaf number), we first prove that
indirect systems differ most from their direct counterparts when group size and
number are equal to each other, and therefore to $\sqrt{N_{\rm d}}$, where
$N_{\rm d}$ is the total number of voters in the direct system. In multitier
systems, we prove that this difference is maximized when group size equals
$\sqrt[n]{N_{\rm d}}$, where $n$ is the number of hierarchical levels. Second,
we show that while direct majority rule always outperforms hierarchical voting
for homogeneous electorates that vote with certainty, as group numbers and size
increase, hierarchical majority voting gains in its ability to represent all
eligible voters. Furthermore, when voter abstention and competency are
correlated within groups, hierarchical systems often outperform direct voting,
which we show by using a generating function approach that is able to
analytically characterize heterogeneous voting systems.",2110.02298v1,math.CO,2021-10-05 19:01:52+00:00,"[arxiv.Result.Author('Lucas Böttcher'), arxiv.Result.Author('Georgia Kernell')]","Collect. Intell. 1, 1--16 (2022)"
272,Invariably Suboptimal - An attempt to improve the voting rules of Treaties of Nice and Lisbon,"We investigate the voting rules in the Council of the European Union. It is
known that the current system, according to the Treaty of Nice, and the voting
system proposed in the Lisbon treaty both strongly deviate from the square root
law by Penrose. This is known to be the ideal voting rule under certain
assumptions. In 2004 Slomczynski and Zyczkowski designed a voting system, now
known as the Jagiellonian Compromise. It satisfies the square root law with
very high accuracy. Each member state in this system obtains a voting weight
proportional to the square root of the population. Additionally the quota is
fixed in such a way that the voting power of each country is also proportional
to the square root of the population.
  In this paper we investigate to which extent a change of the quota in the
Treaty of Nice and the Treaty of Lisbon may bring the voting power closer to
the ideal square root distribution. Our computations show that even with
optimal quota both systems are way off the ideal power distribution.",0812.4114v3,math.HO,2008-12-22 15:50:24+00:00,"[arxiv.Result.Author('Werner Kirsch'), arxiv.Result.Author('Jessica Langner')]",
273,Digital herders and phase transition in a voting model,"In this paper, we discuss a voting model with two candidates, C_1 and C_2. We
set two types of voters--herders and independents. The voting of independent
voters is based on their fundamental values; on the other hand, the voting of
herders is based on the number of votes. Herders always select the majority of
the previous $r$ votes, which is visible to them. We call them digital herders.
We can accurately calculate the distribution of votes for special cases. When
r>=3, we find that a phase transition occurs at the upper limit of t, where t
is the discrete time (or number of votes). As the fraction of herders
increases, the model features a phase transition beyond which a state where
most voters make the correct choice coexists with one where most of them are
wrong. On the other hand, when r<3, there is no phase transition. In this case,
the herders' performance is the same as that of the independent voters.
Finally, we recognize the behavior of human beings by conducting simple
experiments.",1101.3122v2,physics.soc-ph,2011-01-17 04:15:04+00:00,"[arxiv.Result.Author('Masato Hisakado'), arxiv.Result.Author('Shintaro Mori')]",
274,Empirical analysis and agent-based modeling of Lithuanian parliamentary elections,"In this contribution we analyze a parties' vote share distribution across the
polling stations during the Lithuanian parliamentary elections of 1992, 2008
and 2012. We find that the distribution is rather well fitted by the Beta
distribution. To reproduce this empirical observation we propose a simple
multi-state agent-based model of the voting behavior. In the proposed model
agents change the party they vote for either idiosyncratically or due to a
linear recruitment mechanism. We use the model to reproduce the vote share
distribution observed during the election of 1992. We discuss model extensions
needed to reproduce the vote share distribution observed during the other
elections.",1704.02101v2,physics.soc-ph,2017-04-07 06:13:48+00:00,[arxiv.Result.Author('A. Kononovicius')],Complexity 2017: 7354642
275,Optimizing Voting Order on Sequential Juries: A Median Voter Theorem and Beyond,"We consider an odd-sized ""jury"", which votes sequentially between two states
of Nature (say A and B, or Innocent and Guilty) with the majority opinion
determining the verdict. Jurors have private information in the form of a
signal in [-1,+1], with higher signals indicating A more likely. Each juror has
an ability in [0,1], which is proportional to the probability of A given a
positive signal, an analog of Condorcet's p for binary signals. We assume that
jurors vote honestly for the alternative they view more likely, given their
signal and prior voting, because they are experts who want to enhance their
reputation (after their vote and actual state of Nature is revealed). For a
fixed set of jury abilities, the reliability of the verdict depends on the
voting order. For a jury of size three, the optimal ordering is always as
follows: middle ability first, then highest ability, then lowest. For
sufficiently heterogeneous juries, sequential voting is more reliable than
simultaneous voting and is in fact optimal (allowing for non-honest voting).
When average ability is fixed, verdict reliability is increasing in
heterogeneity.
  For medium-sized juries, we find through simulation that the median ability
juror should still vote first and the remaining ones should have increasing and
then decreasing abilities.",2006.14045v2,econ.TH,2020-06-24 20:58:23+00:00,"[arxiv.Result.Author('Steve Alpern'), arxiv.Result.Author('Bo Chen')]",
276,Partisan Lean of States: Electoral College and Popular Vote,"We compare federal election results for each state versus the USA in every
second year from 1992 to 2018, to model partisan lean of each state and its
dependence on the nationwide popular vote. For each state, we model both its
current partisan lean and its rate of change, as well as sensitivity of state
results with respect to the nationwide popular vote, using Bayesian linear
regression. We apply this to simulate the Electoral College outcome in 2020,
given even (equal) nationwide popular vote, as well as 2016, 2008, and 2004
nationwide popular vote. We backtest 2012 and 2016 elections given actual
popular vote. Taking equal popular vote for two major parties, we prove that
the Electoral College is biased towards Republicans.",1905.04444v5,stat.AP,2019-05-11 04:13:54+00:00,[arxiv.Result.Author('Andrey Sarantsev')],
277,Joint Topic-Semantic-aware Social Recommendation for Online Voting,"Online voting is an emerging feature in social networks, in which users can
express their attitudes toward various issues and show their unique interest.
Online voting imposes new challenges on recommendation, because the propagation
of votings heavily depends on the structure of social networks as well as the
content of votings. In this paper, we investigate how to utilize these two
factors in a comprehensive manner when doing voting recommendation. First, due
to the fact that existing text mining methods such as topic model and semantic
model cannot well process the content of votings that is typically short and
ambiguous, we propose a novel Topic-Enhanced Word Embedding (TEWE) method to
learn word and document representation by jointly considering their topics and
semantics. Then we propose our Joint Topic-Semantic-aware social Matrix
Factorization (JTS-MF) model for voting recommendation. JTS-MF model calculates
similarity among users and votings by combining their TEWE representation and
structural information of social networks, and preserves this
topic-semantic-social similarity during matrix factorization. To evaluate the
performance of TEWE representation and JTS-MF model, we conduct extensive
experiments on real online voting dataset. The results prove the efficacy of
our approach against several state-of-the-art baselines.",1712.00731v1,stat.ML,2017-12-03 08:11:43+00:00,"[arxiv.Result.Author('Hongwei Wang'), arxiv.Result.Author('Jia Wang'), arxiv.Result.Author('Miao Zhao'), arxiv.Result.Author('Jiannong Cao'), arxiv.Result.Author('Minyi Guo')]",
278,A Trustworthy Electronic Voting System for Australian Federal Elections,"The existing system for determining election results in Australia is, for the
most part, secure, accurate and understandable by the average voter. This
thesis explores the design of electronic voting systems designed to achieve
these same goals, while also improving on the existing system in the areas of
counting speed, accuracy, tamper resistance and accessibility.
  Electronic voting systems have seen limited use within Australian elections,
most prominently for State Elections in Victoria (2014), New South Wales (2015)
and Western Australia (2017), along with trials of electronic voting systems in
federal elections in 2007.
  This thesis presents an analysis of the iVote electronic voting system used
for the 2017 Western Australian State Election (iVote WA), outlining a number
of security risks introduced by the use of cloud-based distributed denial of
service mitigation. In addition, this thesis presents the results of a
cross-sectional survey of Australian voters regarding levels of trust for three
voting systems: the existing paper-based system used for Australian federal
elections, the iVote WA system, and the vVote system used for the 2014
Victorian State Election.
  The analysis of iVote, combined with the survey results, are used to inform a
recommendation for future research and public policy regarding the use of
electronic voting systems in Australian federal elections.",1805.02202v1,cs.CR,2018-05-06 12:41:10+00:00,[arxiv.Result.Author('Mark Eldridge')],
279,On Penrose's square-root law and beyond,"In certain bodies, like the Council of the EU, the member states have a
voting weight which depends on the population of the re- spective state. In
this article we ask the question which voting weight guarantees a `fair'
representation of the citizens in the union. The tra- ditional answer, the
square-root law by Penrose, is that the weight of a state (more precisely: the
voting power) should be proportional to the square-root of the population of
this state. The square root law is based on the assumption that the voters in
every state cast their vote inde- pendently of each other. In this paper we
concentrate on cases where the independence assumption is not valid.",math/0611418v1,math.PR,2006-11-14 10:53:27+00:00,[arxiv.Result.Author('Werner Kirsch')],
280,Power-Efficient Direct-Voting Assurance for Data Fusion in Wireless Sensor Networks,"Wireless sensor networks place sensors into an area to collect data and send
them back to a base station. Data fusion, which fuses the collected data before
they are sent to the base station, is usually implemented over the network.
Since the sensor is typically placed in locations accessible to malicious
attackers, information assurance of the data fusion process is very important.
A witness-based approach has been proposed to validate the fusion data. In this
approach, the base station receives the fusion data and ""votes"" on the data
from a randomly chosen sensor node. The vote comes from other sensor nodes,
called ""witnesses,"" to verify the correctness of the fusion data. Because the
base station obtains the vote through the chosen node, the chosen node could
forge the vote if it is compromised. Thus, the witness node must encrypt the
vote to prevent this forgery. Compared with the vote, the encryption requires
more bits, increasing transmission burden from the chosen node to the base
station. The chosen node consumes more power. This work improves the
witness-based approach using direct voting mechanism such that the proposed
scheme has better performance in terms of assurance, overhead, and delay. The
witness node transmits the vote directly to the base station. Forgery is not a
problem in this scheme. Moreover, fewer bits are necessary to represent the
vote, significantly reducing the power consumption. Performance analysis and
simulation results indicate that the proposed approach can achieve a 40 times
better overhead than the witness-based approach.",0705.3683v1,cs.CR,2007-05-25 02:56:47+00:00,"[arxiv.Result.Author('H. -T. Pai'), arxiv.Result.Author('Y. S. Han')]",
281,Empirical analysis and agent-based modeling of Lithuanian parliamentary elections,"In this contribution we analyze a parties' vote share distribution across the
polling stations during the Lithuanian parliamentary elections of 1992, 2008
and 2012. We find that the distribution is rather well fitted by the Beta
distribution. To reproduce this empirical observation we propose a simple
multi-state agent-based model of the voting behavior. In the proposed model
agents change the party they vote for either idiosyncratically or due to a
linear recruitment mechanism. We use the model to reproduce the vote share
distribution observed during the election of 1992. We discuss model extensions
needed to reproduce the vote share distribution observed during the other
elections.",1704.02101v2,physics.soc-ph,2017-04-07 06:13:48+00:00,[arxiv.Result.Author('A. Kononovicius')],Complexity 2017: 7354642
282,Swiss Elections to the National Council: First trials with e-voting in elections at federal level,"On October 23rd 2011, around 22'000 voters will be authorized to cast their
votes electronically in occasion of the elections to the National Council.
These are the first trials ever with e-voting in elections at federal level in
Switzerland. Four cantons are going to conduct trials with this new channel.
Only Swiss voters living abroad will be authorized to participate. The Swiss
Confederation pursues the long term goal of the introduction of e-voting as a
third, complementary voting method - in addition to voting in person at the
polling station and postal voting.",1109.2489v2,cs.CY,2011-09-09 15:03:01+00:00,"[arxiv.Result.Author('Anina Weber'), arxiv.Result.Author('Geo Taglioni')]",
283,Measuring and mitigating voting access disparities: a study of race and polling locations in Florida and North Carolina,"Voter suppression and associated racial disparities in access to voting are
long-standing civil rights concerns in the United States. Barriers to voting
have taken many forms over the decades. A history of violent explicit
discouragement has shifted to more subtle access limitations that can include
long lines and wait times, long travel times to reach a polling station, and
other logistical barriers to voting. Our focus in this work is on quantifying
disparities in voting access pertaining to the overall time-to-vote, and how
they could be remedied via a better choice of polling location or provisioning
more sites where voters can cast ballots. However, appropriately calibrating
access disparities is difficult because of the need to account for factors such
as population density and different community expectations for reasonable
travel times.
  In this paper, we quantify access to polling locations, developing a
methodology for the calibrated measurement of racial disparities in polling
location ""load"" and distance to polling locations. We apply this methodology to
a study of real-world data from Florida and North Carolina to identify
disparities in voting access from the 2020 election. We also introduce
algorithms, with modifications to handle scale, that can reduce these
disparities by suggesting new polling locations from a given list of identified
public locations (including schools and libraries). Applying these algorithms
on the 2020 election location data also helps to expose and explore tradeoffs
between the cost of allocating more polling locations and the potential impact
on access disparities. The developed voting access measurement methodology and
algorithmic remediation technique is a first step in better polling location
assignment.",2205.14867v1,cs.CY,2022-05-30 06:13:19+00:00,"[arxiv.Result.Author('Mohsen Abbasi'), arxiv.Result.Author('Suresh Venkatasubramanian'), arxiv.Result.Author('Sorelle A. Friedler'), arxiv.Result.Author('Kristian Lum'), arxiv.Result.Author('Calvin Barrett')]",
284,Testing for voter rigging in small polling stations,"Since the 1970s there has been a large number of countries that combine
formal democratic institutions with authoritarian practices. Although in such
countries the ruling elites may receive considerable voter support they often
employ several manipulation tools to control election outcomes. A common
practice of these regimes is the coercion and mobilization of a significant
amount of voters to guarantee the electoral victory. This electoral
irregularity is known as voter rigging, distinguishing it from vote rigging,
which involves ballot stuffing or stealing. Here we develop a statistical test
to quantify to which extent the results of a particular election display traces
of voter rigging. Our key hypothesis is that small polling stations are more
susceptible to voter rigging, because it is easier to identify opposing
individuals, there are less eye witnesses, and supposedly less visits from
election observers. We devise a general statistical method for testing whether
voting behavior in small polling stations is significantly different from the
behavior of their neighbor stations in a way that is consistent with the
widespread occurrence of voter rigging. Based on a comparative analysis, the
method enables to rule out whether observed differences in voting behavior
might be explained by geographic heterogeneities in vote preferences. We
analyze 21 elections in ten different countries and find significant anomalies
compatible with voter rigging in Russia from 2007-2011, in Venezuela from
2006-2013, and in Uganda 2011. Particularly disturbing is the case of Venezuela
where these distortions have been outcome-determinative in the 2013
presidential elections.",1604.07689v1,stat.AP,2016-04-26 14:29:08+00:00,"[arxiv.Result.Author('Raúl Jiménez'), arxiv.Result.Author('Manuel Hidalgo'), arxiv.Result.Author('Peter Klimek')]",
285,Distributed Monitoring of Election Winners,"We consider distributed elections, where there is a center and $k$ sites. In
such distributed elections, each voter has preferences over some set of
candidates, and each voter is assigned to exactly one site such that each site
is aware only of the voters assigned to it. The center is able to directly
communicate with all sites. We are interested in designing
communication-efficient protocols, allowing the center to maintain a candidate
which, with arbitrarily high probability, is guaranteed to be a winner, or at
least close to being a winner. We consider various single-winner voting rules,
such as variants of Approval voting and scoring rules, tournament-based voting
rules, and several round-based voting rules. For the voting rules we consider,
we show that, using communication which is logarithmic in the number of voters,
it is possible for the center to maintain such approximate winners; that is,
upon a query at any time the center can immediately return a candidate which is
guaranteed to be an approximate winner with high probability. We complement our
protocols with lower bounds. Our results are theoretical in nature and relate
to various scenarios, such as aggregating customer preferences in online
shopping websites or supermarket chains and collecting votes from different
polling stations of political elections.",1805.02206v2,cs.GT,2018-05-06 13:16:26+00:00,"[arxiv.Result.Author('Arnold Filtser'), arxiv.Result.Author('Nimrod Talmon')]",
286,"Electryo, In-person Voting with Transparent Voter Verifiability and Eligibility Verifiability","Selene is an e-voting protocol that allows voters to directly check their
individual vote, in cleartext, in the final tally via a tracker system, while
providing good coercion mitigation. This is in contrast to conventional,
end-to-end verifiable schemes in which the voter verifies the presence of an
encryption of her vote on the bulletin board. The Selene mechanism can be
applied to many e-voting schemes, but here we present an application to the
polling station context, resulting in a voter-verifiable electronic tally with
a paper audit trail. The system uses a smartcard-based public key system to
provide the individual verification and universal eligibility verifiability.
The paper record contains an encrypted link to the voter's identity, requiring
stronger assumptions on ballot privacy than normal paper voting, but with the
benefit of providing good auditability and dispute resolution as well as
supporting (comparison) risk limiting audits.",2105.14783v1,cs.CR,2021-05-31 08:22:29+00:00,"[arxiv.Result.Author('Peter B. Roenne'), arxiv.Result.Author('Peter Y. A Ryan'), arxiv.Result.Author('Marie-Laure Zollinger')]",
287,Statistical anomalies in 2011-2012 Russian elections revealed by 2D correlation analysis,"Here we perform a statistical analysis of the official data from recent
Russian parliamentary and presidential elections (held on December 4th, 2011
and March 4th, 2012, respectively). A number of anomalies are identified that
persistently skew the results in favour of the pro-government party, United
Russia (UR), and its leader Vladimir Putin. The main irregularities are: (i)
remarkably high correlation between turnout and voting results; (ii) a large
number of polling stations where the UR/Putin results are given by a round
number of percent; (iii) constituencies showing improbably low or (iv)
anomalously high dispersion of results across polling stations; (v) substantial
difference between results at paper-based and electronic polling stations.
These anomalies, albeit less prominent in the presidential elections, hardly
conform to the assumptions of fair and free voting. The approaches proposed
here can be readily extended to quantify fingerprints of electoral fraud in any
other problematic elections.",1205.0741v2,physics.soc-ph,2012-05-03 15:46:04+00:00,"[arxiv.Result.Author('Dmitry Kobak'), arxiv.Result.Author('Sergey Shpilkin'), arxiv.Result.Author('Maxim S. Pshenichnikov')]",
288,Election forensic analysis of the Turkish Constitutional Referendum 2017,"With a majority of 'Yes' votes in the Constitutional Referendum of 2017,
Turkey continues its transition from democracy to autocracy. By the will of the
Turkish people, this referendum transferred practically all executive power to
president Erdogan. However, the referendum was confronted with a substantial
number of allegations of electoral misconducts and irregularities, ranging from
state coercion of 'No' supporters to the controversial validity of unstamped
ballots. In this note we report the results of an election forensic analysis of
the 2017 referendum to clarify to what extent these voting irregularities were
present and if they were able to influence the outcome of the referendum. We
specifically apply novel statistical forensics tests to further identify the
specific nature of electoral malpractices. In particular, we test whether the
data contains fingerprints for ballot-stuffing (submission of multiple ballots
per person during the vote) and voter rigging (coercion and intimidation of
voters). Additionally, we perform tests to identify numerical anomalies in the
election results. We find systematic and highly significant support for the
presence of both, ballot-stuffing and voter rigging. In 6% of stations we find
signs for ballot-stuffing with an error (probability of ballot-stuffing not
happening) of 0.15% (3 sigma event). The influence of these vote distortions
were large enough to tip the overall balance from 'No' to a majority of 'Yes'
votes.",1706.09839v2,stat.AP,2017-06-29 16:33:28+00:00,"[arxiv.Result.Author('Peter Klimek'), arxiv.Result.Author('Raul Jimenez'), arxiv.Result.Author('Manuel Hidalgo'), arxiv.Result.Author('Abraham Hinteregger'), arxiv.Result.Author('Stefan Thurner')]",
289,Dealing with missing data under stratified sampling designs where strata are study domains,"A quick count seeks to estimate the voting trends of an election and
communicate them to the population on the evening of the same day of the
election. In quick counts, the sampling is based on a stratified design of
polling stations. Voting information is gathered gradually, often with no
guarantee of obtaining the complete sample or even information in all the
strata. However, accurate interval estimates with partial information must be
obtained. Furthermore, this becomes more challenging if the strata are
additionally study domains. To produce partial estimates, two strategies are
proposed: 1) A Bayesian model using a dynamic post-stratification strategy and
a single imputation process defined after a thorough analysis of historic
voting information. Additionally, a credibility level correction is included to
solve the underestimation of the variance; 2) a frequentist alternative that
combines standard multiple imputation ideas with classic sampling techniques to
obtain estimates under a missing information framework. Both solutions are
illustrated and compared using information from the 2021 quick count. The aim
was to estimate the composition of the Chamber of Deputies in Mexico.",2208.04475v1,stat.AP,2022-08-09 00:31:59+00:00,"[arxiv.Result.Author('Carlos Rodríguez'), arxiv.Result.Author('Luis Nieto-Barajas'), arxiv.Result.Author('Carlos Pérez-Pérez')]",
290,A Novel Hybrid Biometric Electronic Voting System: Integrating Finger Print and Face Recognition,"A novel hybrid design based electronic voting system is proposed, implemented
and analyzed. The proposed system uses two voter verification techniques to
give better results in comparison to single identification based systems.
Finger print and facial recognition based methods are used for voter
identification. Cross verification of a voter during an election process
provides better accuracy than single parameter identification method. The
facial recognition system uses Viola-Jones algorithm along with rectangular
Haar feature selection method for detection and extraction of features to
develop a biometric template and for feature extraction during the voting
process. Cascaded machine learning based classifiers are used for comparing the
features for identity verification using GPCA (Generalized Principle Component
Analysis) and K-NN (K-Nearest Neighbor). It is accomplished through comparing
the Eigen-vectors of the extracted features with the biometric template
pre-stored in the election regulatory body database. The results of the
proposed system show that the proposed cascaded design based system performs
better than the systems using other classifiers or separate schemes i.e. facial
or finger print based schemes. The proposed system will be highly useful for
real time applications due to the reason that it has 91% accuracy under nominal
light in terms of facial recognition. with bags of paper votes. The central
station compiles and publishes the names of winners and losers through
television and radio stations. This method is useful only if the whole process
is completed in a transparent way. However, there are some drawbacks to this
system. These include higher expenses, longer time to complete the voting
process, fraudulent practices by the authorities administering elections as
well as malpractices by the voters [1]. These challenges result in manipulated
election results.",1801.02430v1,cs.CR,2018-01-05 07:57:29+00:00,"[arxiv.Result.Author('Shahram Najam Syed'), arxiv.Result.Author('Aamir Zeb Shaikh'), arxiv.Result.Author('Shabbar Naqvi')]","Mehran University Research Journal of Engineering and Technology,
  Mehran University Research Journal of Engineering and Technology, 2018, 37
  (1), pp.59-68.
  http://publications.muet.edu.pk/index.php/muetrj/article/view/100/50"
291,Experimental Quantum Solution to the Dining Cryptographers Problem,"Quantum resources such as superposition and entanglement have been used to
provide unconditional key distribution, secret sharing and communication
complexity reduction. In this letter we present a novel quantum information
protocol for dining cryptographers problem and anonymous vote casting by a
group of voters. We successfully demonstrate the experimental realization of
the protocol using single photon transmission. Our implementation employs a
flying particle scheme where a photon passes by the voters who perform a
sequence of actions (unitary transformations) on the photonic state at their
local stations.",1702.01984v1,quant-ph,2017-02-07 12:41:52+00:00,"[arxiv.Result.Author('Alley Hameedi'), arxiv.Result.Author('Breno Marques'), arxiv.Result.Author('Sadiq Muhammad'), arxiv.Result.Author('Marcin Wiesniak'), arxiv.Result.Author('Mohamed Bourennane')]",
292,A copula based approach for electoral quick counts,"An electoral quick count is a statistical procedure whose main objective is
to obtain a relatively small but representative sample of all the polling
stations in a certain election, and to measure the uncertainty about the final
result before the total count of votes. A stratified sampling design is
commonly preferred to reduce estimation variability. The present work shows
that dependence among strata and among candidates should be taken into
consideration for statistical inferences therein, and a copula based model is
proposed and applied to Mexico's 2006, 2012, and 2018 presidential elections
data.",1901.01559v1,stat.AP,2019-01-06 15:58:11+00:00,[arxiv.Result.Author('Arturo Erdely')],
293,Results on Three predictions on July 2012 Federal Elections in Mexico based on past regularities,"The Presidential Election in Mexico of July 2012 has been the third time that
PREP, Previous Electoral Results Program works. PREP gives voting outcomes
based in electoral certificates of each polling station that arrive to capture
centers. In previous ones, some statistical regularities had been observed,
three of them were selected to make predictions and were published in
\texttt{arXiv:1207.0078 [physics.soc-ph]}. Using the database made public in
July 2012, two of the predictions were completely fulfilled, while, the third
one was measured and confirmed using the database obtained upon request to the
electoral authorities. The first two predictions confirmed by actual measures
are: (ii) The Partido Revolucionario Institucional, PRI, is a sprinter and has
a better performance in polling stations arriving late to capture centers
during the process. (iii) Distribution of vote of this party is well described
by a smooth function named a Daisy model. A Gamma distribution, but compatible
with a Daisy model, fits the distribution as well. The third prediction
confirms that {\it errare humanum est}, since the error distributions of all
the self-consistency variables appeared as a central power law with lateral
lobes as in 2000 and 2006 electoral processes. The three measured regularities
appeared no matter the political environment.",1212.6676v4,physics.soc-ph,2012-12-30 00:39:05+00:00,[arxiv.Result.Author('H. Hernández-Saldaña')],
294,Deep Learning Approaches for Forecasting Strawberry Yields and Prices Using Satellite Images and Station-Based Soil Parameters,"Computational tools for forecasting yields and prices for fresh produce have
been based on traditional machine learning approaches or time series modelling.
We propose here an alternate approach based on deep learning algorithms for
forecasting strawberry yields and prices in Santa Barbara county, California.
Building the proposed forecasting model comprises three stages: first, the
station-based ensemble model (ATT-CNN-LSTM-SeriesNet_Ens) with its compound
deep learning components, SeriesNet with Gated Recurrent Unit (GRU) and
Convolutional Neural Network LSTM with Attention layer (Att-CNN-LSTM), are
trained and tested using the station-based soil temperature and moisture data
of SantaBarbara as input and the corresponding strawberry yields or prices as
output. Secondly, the remote sensing ensemble model (SIM_CNN-LSTM_Ens), which
is an ensemble model of Convolutional NeuralNetwork LSTM (CNN-LSTM) models, is
trained and tested using satellite images of the same county as input mapped to
the same yields and prices as output. These two ensembles forecast strawberry
yields and prices with minimal forecasting errors and highest model correlation
for five weeks ahead forecasts.Finally, the forecasts of these two models are
ensembled to have a final forecasted value for yields and prices by introducing
a voting ensemble. Based on an aggregated performance measure (AGM), it is
found that this voting ensemble not only enhances the forecasting performance
by 5% compared to its best performing component model but also outperforms the
Deep Learning (DL) ensemble model found in literature by 33% for forecasting
yields and 21% for forecasting prices",2102.09024v1,cs.LG,2021-02-17 20:54:34+00:00,"[arxiv.Result.Author('Mohita Chaudhary'), arxiv.Result.Author('Mohamed Sadok Gastli'), arxiv.Result.Author('Lobna Nassar'), arxiv.Result.Author('Fakhri Karray')]","AAAI 2021 Spring Symposium on Combining Machine Learning and
  Knowledge Engineering (AAAI-MAKE 2021)"
295,Generating Single Peaked Votes,"We discuss how to generate singled peaked votes uniformly from the Impartial
Culture model.",1503.02766v1,cs.GT,2015-03-10 04:15:23+00:00,[arxiv.Result.Author('Toby Walsh')],
296,Normalized Range Voting Broadly Resists Control,"We study the behavior of Range Voting and Normalized Range Voting with
respect to electoral control. Electoral control encompasses attempts from an
election chair to alter the structure of an election in order to change the
outcome. We show that a voting system resists a case of control by proving that
performing that case of control is computationally infeasible. Range Voting is
a natural extension of approval voting, and Normalized Range Voting is a simple
variant which alters each vote to maximize the potential impact of each voter.
We show that Normalized Range Voting has among the largest number of control
resistances among natural voting systems.",1005.5698v3,cs.GT,2010-05-31 15:34:11+00:00,[arxiv.Result.Author('Curtis Menton')],
297,Strategic Voting Under Uncertainty About the Voting Method,"Much of the theoretical work on strategic voting makes strong assumptions
about what voters know about the voting situation. A strategizing voter is
typically assumed to know how other voters will vote and to know the rules of
the voting method. A growing body of literature explores strategic voting when
there is uncertainty about how others will vote. In this paper, we study
strategic voting when there is uncertainty about the voting method. We
introduce three notions of manipulability for a set of voting methods: sure,
safe, and expected manipulability. With the help of a computer program, we
identify voting scenarios in which uncertainty about the voting method may
reduce or even eliminate a voter's incentive to misrepresent her preferences.
Thus, it may be in the interest of an election designer who wishes to reduce
strategic voting to leave voters uncertain about which of several reasonable
voting methods will be used to determine the winners of an election.",1907.09110v1,cs.MA,2019-07-22 03:17:19+00:00,"[arxiv.Result.Author('Wesley H. Holliday'), arxiv.Result.Author('Eric Pacuit')]","EPTCS 297, 2019, pp. 252-272"
298,Vote Delegation and Misbehavior,"We study vote delegation with ""well-behaving"" and ""misbehaving"" agents and
compare it with conventional voting. Typical examples for vote delegation are
validation or governance tasks on blockchains. There is a majority of
well-behaving agents, but they may abstain or delegate their vote to other
agents since voting is costly. Misbehaving agents always vote. We compare
conventional voting allowing for abstention with vote delegation. Preferences
of voters are private information and a positive outcome is achieved if
well-behaving agents win. We illustrate that vote delegation leads to quite
different outcomes than conventional voting with abstention. In particular, we
obtain three insights: First, if the number of misbehaving voters, denoted by f
, is high, both voting methods fail to deliver a positive outcome. Second, if f
takes an intermediate value, conventional voting delivers a positive outcome,
while vote delegation fails with probability one. Third, if f is low,
delegation delivers a positive outcome with higher probability than
conventional voting. Finally, our results characterize worst-case outcomes that
can happen in a liquid democracy.",2102.08823v2,cs.GT,2021-02-17 15:32:32+00:00,"[arxiv.Result.Author('Hans Gersbach'), arxiv.Result.Author('Akaki Mamageishvili'), arxiv.Result.Author('Manvir Schneider')]",
299,Automatic morphological classification of galaxies: convolutional autoencoder and bagging-based multiclustering model,"In order to obtain morphological information of unlabeled galaxies, we
present an unsupervised machine-learning (UML) method for morphological
classification of galaxies, which can be summarized as two aspects: (1) the
methodology of convolutional autoencoder (CAE) is used to reduce the dimensions
and extract features from the imaging data; (2) the bagging-based
multiclustering model is proposed to obtain the classifications with high
confidence at the cost of rejecting the disputed sources that are
inconsistently voted. We apply this method on the sample of galaxies with
$H<24.5$ in CANDELS. Galaxies are clustered into 100 groups, each contains
galaxies with analogous characteristics. To explore the robustness of the
morphological classifications, we merge 100 groups into five categories by
visual verification, including spheroid, early-type disk, late-type disk,
irregular, and unclassifiable. After eliminating the unclassifiable category
and the sources with inconsistent voting, the purity of the remaining four
subclasses are significantly improved. Massive galaxies ($M_*>10^{10}M_\odot$)
are selected to investigate the connection with other physical properties. The
classification scheme separates galaxies well in the U-V and V-J color space
and Gini-$M_{20}$ space. The gradual tendency of S\'{e}rsic indexes and
effective radii is shown from the spheroid subclass to the irregular subclass.
It suggests that the combination of CAE and multi-clustering strategy is an
effective method to cluster galaxies with similar features and can yield
high-quality morphological classifications. Our study demonstrates the
feasibility of UML in morphological analysis that would develop and serve the
future observations made with China Space Station telescope.",2112.13957v2,astro-ph.GA,2021-12-28 01:17:20+00:00,"[arxiv.Result.Author('C. C. Zhou'), arxiv.Result.Author('Y. Z. Gu'), arxiv.Result.Author('G. W. Fang'), arxiv.Result.Author('Z. S. Lin')]",
300,Generating Single Peaked Votes,"We discuss how to generate singled peaked votes uniformly from the Impartial
Culture model.",1503.02766v1,cs.GT,2015-03-10 04:15:23+00:00,[arxiv.Result.Author('Toby Walsh')],
301,Normalized Range Voting Broadly Resists Control,"We study the behavior of Range Voting and Normalized Range Voting with
respect to electoral control. Electoral control encompasses attempts from an
election chair to alter the structure of an election in order to change the
outcome. We show that a voting system resists a case of control by proving that
performing that case of control is computationally infeasible. Range Voting is
a natural extension of approval voting, and Normalized Range Voting is a simple
variant which alters each vote to maximize the potential impact of each voter.
We show that Normalized Range Voting has among the largest number of control
resistances among natural voting systems.",1005.5698v3,cs.GT,2010-05-31 15:34:11+00:00,[arxiv.Result.Author('Curtis Menton')],
302,Strategic Voting Under Uncertainty About the Voting Method,"Much of the theoretical work on strategic voting makes strong assumptions
about what voters know about the voting situation. A strategizing voter is
typically assumed to know how other voters will vote and to know the rules of
the voting method. A growing body of literature explores strategic voting when
there is uncertainty about how others will vote. In this paper, we study
strategic voting when there is uncertainty about the voting method. We
introduce three notions of manipulability for a set of voting methods: sure,
safe, and expected manipulability. With the help of a computer program, we
identify voting scenarios in which uncertainty about the voting method may
reduce or even eliminate a voter's incentive to misrepresent her preferences.
Thus, it may be in the interest of an election designer who wishes to reduce
strategic voting to leave voters uncertain about which of several reasonable
voting methods will be used to determine the winners of an election.",1907.09110v1,cs.MA,2019-07-22 03:17:19+00:00,"[arxiv.Result.Author('Wesley H. Holliday'), arxiv.Result.Author('Eric Pacuit')]","EPTCS 297, 2019, pp. 252-272"
303,Designing Stable Elections: A Survey,"We survey the design of elections that are resilient to attempted
interference by third parties. For example, suppose votes have been cast in an
election between two candidates, and then each vote is randomly changed with a
small probability, independently of the other votes. It is desirable to keep
the outcome of the election the same, regardless of the changes to the votes.
It is well known that the US electoral college system is about 5 times more
likely to have a changed outcome due to vote corruption, when compared to a
majority vote. In fact, Mossel, O'Donnell and Oleszkiewicz proved in 2005 that
the majority voting method is most stable to this random vote corruption, among
voting methods where each person has a small influence on the election. We
discuss some recent progress on the analogous result for elections between more
than two candidates. In this case, plurality should be most stable to
corruption in votes. We also survey results on adversarial election
manipulation (where an adversary can select particular votes to change, perhaps
in a non-random way), and we briefly discuss ranked choice voting methods
(where a vote is a ranked list of candidates).",2006.05460v2,math.PR,2020-06-09 18:59:48+00:00,[arxiv.Result.Author('Steven Heilman')],
304,Journalistic Voting System's Effects on Election Security Threats and Gerrymandering,"The Journalistic Voting System is a proxy voting system in which journalists
are delegated the task of voting on behalf of individual voters in a
western-style democracy. We introduce the Journalistic Voting System and
discuss its potential advantages and potential problems. In particular, we
discuss its advantages to individuals in the system (voters, journalists, and
politicians) and we discuss its effects relative to several widely discussed
threats to election security, namely: cybersecurity, social media, big data,
artificial intelligence (AI), and gerrymandering. The Journalistic Voting
System is modeled on a predecessor system, called the Valence Voting System,
which is reviewed.",2110.04642v1,physics.soc-ph,2021-10-09 20:57:16+00:00,[arxiv.Result.Author('Lucius Schoenbaum')],
305,Transparent Voting Platform Based on Permissioned Blockchain,"Since 2004, different research was handling the challenges in the centralized
voting systems, e-voting protocols and recently the decentralized voting. So
electronic voting puts forward some difficulties regarding the voter anonymity,
the secure casting of the votes and to prevent the voting process from
frauding. The Decentralized property of the technology called ""blockchain""
could have the solution for many of the challenges in voting research area and
brings a new secure mechanism of safe and transparent voting. In this paper, a
broad comparison between ongoing voting systems has studied by analyzing their
structure and the drawbacks that should consider in future to improve the whole
election process from keeping the privacy of the voter, casting a vote with the
possibility to check if it was counted correctly to publishing the results. The
result of the paper will give a new approach to extend the target of the
election from small scale to large scale despite the fact of Ethereum
limitation which can cast on the blockchain just five votes per minute. The
primary challenge is to find an answer for this question: ""How to balance
between voter privacy and transparency without breaking the important rule
where the voter can proof for a specific candidate that he voted for him in a
bribe situation?"".",1802.10134v1,cs.CY,2018-02-22 14:20:35+00:00,[arxiv.Result.Author('Nazim Faour')],
306,Heuristics in Multi-Winner Approval Voting,"In many real world situations, collective decisions are made using voting.
Moreover, scenarios such as committee or board elections require voting rules
that return multiple winners. In multi-winner approval voting (AV), an agent
may vote for as many candidates as they wish. Winners are chosen by tallying up
the votes and choosing the top-$k$ candidates receiving the most votes. An
agent may manipulate the vote to achieve a better outcome by voting in a way
that does not reflect their true preferences. In complex and uncertain
situations, agents may use heuristics to strategize, instead of incurring the
additional effort required to compute the manipulation which most favors them.
In this paper, we examine voting behavior in multi-winner approval voting
scenarios with complete information. We show that people generally manipulate
their vote to obtain a better outcome, but often do not identify the optimal
manipulation. Instead, voters tend to prioritize the candidates with the
highest utilities. Using simulations, we demonstrate the effectiveness of these
heuristics in situations where agents only have access to partial information.",1905.12104v2,cs.GT,2019-05-28 21:44:07+00:00,"[arxiv.Result.Author('Jaelle Scheuerman'), arxiv.Result.Author('Jason L. Harman'), arxiv.Result.Author('Nicholas Mattei'), arxiv.Result.Author('K. Brent Venable')]",
307,Bucklin Voting is Broadly Resistant to Control,"Electoral control models ways of changing the outcome of an election via such
actions as adding/deleting/partitioning either candidates or voters. These
actions modify an election's participation structure and aim at either making a
favorite candidate win (""constructive control"") or prevent a despised candidate
from winning (""destructive control""), which yields a total of 22 standard
control scenarios. To protect elections from such control attempts,
computational complexity has been used to show that electoral control, though
not impossible, is computationally prohibitive. Among natural voting systems
with a polynomial-time winner problem, the two systems with the highest number
of proven resistances to control types (namely 19 out of 22) are
""sincere-strategy preference-based approval voting"" (SP-AV, a modification of a
system proposed by Brams and Sanver) and fallback voting. Both are hybrid
systems; e.g., fallback voting combines approval with Bucklin voting. In this
paper, we study the control complexity of Bucklin voting itself and show that
it behaves equally well in terms of control resistance for the 20 cases
investigated so far. As Bucklin voting is a special case of fallback voting,
all resistances shown for Bucklin voting in this paper strengthen the
corresponding resistance for fallback voting.",1005.4115v1,cs.CC,2010-05-22 09:12:04+00:00,"[arxiv.Result.Author('Gábor Erdélyi'), arxiv.Result.Author('Lena Piras'), arxiv.Result.Author('Jörg Rothe')]",
308,"Square root voting system, optimal threshold and π","The problem of designing an optimal weighted voting system for the two-tier
voting, applicable in the case of the Council of Ministers of the European
Union (EU), is investigated. Various arguments in favour of the square root
voting system, where the voting weights of member states are proportional to
the square root of their population are discussed and a link between this
solution and the random walk in the one-dimensional lattice is established. It
is known that the voting power of every member state is approximately equal to
its voting weight, if the threshold q for the qualified majority in the voting
body is optimally chosen. We analyze the square root voting system for a
generic 'union' of M states and derive in this case an explicit approximate
formula for the level of the optimal threshold: q \simeq 1/2+1/\sqrt{{\pi} M}.
The prefactor 1/\sqrt{{\pi}} appears here as a result of averaging over the
ensemble of unions with random populations.",1104.5213v2,physics.soc-ph,2011-04-27 18:51:41+00:00,"[arxiv.Result.Author('Karol Zyczkowski'), arxiv.Result.Author('Wojciech Slomczynski')]",
309,Penrose voting system and optimal quota,"Systems of indirect voting based on the principle of qualified majority can
be analysed using the methods of game theory. In particular, this applies to
the voting system in the Council of the European Union, which was recently a
subject of a vivid political discussion. The a priori voting power of a voter
measures his potential influence over the decisions of the voting body under a
given decision rule. We investigate a system based on the law of Penrose, in
which each representative in the voting body receives the number of votes (the
voting weight) proportional to the square root of the population he or she
represents. Here we demonstrate that for a generic distribution of the
population there exists an optimal quota for which the voting power of any
state is proportional to its weight. The optimal quota is shown to decrease
with the number of voting countries.",physics/0610271v1,physics.soc-ph,2006-10-30 12:49:18+00:00,"[arxiv.Result.Author('Wojciech Slomczynski'), arxiv.Result.Author('Karol Zyczkowski')]","Acta Physica Polonica B37, 3133-3143 (2006)"
310,Manipulation Can Be Hard in Tractable Voting Systems Even for Constant-Sized Coalitions,"Voting theory has become increasingly integrated with computational social
choice and multiagent systems. Computational complexity has been extensively
used as a shield against manipulation of voting systems, however for several
voting schemes this complexity may cause calculating the winner to be
computationally difficult. Of the many voting systems that have been studied
with regard to election manipulation, a few have been found to have an
unweighted coalitional manipulation problem that is NP-hard for a constant
number of manipulators despite having a winner problem that is in P. We survey
this interesting class of voting systems and the work that has analyzed their
complexity.",1108.4439v1,cs.GT,2011-08-22 21:02:46+00:00,"[arxiv.Result.Author('Curtis Menton'), arxiv.Result.Author('Preetjot Singh')]",
311,A mathematical evaluation of vote transfer systems,"The paper builds a general model of vote transfer systems on the basis of the
current Hungarian electoral rules. It combines single-seat districts and list
mandates with three possible compensation rule for 'wasted' votes in
constituencies: no compensation (direct vote transfer, DVT), compensation for
votes cast for losing party candidates (positive vote transfer, PVT) and
compensation for all votes that are not necessary to win the district (negative
vote transfer, NVT).
  The model is studied in the case of two parties. When the number of votes for
the majority party follows a uniform distribution in each district, DVT results
in the greatest expected seat share, however, application of PVT, and,
especially, NVT increases the probability of winning the election. The
trade-off between vote transfer formulas and the number of list mandates
reveals that the majority party should use an appropriately calibrated NVT
system if it focuses on these two variables.",1607.02879v3,cs.GT,2016-07-11 09:58:58+00:00,[arxiv.Result.Author('László Csató')],
312,Designing Strategyproof Election Systems with Score Voting,"We focus on the strategyproofness of voting systems where voters must choose
a number of options among several possibilities. These systems include those
that are used for Participatory Budgeting, where we organize an election to
determine the allocation of a community's budget (city, region, etc.) dedicated
to the financing of projects.
  We present a model for studying voting mechanisms and the Constrained Change
Property (CCP), which will be used to design voting mechanisms that are always
strategyproof. We also define a new notion of social choice function and use it
to design a new class of utilitarian voting mechanisms that we call score
voting. We prove that the mechanisms designed with core voting with a neutral
score function are equivalent to knapsack voting on the same instance and that
any score voting designed with a total score function is strategyproof if and
only if its score function satisfies CCP.
  These results are combined to devise an algorithm that can find the closest
total score function that makes any given score voting to be strategyproof.",2210.02496v1,cs.GT,2022-10-05 18:19:52+00:00,"[arxiv.Result.Author('Johanne Cohen'), arxiv.Result.Author('Daniel Cordeiro'), arxiv.Result.Author('Valentin Dardilhac'), arxiv.Result.Author('Victor Glaser')]",
313,Manipulation and Control Complexity of Schulze Voting,"Schulze voting is a recently introduced voting system enjoying unusual
popularity and a high degree of real-world use, with users including the
Wikimedia foundation, several branches of the Pirate Party, and MTV. It is a
Condorcet voting system that determines the winners of an election using
information about paths in a graph representation of the election. We resolve
the complexity of many electoral control cases for Schulze voting. We find that
it falls short of the best known voting systems in terms of control resistance,
demonstrating vulnerabilities of concern to some prospective users of the
system.",1206.2111v4,cs.GT,2012-06-11 06:58:50+00:00,"[arxiv.Result.Author('Curtis Menton'), arxiv.Result.Author('Preetjot Singh')]",
314,Voting Framework for Distributed Real-Time Ethernet based Dependable and Safe Systems,"In many industrial sectors such as factory automation and process control
sensor redundancy is required to ensure reliable and highly-available
operation. Measured values from N-redundant sensors are typically subjected to
some voting scheme to determine a value which is used in further processing. In
this paper we present a voting framework which allows the sensors and the
voting scheme to be configured at systemconfiguration time. The voting scheme
is designed as a Real Time Ethernet profile. We describe the structure of the
voting system and the design and verification of the framework. We argue the
applicability of this sub-system based on a successful prototype
implementation.",2005.07262v1,cs.DC,2020-04-30 22:05:13+00:00,[arxiv.Result.Author('Hans Dermot Doran')],
315,"Effectiveness, Decisiveness and Success in Weighted Voting Systems","We compare the notions ""Decisiveness"" and ""Success"" for certain weighted
voting systems and various underlying voting measures. In particular, we
compute the success rate for the Shapley-Shubik meassure and, more generally,
for Common Belief Models.",1706.08382v1,math.GM,2017-06-08 10:27:31+00:00,[arxiv.Result.Author('Werner Kirsch')],
316,Information-Theoretic Secure and Private Voting System,"In this paper, we present a private voting system that consists of N
authorized voters who may vote to one of the K candidates or vote abstain. Each
voter wants to compute the final tally while staying private and robust against
malicious voters, who try to gain information about the vote of the other
voters beyond the final result, or send incorrect information to affect the
final tally. We design an information-theoretic private voting system based on
Shamir secret sharing, which is secure and robust as long as there are up to
(N-1)/3 malicious voters.",2203.07355v1,cs.IT,2022-03-14 17:53:35+00:00,"[arxiv.Result.Author('Seyed Reza Hoseini Najarkolaei'), arxiv.Result.Author('Narges Kazempour'), arxiv.Result.Author('Hasti Rostami'), arxiv.Result.Author('Mohammad Reza Aref')]",
317,Computing voting power in easy weighted voting games,"Weighted voting games are ubiquitous mathematical models which are used in
economics, political science, neuroscience, threshold logic, reliability theory
and distributed systems. They model situations where agents with variable
voting weight vote in favour of or against a decision. A coalition of agents is
winning if and only if the sum of weights of the coalition exceeds or equals a
specified quota. The Banzhaf index is a measure of voting power of an agent in
a weighted voting game. It depends on the number of coalitions in which the
agent is the difference in the coalition winning or losing. It is well known
that computing Banzhaf indices in a weighted voting game is NP-hard. We give a
comprehensive classification of weighted voting games which can be solved in
polynomial time. Among other results, we provide a polynomial
($O(k{(\frac{n}{k})}^k)$) algorithm to compute the Banzhaf indices in weighted
voting games in which the number of weight values is bounded by $k$.",0811.2497v2,cs.GT,2008-11-15 14:55:51+00:00,"[arxiv.Result.Author('Haris Aziz'), arxiv.Result.Author('Mike Paterson')]",
318,How Many Vote Operations Are Needed to Manipulate A Voting System?,"In this paper, we propose a framework to study a general class of strategic
behavior in voting, which we call vote operations. We prove the following
theorem: if we fix the number of alternatives, generate $n$ votes i.i.d.
according to a distribution $\pi$, and let $n$ go to infinity, then for any
$\epsilon >0$, with probability at least $1-\epsilon$, the minimum number of
operations that are needed for the strategic individual to achieve her goal
falls into one of the following four categories: (1) 0, (2) $\Theta(\sqrt n)$,
(3) $\Theta(n)$, and (4) $\infty$. This theorem holds for any set of vote
operations, any individual vote distribution $\pi$, and any integer generalized
scoring rule, which includes (but is not limited to) almost all commonly
studied voting rules, e.g., approval voting, all positional scoring rules
(including Borda, plurality, and veto), plurality with runoff, Bucklin,
Copeland, maximin, STV, and ranked pairs.
  We also show that many well-studied types of strategic behavior fall under
our framework, including (but not limited to) constructive/destructive
manipulation, bribery, and control by adding/deleting votes, margin of victory,
and minimum manipulation coalition size. Therefore, our main theorem naturally
applies to these problems.",1204.1231v3,cs.AI,2012-04-05 14:00:21+00:00,[arxiv.Result.Author('Lirong Xia')],
319,"Complexity of Manipulation, Bribery, and Campaign Management in Bucklin and Fallback Voting","A central theme in computational social choice is to study the extent to
which voting systems computationally resist manipulative attacks seeking to
influence the outcome of elections, such as manipulation (i.e., strategic
voting), control, and bribery. Bucklin and fallback voting are among the voting
systems with the broadest resistance (i.e., NP-hardness) to control attacks.
However, only little is known about their behavior regarding manipulation and
bribery attacks. We comprehensively investigate the computational resistance of
Bucklin and fallback voting for many of the common manipulation and bribery
scenarios; we also complement our discussion by considering several campaign
management problems for Bucklin and fallback.",1307.7322v1,cs.GT,2013-07-28 00:24:35+00:00,"[arxiv.Result.Author('Piotr Faliszewski'), arxiv.Result.Author('Yannick Reisch'), arxiv.Result.Author('Jörg Rothe'), arxiv.Result.Author('Lena Schend')]",
320,Generating Single Peaked Votes,"We discuss how to generate singled peaked votes uniformly from the Impartial
Culture model.",1503.02766v1,cs.GT,2015-03-10 04:15:23+00:00,[arxiv.Result.Author('Toby Walsh')],
321,Normalized Range Voting Broadly Resists Control,"We study the behavior of Range Voting and Normalized Range Voting with
respect to electoral control. Electoral control encompasses attempts from an
election chair to alter the structure of an election in order to change the
outcome. We show that a voting system resists a case of control by proving that
performing that case of control is computationally infeasible. Range Voting is
a natural extension of approval voting, and Normalized Range Voting is a simple
variant which alters each vote to maximize the potential impact of each voter.
We show that Normalized Range Voting has among the largest number of control
resistances among natural voting systems.",1005.5698v3,cs.GT,2010-05-31 15:34:11+00:00,[arxiv.Result.Author('Curtis Menton')],
322,Strategic Voting Under Uncertainty About the Voting Method,"Much of the theoretical work on strategic voting makes strong assumptions
about what voters know about the voting situation. A strategizing voter is
typically assumed to know how other voters will vote and to know the rules of
the voting method. A growing body of literature explores strategic voting when
there is uncertainty about how others will vote. In this paper, we study
strategic voting when there is uncertainty about the voting method. We
introduce three notions of manipulability for a set of voting methods: sure,
safe, and expected manipulability. With the help of a computer program, we
identify voting scenarios in which uncertainty about the voting method may
reduce or even eliminate a voter's incentive to misrepresent her preferences.
Thus, it may be in the interest of an election designer who wishes to reduce
strategic voting to leave voters uncertain about which of several reasonable
voting methods will be used to determine the winners of an election.",1907.09110v1,cs.MA,2019-07-22 03:17:19+00:00,"[arxiv.Result.Author('Wesley H. Holliday'), arxiv.Result.Author('Eric Pacuit')]","EPTCS 297, 2019, pp. 252-272"
323,Designing Stable Elections: A Survey,"We survey the design of elections that are resilient to attempted
interference by third parties. For example, suppose votes have been cast in an
election between two candidates, and then each vote is randomly changed with a
small probability, independently of the other votes. It is desirable to keep
the outcome of the election the same, regardless of the changes to the votes.
It is well known that the US electoral college system is about 5 times more
likely to have a changed outcome due to vote corruption, when compared to a
majority vote. In fact, Mossel, O'Donnell and Oleszkiewicz proved in 2005 that
the majority voting method is most stable to this random vote corruption, among
voting methods where each person has a small influence on the election. We
discuss some recent progress on the analogous result for elections between more
than two candidates. In this case, plurality should be most stable to
corruption in votes. We also survey results on adversarial election
manipulation (where an adversary can select particular votes to change, perhaps
in a non-random way), and we briefly discuss ranked choice voting methods
(where a vote is a ranked list of candidates).",2006.05460v2,math.PR,2020-06-09 18:59:48+00:00,[arxiv.Result.Author('Steven Heilman')],
324,Journalistic Voting System's Effects on Election Security Threats and Gerrymandering,"The Journalistic Voting System is a proxy voting system in which journalists
are delegated the task of voting on behalf of individual voters in a
western-style democracy. We introduce the Journalistic Voting System and
discuss its potential advantages and potential problems. In particular, we
discuss its advantages to individuals in the system (voters, journalists, and
politicians) and we discuss its effects relative to several widely discussed
threats to election security, namely: cybersecurity, social media, big data,
artificial intelligence (AI), and gerrymandering. The Journalistic Voting
System is modeled on a predecessor system, called the Valence Voting System,
which is reviewed.",2110.04642v1,physics.soc-ph,2021-10-09 20:57:16+00:00,[arxiv.Result.Author('Lucius Schoenbaum')],
325,Transparent Voting Platform Based on Permissioned Blockchain,"Since 2004, different research was handling the challenges in the centralized
voting systems, e-voting protocols and recently the decentralized voting. So
electronic voting puts forward some difficulties regarding the voter anonymity,
the secure casting of the votes and to prevent the voting process from
frauding. The Decentralized property of the technology called ""blockchain""
could have the solution for many of the challenges in voting research area and
brings a new secure mechanism of safe and transparent voting. In this paper, a
broad comparison between ongoing voting systems has studied by analyzing their
structure and the drawbacks that should consider in future to improve the whole
election process from keeping the privacy of the voter, casting a vote with the
possibility to check if it was counted correctly to publishing the results. The
result of the paper will give a new approach to extend the target of the
election from small scale to large scale despite the fact of Ethereum
limitation which can cast on the blockchain just five votes per minute. The
primary challenge is to find an answer for this question: ""How to balance
between voter privacy and transparency without breaking the important rule
where the voter can proof for a specific candidate that he voted for him in a
bribe situation?"".",1802.10134v1,cs.CY,2018-02-22 14:20:35+00:00,[arxiv.Result.Author('Nazim Faour')],
326,Heuristics in Multi-Winner Approval Voting,"In many real world situations, collective decisions are made using voting.
Moreover, scenarios such as committee or board elections require voting rules
that return multiple winners. In multi-winner approval voting (AV), an agent
may vote for as many candidates as they wish. Winners are chosen by tallying up
the votes and choosing the top-$k$ candidates receiving the most votes. An
agent may manipulate the vote to achieve a better outcome by voting in a way
that does not reflect their true preferences. In complex and uncertain
situations, agents may use heuristics to strategize, instead of incurring the
additional effort required to compute the manipulation which most favors them.
In this paper, we examine voting behavior in multi-winner approval voting
scenarios with complete information. We show that people generally manipulate
their vote to obtain a better outcome, but often do not identify the optimal
manipulation. Instead, voters tend to prioritize the candidates with the
highest utilities. Using simulations, we demonstrate the effectiveness of these
heuristics in situations where agents only have access to partial information.",1905.12104v2,cs.GT,2019-05-28 21:44:07+00:00,"[arxiv.Result.Author('Jaelle Scheuerman'), arxiv.Result.Author('Jason L. Harman'), arxiv.Result.Author('Nicholas Mattei'), arxiv.Result.Author('K. Brent Venable')]",
327,Bucklin Voting is Broadly Resistant to Control,"Electoral control models ways of changing the outcome of an election via such
actions as adding/deleting/partitioning either candidates or voters. These
actions modify an election's participation structure and aim at either making a
favorite candidate win (""constructive control"") or prevent a despised candidate
from winning (""destructive control""), which yields a total of 22 standard
control scenarios. To protect elections from such control attempts,
computational complexity has been used to show that electoral control, though
not impossible, is computationally prohibitive. Among natural voting systems
with a polynomial-time winner problem, the two systems with the highest number
of proven resistances to control types (namely 19 out of 22) are
""sincere-strategy preference-based approval voting"" (SP-AV, a modification of a
system proposed by Brams and Sanver) and fallback voting. Both are hybrid
systems; e.g., fallback voting combines approval with Bucklin voting. In this
paper, we study the control complexity of Bucklin voting itself and show that
it behaves equally well in terms of control resistance for the 20 cases
investigated so far. As Bucklin voting is a special case of fallback voting,
all resistances shown for Bucklin voting in this paper strengthen the
corresponding resistance for fallback voting.",1005.4115v1,cs.CC,2010-05-22 09:12:04+00:00,"[arxiv.Result.Author('Gábor Erdélyi'), arxiv.Result.Author('Lena Piras'), arxiv.Result.Author('Jörg Rothe')]",
328,"Square root voting system, optimal threshold and π","The problem of designing an optimal weighted voting system for the two-tier
voting, applicable in the case of the Council of Ministers of the European
Union (EU), is investigated. Various arguments in favour of the square root
voting system, where the voting weights of member states are proportional to
the square root of their population are discussed and a link between this
solution and the random walk in the one-dimensional lattice is established. It
is known that the voting power of every member state is approximately equal to
its voting weight, if the threshold q for the qualified majority in the voting
body is optimally chosen. We analyze the square root voting system for a
generic 'union' of M states and derive in this case an explicit approximate
formula for the level of the optimal threshold: q \simeq 1/2+1/\sqrt{{\pi} M}.
The prefactor 1/\sqrt{{\pi}} appears here as a result of averaging over the
ensemble of unions with random populations.",1104.5213v2,physics.soc-ph,2011-04-27 18:51:41+00:00,"[arxiv.Result.Author('Karol Zyczkowski'), arxiv.Result.Author('Wojciech Slomczynski')]",
329,Penrose voting system and optimal quota,"Systems of indirect voting based on the principle of qualified majority can
be analysed using the methods of game theory. In particular, this applies to
the voting system in the Council of the European Union, which was recently a
subject of a vivid political discussion. The a priori voting power of a voter
measures his potential influence over the decisions of the voting body under a
given decision rule. We investigate a system based on the law of Penrose, in
which each representative in the voting body receives the number of votes (the
voting weight) proportional to the square root of the population he or she
represents. Here we demonstrate that for a generic distribution of the
population there exists an optimal quota for which the voting power of any
state is proportional to its weight. The optimal quota is shown to decrease
with the number of voting countries.",physics/0610271v1,physics.soc-ph,2006-10-30 12:49:18+00:00,"[arxiv.Result.Author('Wojciech Slomczynski'), arxiv.Result.Author('Karol Zyczkowski')]","Acta Physica Polonica B37, 3133-3143 (2006)"
330,Manipulation Can Be Hard in Tractable Voting Systems Even for Constant-Sized Coalitions,"Voting theory has become increasingly integrated with computational social
choice and multiagent systems. Computational complexity has been extensively
used as a shield against manipulation of voting systems, however for several
voting schemes this complexity may cause calculating the winner to be
computationally difficult. Of the many voting systems that have been studied
with regard to election manipulation, a few have been found to have an
unweighted coalitional manipulation problem that is NP-hard for a constant
number of manipulators despite having a winner problem that is in P. We survey
this interesting class of voting systems and the work that has analyzed their
complexity.",1108.4439v1,cs.GT,2011-08-22 21:02:46+00:00,"[arxiv.Result.Author('Curtis Menton'), arxiv.Result.Author('Preetjot Singh')]",
331,A mathematical evaluation of vote transfer systems,"The paper builds a general model of vote transfer systems on the basis of the
current Hungarian electoral rules. It combines single-seat districts and list
mandates with three possible compensation rule for 'wasted' votes in
constituencies: no compensation (direct vote transfer, DVT), compensation for
votes cast for losing party candidates (positive vote transfer, PVT) and
compensation for all votes that are not necessary to win the district (negative
vote transfer, NVT).
  The model is studied in the case of two parties. When the number of votes for
the majority party follows a uniform distribution in each district, DVT results
in the greatest expected seat share, however, application of PVT, and,
especially, NVT increases the probability of winning the election. The
trade-off between vote transfer formulas and the number of list mandates
reveals that the majority party should use an appropriately calibrated NVT
system if it focuses on these two variables.",1607.02879v3,cs.GT,2016-07-11 09:58:58+00:00,[arxiv.Result.Author('László Csató')],
332,Designing Strategyproof Election Systems with Score Voting,"We focus on the strategyproofness of voting systems where voters must choose
a number of options among several possibilities. These systems include those
that are used for Participatory Budgeting, where we organize an election to
determine the allocation of a community's budget (city, region, etc.) dedicated
to the financing of projects.
  We present a model for studying voting mechanisms and the Constrained Change
Property (CCP), which will be used to design voting mechanisms that are always
strategyproof. We also define a new notion of social choice function and use it
to design a new class of utilitarian voting mechanisms that we call score
voting. We prove that the mechanisms designed with core voting with a neutral
score function are equivalent to knapsack voting on the same instance and that
any score voting designed with a total score function is strategyproof if and
only if its score function satisfies CCP.
  These results are combined to devise an algorithm that can find the closest
total score function that makes any given score voting to be strategyproof.",2210.02496v1,cs.GT,2022-10-05 18:19:52+00:00,"[arxiv.Result.Author('Johanne Cohen'), arxiv.Result.Author('Daniel Cordeiro'), arxiv.Result.Author('Valentin Dardilhac'), arxiv.Result.Author('Victor Glaser')]",
333,Manipulation and Control Complexity of Schulze Voting,"Schulze voting is a recently introduced voting system enjoying unusual
popularity and a high degree of real-world use, with users including the
Wikimedia foundation, several branches of the Pirate Party, and MTV. It is a
Condorcet voting system that determines the winners of an election using
information about paths in a graph representation of the election. We resolve
the complexity of many electoral control cases for Schulze voting. We find that
it falls short of the best known voting systems in terms of control resistance,
demonstrating vulnerabilities of concern to some prospective users of the
system.",1206.2111v4,cs.GT,2012-06-11 06:58:50+00:00,"[arxiv.Result.Author('Curtis Menton'), arxiv.Result.Author('Preetjot Singh')]",
334,Voting Framework for Distributed Real-Time Ethernet based Dependable and Safe Systems,"In many industrial sectors such as factory automation and process control
sensor redundancy is required to ensure reliable and highly-available
operation. Measured values from N-redundant sensors are typically subjected to
some voting scheme to determine a value which is used in further processing. In
this paper we present a voting framework which allows the sensors and the
voting scheme to be configured at systemconfiguration time. The voting scheme
is designed as a Real Time Ethernet profile. We describe the structure of the
voting system and the design and verification of the framework. We argue the
applicability of this sub-system based on a successful prototype
implementation.",2005.07262v1,cs.DC,2020-04-30 22:05:13+00:00,[arxiv.Result.Author('Hans Dermot Doran')],
335,"Effectiveness, Decisiveness and Success in Weighted Voting Systems","We compare the notions ""Decisiveness"" and ""Success"" for certain weighted
voting systems and various underlying voting measures. In particular, we
compute the success rate for the Shapley-Shubik meassure and, more generally,
for Common Belief Models.",1706.08382v1,math.GM,2017-06-08 10:27:31+00:00,[arxiv.Result.Author('Werner Kirsch')],
336,Information-Theoretic Secure and Private Voting System,"In this paper, we present a private voting system that consists of N
authorized voters who may vote to one of the K candidates or vote abstain. Each
voter wants to compute the final tally while staying private and robust against
malicious voters, who try to gain information about the vote of the other
voters beyond the final result, or send incorrect information to affect the
final tally. We design an information-theoretic private voting system based on
Shamir secret sharing, which is secure and robust as long as there are up to
(N-1)/3 malicious voters.",2203.07355v1,cs.IT,2022-03-14 17:53:35+00:00,"[arxiv.Result.Author('Seyed Reza Hoseini Najarkolaei'), arxiv.Result.Author('Narges Kazempour'), arxiv.Result.Author('Hasti Rostami'), arxiv.Result.Author('Mohammad Reza Aref')]",
337,Computing voting power in easy weighted voting games,"Weighted voting games are ubiquitous mathematical models which are used in
economics, political science, neuroscience, threshold logic, reliability theory
and distributed systems. They model situations where agents with variable
voting weight vote in favour of or against a decision. A coalition of agents is
winning if and only if the sum of weights of the coalition exceeds or equals a
specified quota. The Banzhaf index is a measure of voting power of an agent in
a weighted voting game. It depends on the number of coalitions in which the
agent is the difference in the coalition winning or losing. It is well known
that computing Banzhaf indices in a weighted voting game is NP-hard. We give a
comprehensive classification of weighted voting games which can be solved in
polynomial time. Among other results, we provide a polynomial
($O(k{(\frac{n}{k})}^k)$) algorithm to compute the Banzhaf indices in weighted
voting games in which the number of weight values is bounded by $k$.",0811.2497v2,cs.GT,2008-11-15 14:55:51+00:00,"[arxiv.Result.Author('Haris Aziz'), arxiv.Result.Author('Mike Paterson')]",
338,How Many Vote Operations Are Needed to Manipulate A Voting System?,"In this paper, we propose a framework to study a general class of strategic
behavior in voting, which we call vote operations. We prove the following
theorem: if we fix the number of alternatives, generate $n$ votes i.i.d.
according to a distribution $\pi$, and let $n$ go to infinity, then for any
$\epsilon >0$, with probability at least $1-\epsilon$, the minimum number of
operations that are needed for the strategic individual to achieve her goal
falls into one of the following four categories: (1) 0, (2) $\Theta(\sqrt n)$,
(3) $\Theta(n)$, and (4) $\infty$. This theorem holds for any set of vote
operations, any individual vote distribution $\pi$, and any integer generalized
scoring rule, which includes (but is not limited to) almost all commonly
studied voting rules, e.g., approval voting, all positional scoring rules
(including Borda, plurality, and veto), plurality with runoff, Bucklin,
Copeland, maximin, STV, and ranked pairs.
  We also show that many well-studied types of strategic behavior fall under
our framework, including (but not limited to) constructive/destructive
manipulation, bribery, and control by adding/deleting votes, margin of victory,
and minimum manipulation coalition size. Therefore, our main theorem naturally
applies to these problems.",1204.1231v3,cs.AI,2012-04-05 14:00:21+00:00,[arxiv.Result.Author('Lirong Xia')],
339,"Complexity of Manipulation, Bribery, and Campaign Management in Bucklin and Fallback Voting","A central theme in computational social choice is to study the extent to
which voting systems computationally resist manipulative attacks seeking to
influence the outcome of elections, such as manipulation (i.e., strategic
voting), control, and bribery. Bucklin and fallback voting are among the voting
systems with the broadest resistance (i.e., NP-hardness) to control attacks.
However, only little is known about their behavior regarding manipulation and
bribery attacks. We comprehensively investigate the computational resistance of
Bucklin and fallback voting for many of the common manipulation and bribery
scenarios; we also complement our discussion by considering several campaign
management problems for Bucklin and fallback.",1307.7322v1,cs.GT,2013-07-28 00:24:35+00:00,"[arxiv.Result.Author('Piotr Faliszewski'), arxiv.Result.Author('Yannick Reisch'), arxiv.Result.Author('Jörg Rothe'), arxiv.Result.Author('Lena Schend')]",
340,Predicting Missing Information of Key Aspects in Vulnerability Reports,"Software vulnerabilities have been continually disclosed and documented. An
important practice in documenting vulnerabilities is to describe the key
vulnerability aspects, such as vulnerability type, root cause, affected
product, impact, attacker type and attack vector, for the effective search and
management of fast-growing vulnerabilities. We investigate 120,103
vulnerability reports in the Common Vulnerabilities and Exposures (CVE) over
the past 20 years. We find that 56%, 85%, 38% and 28% of CVEs miss
vulnerability type, root causes, attack vector and attacker type respectively.
To help to complete the missing information of these vulnerability aspects, we
propose a neural-network based approach for predicting the missing information
of a key aspect of a vulnerability based on the known aspects of the
vulnerability. We explore the design space of the neural network models and
empirically identify the most effective model design. Using a large-scale
vulnerability datas\-et from CVE, we show that we can effectively train a
neural-network based classifier with less than 20% of historical CVEs. Our
model achieves the prediction accuracy 94%, 79%, 89%and 70% for vulnerability
type, root cause, attacker type and attack vector, respectively. Our ablation
study reveals the prominent correlations among vulnerability aspects and
further confirms the practicality of our approach.",2008.02456v1,cs.SE,2020-08-06 04:53:33+00:00,"[arxiv.Result.Author('Hao Guo'), arxiv.Result.Author('Zhenchang Xing'), arxiv.Result.Author('Xiaohong Li')]",
341,LEOPARD: Identifying Vulnerable Code for Vulnerability Assessment through Program Metrics,"Identifying potentially vulnerable locations in a code base is critical as a
pre-step for effective vulnerability assessment; i.e., it can greatly help
security experts put their time and effort to where it is needed most.
Metric-based and pattern-based methods have been presented for identifying
vulnerable code. The former relies on machine learning and cannot work well due
to the severe imbalance between non-vulnerable and vulnerable code or lack of
features to characterize vulnerabilities. The latter needs the prior knowledge
of known vulnerabilities and can only identify similar but not new types of
vulnerabilities.
  In this paper, we propose and implement a generic, lightweight and extensible
framework, LEOPARD, to identify potentially vulnerable functions through
program metrics. LEOPARD requires no prior knowledge about known
vulnerabilities. It has two steps by combining two sets of systematically
derived metrics. First, it uses complexity metrics to group the functions in a
target application into a set of bins. Then, it uses vulnerability metrics to
rank the functions in each bin and identifies the top ones as potentially
vulnerable. Our experimental results on 11 real-world projects have
demonstrated that, LEOPARD can cover 74.0% of vulnerable functions by
identifying 20% of functions as vulnerable and outperform machine
learning-based and static analysis-based techniques. We further propose three
applications of LEOPARD for manual code review and fuzzing, through which we
discovered 22 new bugs in real applications like PHP, radare2 and FFmpeg, and
eight of them are new vulnerabilities.",1901.11479v2,cs.SE,2019-01-31 17:09:15+00:00,"[arxiv.Result.Author('Xiaoning Du'), arxiv.Result.Author('Bihuan Chen'), arxiv.Result.Author('Yuekang Li'), arxiv.Result.Author('Jianmin Guo'), arxiv.Result.Author('Yaqin Zhou'), arxiv.Result.Author('Yang Liu'), arxiv.Result.Author('Yu Jiang')]",
342,Vulnerability Coverage as an Adequacy Testing Criterion,"Mainstream software applications and tools are the configurable platforms
with an enormous number of parameters along with their values. Certain settings
and possible interactions between these parameters may harden (or soften) the
security and robustness of these applications against some known
vulnerabilities. However, the large number of vulnerabilities reported and
associated with these tools make the exhaustive testing of these tools
infeasible against these vulnerabilities infeasible. As an instance of general
software testing problem, the research question to address is whether the
system under test is robust and secure against these vulnerabilities. This
paper introduces the idea of ``vulnerability coverage,'' a concept to
adequately test a given application for a certain classes of vulnerabilities,
as reported by the National Vulnerability Database (NVD). The deriving idea is
to utilize the Common Vulnerability Scoring System (CVSS) as a means to measure
the fitness of test inputs generated by evolutionary algorithms and then
through pattern matching identify vulnerabilities that match the generated
vulnerability vectors and then test the system under test for those identified
vulnerabilities. We report the performance of two evolutionary algorithms
(i.e., Genetic Algorithms and Particle Swarm Optimization) in generating the
vulnerability pattern vectors.",2006.08606v1,cs.SE,2020-06-14 15:53:10+00:00,"[arxiv.Result.Author('Shuvalaxmi Dass'), arxiv.Result.Author('Akbar Siami Namin')]",
343,Vulnerability Detection is Just the Beginning,"Vulnerability detection plays a key role in secure software development.
There are many different vulnerability detection tools and techniques to choose
from, and insufficient information on which vulnerability detection techniques
to use and when. The goal of this research is to assist managers and other
decision-makers on software projects in making informed choices about the use
of different software vulnerability detection techniques through empirical
analysis of the efficiency and effectiveness of each technique. We will examine
the relationships between the vulnerability detection technique used to find a
vulnerability, the type of vulnerability found, the exploitability of the
vulnerability, and the effort needed to fix a vulnerability on two projects
where we ensure all vulnerabilities found have been fixed. We will then examine
how these relationships are seen in Open Source Software more broadly where
practitioners may use different vulnerability detection techniques, or may not
fix all vulnerabilities found due to resource constraints.",2103.05160v1,cs.SE,2021-03-09 01:03:03+00:00,[arxiv.Result.Author('Sarah Elder')],
344,On the Threat of npm Vulnerable Dependencies in Node.js Applications,"Software vulnerabilities have a large negative impact on the software systems
that we depend on daily. Reports on software vulnerabilities always paint a
grim picture, with some reports showing that 83% of organizations depend on
vulnerable software. However, our experience leads us to believe that, in the
grand scheme of things, these software vulnerabilities may have less impact
than what is reported. Therefore, we perform a study to better understand the
threat of npm vulnerable packages used in Node.js applications. We define three
threat levels for vulnerabilities in packages, based on their lifecycle, where
a package vulnerability is assigned a low threat level if it was hidden or
still unknown at the time it was used in the dependent application (t), medium
threat level if the vulnerability was reported but not yet published at t, and
high if it was publicly announced at t. Then, we perform an empirical study
involving 6,673 real-world, active, and mature open source Node.js
applications. Our findings show that although 67.93% of the examined
applications depend on at least one vulnerable package, 94.91% of the
vulnerable packages in those affected applications are classified as having low
threat. Moreover, we find that in the case of vulnerable packages classified as
having high threat, it is the application's lack of updating that makes them
vulnerable, i.e., it is not the existence of the vulnerability that is the real
problem. Furthermore, we verify our findings at different stages of the
application's lifetime and find that our findings still hold. Our study argues
that when it comes to software vulnerabilities, things may not be as bad as
they seem and that considering vulnerability threat is key.",2009.09019v1,cs.SE,2020-09-18 18:45:21+00:00,"[arxiv.Result.Author('Mahmoud Alfadel'), arxiv.Result.Author('Diego Elias Costa'), arxiv.Result.Author('Mouafak Mokhallalati'), arxiv.Result.Author('Emad Shihab'), arxiv.Result.Author('Bram Adams')]",
345,Vulnerability Detection with Fine-grained Interpretations,"Despite the successes of machine learning (ML) and deep learning (DL) based
vulnerability detectors (VD), they are limited to providing only the decision
on whether a given code is vulnerable or not, without details on what part of
the code is relevant to the detected vulnerability. We present IVDetect an
interpretable vulnerability detector with the philosophy of using Artificial
Intelligence (AI) to detect vulnerabilities, while using Intelligence Assistant
(IA) via providing VD interpretations in terms of vulnerable statements.
  For vulnerability detection, we separately consider the vulnerable statements
and their surrounding contexts via data and control dependencies. This allows
our model better discriminate vulnerable statements than using the mixture of
vulnerable code and~contextual code as in existing approaches. In addition to
the coarse-grained vulnerability detection result, we leverage interpretable AI
to provide users with fine-grained interpretations that include the sub-graph
in the Program Dependency Graph (PDG) with the crucial statements that are
relevant to the detected vulnerability. Our empirical evaluation on
vulnerability databases shows that IVDetect outperforms the existing DL-based
approaches by 43%--84% and 105%--255% in top-10 nDCG and MAP ranking scores.
IVDetect correctly points out the vulnerable statements relevant to the
vulnerability via its interpretation~in 67% of the cases with a top-5 ranked
list. It improves over baseline interpretation models by 12.3%--400% and
9%--400% in accuracy.",2106.10478v1,cs.CR,2021-06-19 11:53:49+00:00,"[arxiv.Result.Author('Yi Li'), arxiv.Result.Author('Shaohua Wang'), arxiv.Result.Author('Tien N. Nguyen')]",
346,An Exploratory Study into Vulnerability Chaining Blindness Terminology and Viability,"To tie together the concepts of linkage blindness and the inability to link
vulnerabilities together in a Vulnerability Management Program (VMP), the
researcher postulated new terminology. The terminology of vulnerability
chaining blindness is proposed to understand the underlying issues behind
vulnerability management and vulnerabilities that can be used in combination.
The general problem is that IT and cybersecurity professionals have a difficult
time identifying chained vulnerabilities due to the complexity of vulnerability
prioritization and remediation (Abomhara & K{\o}ien, 2015; Felmetsger et al.,
2010). The specific problem is the inability to link and view multiple
vulnerabilities in combination based on limited expertise and awareness of
vulnerability chaining (Tang et al., 2017). The population of this study was
limited to one focus group, within the IT and Security fields, within the
United States. The sample size consisted of one focus group comprised of 8-10
IT and cybersecurity professionals. The research questions focused on if
participants were aware of linkage blindness or vulnerability chaining, as well
as if vulnerability chaining blindness would be applicable to describe the
phenomenon. Several themes emerged through top-level, eclectic, and
second-level coding data analysis. These themes included complexity in
cybersecurity programs, new concepts in vulnerability management, as well as
fear of the unknown and where security meets technology.
  Keywords: linkage blindness, vulnerability chaining, vulnerability chaining
blindness, vulnerability management",2203.10403v1,cs.CR,2022-03-19 22:40:32+00:00,[arxiv.Result.Author('Nikki Robinson')],
347,Examining the Relationship of Code and Architectural Smells with Software Vulnerabilities,"Context: Security is vital to software developed for commercial or personal
use. Although more organizations are realizing the importance of applying
secure coding practices, in many of them, security concerns are not known or
addressed until a security failure occurs. The root cause of security failures
is vulnerable code. While metrics have been used to predict software
vulnerabilities, we explore the relationship between code and architectural
smells with security weaknesses. As smells are surface indicators of a deeper
problem in software, determining the relationship between smells and software
vulnerabilities can play a significant role in vulnerability prediction models.
Objective: This study explores the relationship between smells and software
vulnerabilities to identify the smells. Method: We extracted the class, method,
file, and package level smells for three systems: Apache Tomcat, Apache CXF,
and Android. We then compared their occurrences in the vulnerable classes which
were reported to contain vulnerable code and in the neutral classes
(non-vulnerable classes where no vulnerability had yet been reported). Results:
We found that a vulnerable class is more likely to have certain smells compared
to a non-vulnerable class. God Class, Complex Class, Large Class, Data Class,
Feature Envy, Brain Class have a statistically significant relationship with
software vulnerabilities. We found no significant relationship between
architectural smells and software vulnerabilities. Conclusion: We can conclude
that for all the systems examined, there is a statistically significant
correlation between software vulnerabilities and some smells.",2010.15978v1,cs.SE,2020-10-29 22:50:31+00:00,"[arxiv.Result.Author('Kazi Zakia Sultana'), arxiv.Result.Author('Zadia Codabux'), arxiv.Result.Author('Byron Williams')]",
348,A Historical and Statistical Studyof the Software Vulnerability Landscape,"Understanding the landscape of software vulnerabilities is key for developing
effective security solutions. Fortunately, the evaluation of vulnerability
databases that use a framework for communicating vulnerability attributes and
their severity scores, such as the Common Vulnerability Scoring System (CVSS),
can help shed light on the nature of publicly published vulnerabilities. In
this paper, we characterize the software vulnerability landscape by performing
a historical and statistical analysis of CVSS vulnerability metrics over the
period of 2005 to 2019 through using data from the National Vulnerability
Database. We conduct three studies analyzing the following: the distribution of
CVSS scores (both empirical and theoretical), the distribution of CVSS metric
values and how vulnerability characteristics change over time, and the relative
rankings of the most frequent metric value over time. Our resulting analysis
shows that the vulnerability threat landscape has been dominated by only a few
vulnerability types and has changed little during the time period of the study.
The overwhelming majority of vulnerabilities are exploitable over the network.
The complexity to successfully exploit these vulnerabilities is dominantly low;
very little authentication to the target victim is necessary for a successful
attack. And most of the flaws require very limited interaction with users.
However on the positive side, the damage of these vulnerabilities is mostly
confined within the security scope of the impacted components. A discussion of
lessons that could be learned from this analysis is presented.",2102.01722v1,cs.CR,2021-02-02 19:35:50+00:00,"[arxiv.Result.Author('Assane Gueye'), arxiv.Result.Author('Peter Mell')]",
349,CVEfixes: Automated Collection of Vulnerabilities and Their Fixes from Open-Source Software,"Data-driven research on the automated discovery and repair of security
vulnerabilities in source code requires comprehensive datasets of real-life
vulnerable code and their fixes. To assist in such research, we propose a
method to automatically collect and curate a comprehensive vulnerability
dataset from Common Vulnerabilities and Exposures (CVE) records in the public
National Vulnerability Database (NVD). We implement our approach in a fully
automated dataset collection tool and share an initial release of the resulting
vulnerability dataset named CVEfixes.
  The CVEfixes collection tool automatically fetches all available CVE records
from the NVD, gathers the vulnerable code and corresponding fixes from
associated open-source repositories, and organizes the collected information in
a relational database. Moreover, the dataset is enriched with meta-data such as
programming language, and detailed code and security metrics at five levels of
abstraction. The collection can easily be repeated to keep up-to-date with
newly discovered or patched vulnerabilities. The initial release of CVEfixes
spans all published CVEs up to 9 June 2021, covering 5365 CVE records for 1754
open-source projects that were addressed in a total of 5495 vulnerability
fixing commits.
  CVEfixes supports various types of data-driven software security research,
such as vulnerability prediction, vulnerability classification, vulnerability
severity prediction, analysis of vulnerability-related code changes, and
automated vulnerability repair.",2107.08760v1,cs.SE,2021-07-19 11:34:09+00:00,"[arxiv.Result.Author('Guru Prasad Bhandari'), arxiv.Result.Author('Amara Naseer'), arxiv.Result.Author('Leon Moonen')]",
350,Web Vulnerability Scanners: A Case Study,"Cloud security is one of the biggest concerns for many companies. The growth
in the number and size of websites increases the need for better securing those
websites. Manual testing and detection of web vulnerabilities can be very time
consuming. Automated Web Vulnerability Scanners (WVS) help with the detection
of vulnerabilities in web applications. Acunetix is one of the widely used
vulnerability scanners. Acunetix is also easy to implement and to use. The scan
results not only provide the details of the vulnerabilities, but also give
information about fixing the vulnerabilities. AcuSensor and AcuMonitor
(technologies used by Acunetix) help generate more accurate potential
vulnerability results. One of the purposes of this paper is to orient current
students of computer security with using vulnerability scanners. Secondly, this
paper provides a literature review related to the topic of security
vulnerability scanners. Finally, web vulnerabilities are addressed from the
mobile device and browser perspectives.",1706.08017v1,cs.CR,2017-06-25 01:04:29+00:00,"[arxiv.Result.Author('Emre Erturk'), arxiv.Result.Author('Angel Rajan')]",
351,Vulnerability Coverage for Secure Configuration,"We present a novel idea on adequacy testing called ``{vulnerability
coverage}.'' The introduced coverage measure examines the underlying software
for the presence of certain classes of vulnerabilities often found in the
National Vulnerability Database (NVD) website. The thoroughness of the test
input generation procedure is performed through the adaptation of evolutionary
algorithms namely Genetic Algorithms (GA) and Particle Swarm Optimization
(PSO). The methodology utilizes the Common Vulnerability Scoring System (CVSS),
a free and open industry standard for assessing the severity of computer system
security vulnerabilities, as a fitness measure for test inputs generation. The
outcomes of these evolutionary algorithms are then evaluated in order to
identify the vulnerabilities that match a class of vulnerability patterns for
testing purposes.",2006.08604v1,cs.CR,2020-06-14 14:47:57+00:00,"[arxiv.Result.Author('Shuvalaxmi Dass'), arxiv.Result.Author('Akbar Siami Namin')]",
352,Learning from What We Know: How to Perform Vulnerability Prediction using Noisy Historical Data,"Vulnerability prediction refers to the problem of identifying system
components that are most likely to be vulnerable. Typically, this problem is
tackled by training binary classifiers on historical data. Unfortunately,
recent research has shown that such approaches underperform due to the
following two reasons: a) the imbalanced nature of the problem, and b) the
inherently noisy historical data, i.e., most vulnerabilities are discovered
much later than they are introduced. This misleads classifiers as they learn to
recognize actual vulnerable components as non-vulnerable. To tackle these
issues, we propose TROVON, a technique that learns from known vulnerable
components rather than from vulnerable and non-vulnerable components, as
typically performed. We perform this by contrasting the known vulnerable, and
their respective fixed components. This way, TROVON manages to learn from the
things we know, i.e., vulnerabilities, hence reducing the effects of noisy and
unbalanced data. We evaluate TROVON by comparing it with existing techniques on
three security-critical open source systems, i.e., Linux Kernel, OpenSSL, and
Wireshark, with historical vulnerabilities that have been reported in the
National Vulnerability Database (NVD). Our evaluation demonstrates that the
prediction capability of TROVON significantly outperforms existing
vulnerability prediction techniques such as Software Metrics, Imports, Function
Calls, Text Mining, Devign, LSTM, and LSTM-RF with an improvement of 40.84% in
Matthews Correlation Coefficient (MCC) score under Clean Training Data
Settings, and an improvement of 35.52% under Realistic Training Data Settings.",2012.11701v3,cs.CR,2020-12-21 21:59:12+00:00,"[arxiv.Result.Author('Aayush Garg'), arxiv.Result.Author('Renzo Degiovanni'), arxiv.Result.Author('Matthieu Jimenez'), arxiv.Result.Author('Maxime Cordy'), arxiv.Result.Author('Mike Papadakis'), arxiv.Result.Author('Yves Le Traon')]",
353,Learning from what we know: How to perform vulnerability prediction using noisy historical data,"Vulnerability prediction refers to the problem of identifying system
components that are most likely to be vulnerable. Typically, this problem is
tackled by training binary classifiers on historical data. Unfortunately,
recent research has shown that such approaches underperform due to the
following two reasons: a) the imbalanced nature of the problem, and b) the
inherently noisy historical data, i.e., most vulnerabilities are discovered
much later than they are introduced. This misleads classifiers as they learn to
recognize actual vulnerable components as non-vulnerable. To tackle these
issues, we propose TROVON, a technique that learns from known vulnerable
components rather than from vulnerable and non-vulnerable components, as
typically performed. We perform this by contrasting the known vulnerable, and
their respective fixed components. This way, TROVON manages to learn from the
things we know, i.e., vulnerabilities, hence reducing the effects of noisy and
unbalanced data. We evaluate TROVON by comparing it with existing techniques on
three security-critical open source systems, i.e., Linux Kernel, OpenSSL, and
Wireshark, with historical vulnerabilities that have been reported in the
National Vulnerability Database (NVD). Our evaluation demonstrates that the
prediction capability of TROVON significantly outperforms existing
vulnerability prediction techniques such as Software Metrics, Imports, Function
Calls, Text Mining, Devign, LSTM, and LSTM-RF with an improvement of 40.84% in
Matthews Correlation Coefficient (MCC) score under Clean Training Data
Settings, and an improvement of 35.52% under Realistic Training Data Settings.",2207.11018v2,cs.SE,2022-07-22 11:45:20+00:00,"[arxiv.Result.Author('Aayush Garg'), arxiv.Result.Author('Renzo Degiovanni'), arxiv.Result.Author('Matthieu Jimenez'), arxiv.Result.Author('Maxime Cordy'), arxiv.Result.Author('Mike Papadakis'), arxiv.Result.Author('Yves LeTraon')]",
354,Evaluation of Static Analysis on Web Applications,"Web services are becoming business-critical components, often deployed with
critical software bugs that can be maliciously explored. Web vulnerability
scanners allow the detection of security vulnerabilities in web services by
stressing the service from the point of view of an attacker. However, research
and practice show that different scanners perform differently in vulnerability
detection. This paper presents a qualitative evaluation of security
vulnerabilities found in web applications. Some well-known vulnerability
scanners have been used to identify security flaws in web service
implementations. Many vulnerabilities have been observed, which confirms that
many services are deployed without proper security testing. Additionally,
having reviewed and considered several articles, the differences in the
vulnerabilities detected and the high number of false positives observed
highlight the limitations of web vulnerability scanners in detecting security
vulnerabilities in web services. Furthermore, this work will discuss the static
analysis approach for discovering security vulnerabilities in web applications
and complimenting it with proven research findings or solutions. These
vulnerabilities include broken access control, cross-site scripting, SQL
injections, buffer overflow, unrestricted file upload, broken authentications,
etc. Web applications are becoming mission-essential components for businesses,
potentially risking having several software vulnerabilities that hackers can
exploit maliciously. A few Vulnerability scanners have been used to detect
security weaknesses in web service applications, and many vulnerabilities have
been discovered, thus confirming that many online apps are launched without
sufficient security testing.",2212.12308v1,cs.CR,2022-12-13 17:03:27+00:00,"[arxiv.Result.Author('Osejobe Ehichoya'), arxiv.Result.Author('Chinwuba Christian Nnaemeka')]",
355,The (Un)Reliability of NVD Vulnerable Versions Data: an Empirical Experiment on Google Chrome Vulnerabilities,"NVD is one of the most popular databases used by researchers to conduct
empirical research on data sets of vulnerabilities. Our recent analysis on
Chrome vulnerability data reported by NVD has revealed an abnormally phenomenon
in the data where almost vulnerabilities were originated from the first
versions. This inspires our experiment to validate the reliability of the NVD
vulnerable version data. In this experiment, we verify for each version of
Chrome that NVD claims vulnerable is actually vulnerable. The experiment
revealed several errors in the vulnerability data of Chrome. Furthermore, we
have also analyzed how these errors might impact the conclusions of an
empirical study on foundational vulnerability. Our results show that different
conclusions could be obtained due to the data errors.",1302.4133v1,cs.CR,2013-02-17 23:19:52+00:00,"[arxiv.Result.Author('Viet Hung Nguyen'), arxiv.Result.Author('Fabio Massacci')]",
356,Identifying Relevant Information Cues for Vulnerability Assessment Using CVSS,"The assessment of new vulnerabilities is an activity that accounts for
information from several data sources and produces a `severity' score for the
vulnerability. The Common Vulnerability Scoring System (\CVSS) is the reference
standard for this assessment. Yet, no guidance currently exists on \emph{which
information} aids a correct assessment and should therefore be considered.
  In this paper we address this problem by evaluating which information cues
increase (or decrease) assessment accuracy.
  We devise a block design experiment with 67 software engineering students
with varying vulnerability information and measure scoring accuracy under
different information sets.
  We find that baseline vulnerability descriptions provided by standard
vulnerability sources provide only part of the information needed to achieve an
accurate vulnerability assessment. Further, we find that additional information
on \texttt{assets}, \texttt{attacks}, and \texttt{vulnerability type}
contributes in increasing the accuracy of the assessment; conversely,
information on \texttt{known threats} misleads the assessor and decreases
assessment accuracy and should be avoided when assessing vulnerabilities. These
results go in the direction of formalizing the vulnerability communication to,
for example, fully automate security assessments.",1803.07648v1,cs.CR,2018-03-20 20:51:07+00:00,"[arxiv.Result.Author('Luca Allodi'), arxiv.Result.Author('Sebastian Banescu'), arxiv.Result.Author('Henning Femmer'), arxiv.Result.Author('Kristian Beckers')]","Proceedings of the 2018 ACM Conference on Data and Application
  Security and Privacy"
357,$μ$VulDeePecker: A Deep Learning-Based System for Multiclass Vulnerability Detection,"Fine-grained software vulnerability detection is an important and challenging
problem. Ideally, a detection system (or detector) not only should be able to
detect whether or not a program contains vulnerabilities, but also should be
able to pinpoint the type of a vulnerability in question. Existing
vulnerability detection methods based on deep learning can detect the presence
of vulnerabilities (i.e., addressing the binary classification or detection
problem), but cannot pinpoint types of vulnerabilities (i.e., incapable of
addressing multiclass classification). In this paper, we propose the first deep
learning-based system for multiclass vulnerability detection, dubbed
$\mu$VulDeePecker. The key insight underlying $\mu$VulDeePecker is the concept
of code attention, which can capture information that can help pinpoint types
of vulnerabilities, even when the samples are small. For this purpose, we
create a dataset from scratch and use it to evaluate the effectiveness of
$\mu$VulDeePecker. Experimental results show that $\mu$VulDeePecker is
effective for multiclass vulnerability detection and that accommodating
control-dependence (other than data-dependence) can lead to higher detection
capabilities.",2001.02334v1,cs.CR,2020-01-08 01:47:22+00:00,"[arxiv.Result.Author('Deqing Zou'), arxiv.Result.Author('Sujuan Wang'), arxiv.Result.Author('Shouhuai Xu'), arxiv.Result.Author('Zhen Li'), arxiv.Result.Author('Hai Jin')]",
358,Attack Potential in Impact and Complexity,"Vulnerability exploitation is reportedly one of the main attack vectors
against computer systems. Yet, most vulnerabilities remain unexploited by
attackers. It is therefore of central importance to identify vulnerabilities
that carry a high `potential for attack'. In this paper we rely on Symantec
data on real attacks detected in the wild to identify a trade-off in the Impact
and Complexity of a vulnerability, in terms of attacks that it generates;
exploiting this effect, we devise a readily computable estimator of the
vulnerability's Attack Potential that reliably estimates the expected volume of
attacks against the vulnerability. We evaluate our estimator performance
against standard patching policies by measuring foiled attacks and demanded
workload expressed as the number of vulnerabilities entailed to patch. We show
that our estimator significantly improves over standard patching policies by
ruling out low-risk vulnerabilities, while maintaining invariant levels of
coverage against attacks in the wild. Our estimator can be used as a first aid
for vulnerability prioritisation to focus assessment efforts on high-potential
vulnerabilities.",1801.04703v1,cs.CR,2018-01-15 09:03:27+00:00,"[arxiv.Result.Author('Luca Allodi'), arxiv.Result.Author('Fabio Massacci')]","Proceedings of the 12th International Conference on Availability,
  Reliability and Security (ARES 2017)"
359,VulDeePecker: A Deep Learning-Based System for Vulnerability Detection,"The automatic detection of software vulnerabilities is an important research
problem. However, existing solutions to this problem rely on human experts to
define features and often miss many vulnerabilities (i.e., incurring high false
negative rate). In this paper, we initiate the study of using deep
learning-based vulnerability detection to relieve human experts from the
tedious and subjective task of manually defining features. Since deep learning
is motivated to deal with problems that are very different from the problem of
vulnerability detection, we need some guiding principles for applying deep
learning to vulnerability detection. In particular, we need to find
representations of software programs that are suitable for deep learning. For
this purpose, we propose using code gadgets to represent programs and then
transform them into vectors, where a code gadget is a number of (not
necessarily consecutive) lines of code that are semantically related to each
other. This leads to the design and implementation of a deep learning-based
vulnerability detection system, called Vulnerability Deep Pecker
(VulDeePecker). In order to evaluate VulDeePecker, we present the first
vulnerability dataset for deep learning approaches. Experimental results show
that VulDeePecker can achieve much fewer false negatives (with reasonable false
positives) than other approaches. We further apply VulDeePecker to 3 software
products (namely Xen, Seamonkey, and Libav) and detect 4 vulnerabilities, which
are not reported in the National Vulnerability Database but were ""silently""
patched by the vendors when releasing later versions of these products; in
contrast, these vulnerabilities are almost entirely missed by the other
vulnerability detection systems we experimented with.",1801.01681v1,cs.CR,2018-01-05 09:37:18+00:00,"[arxiv.Result.Author('Zhen Li'), arxiv.Result.Author('Deqing Zou'), arxiv.Result.Author('Shouhuai Xu'), arxiv.Result.Author('Xinyu Ou'), arxiv.Result.Author('Hai Jin'), arxiv.Result.Author('Sujuan Wang'), arxiv.Result.Author('Zhijun Deng'), arxiv.Result.Author('Yuyi Zhong')]",
360,vVote: a Verifiable Voting System,"The Pret a Voter cryptographic voting system was designed to be flexible and
to offer voters a familiar and easy voting experience. In this paper we present
a case study of our efforts to adapt Pret a Voter to the idiosyncrasies of
elections in the Australian state of Victoria. This technical report includes
general background, user experience and details of the cryptographic protocols
and human processes. We explain the problems, present solutions, then analyse
their security properties and explain how they tie in to other design
decisions. We hope this will be an interesting case study on the application of
end-to-end verifiable voting protocols to real elections.
  A preliminary version of this paper appeared as the 10th February 2014
version of ""Draft Technical Report for VEC vVote System"".
  The team involved in developing the vVote design described in this report
were: Craig Burton, Chris Culnane, James Heather, Rui Joaquim, Peter Y. A.
Ryan, Steve Schneider and Vanessa Teague.",1404.6822v4,cs.CR,2014-04-27 20:28:25+00:00,"[arxiv.Result.Author('Chris Culnane'), arxiv.Result.Author('Peter Y. A. Ryan'), arxiv.Result.Author('Steve Schneider'), arxiv.Result.Author('Vanessa Teague')]",
361,Secure and Verifiable Electronic Voting in Practice: the use of vVote in the Victorian State Election,"The November 2014 Australian State of Victoria election was the first
statutory political election worldwide at State level which deployed an
end-to-end verifiable electronic voting system in polling places. This was the
first time blind voters have been able to cast a fully secret ballot in a
verifiable way, and the first time a verifiable voting system has been used to
collect remote votes in a political election. The code is open source, and the
output from the election is verifiable. The system took 1121 votes from these
particular groups, an increase on 2010 and with fewer polling places.",1504.07098v1,cs.CR,2015-04-27 14:08:24+00:00,"[arxiv.Result.Author('Craig Burton'), arxiv.Result.Author('Chris Culnane'), arxiv.Result.Author('Steve Schneider')]",
362,Automated Analysis of Voting Systems under an Active Intruder Model in CSP,"This article presents a novel intruder model for automated reasoning about
anonymity (vote-privacy) and secrecy properties of voting systems. We adapt the
lazy spy for this purpose, as it avoids the eagerness of pre-computation of
unnecessary deductions, reducing the required state space for the analysis.
This powerful intruder behaves as a Dolev-Yao intruder, which not only observes
a protocol run but also interacts with the protocol participants, overhears
communication channels, intercepts and spoofs any messages that he has learned
or generated from any prior knowledge.
  We make several important modifications in relation to existing channel types
and the deductive system. For the former, we define various channel types for
different threat models. For the latter, we construct a large deductive system
over the space of messages transmitted in the voting system model. The model
represents the first formal treatment of the vVote system, which was used in
November 2014, in state elections in Victoria, Australia.",1705.00795v1,cs.CR,2017-05-02 04:53:07+00:00,"[arxiv.Result.Author('Murat Moran'), arxiv.Result.Author('James Heather')]",
363,A Trustworthy Electronic Voting System for Australian Federal Elections,"The existing system for determining election results in Australia is, for the
most part, secure, accurate and understandable by the average voter. This
thesis explores the design of electronic voting systems designed to achieve
these same goals, while also improving on the existing system in the areas of
counting speed, accuracy, tamper resistance and accessibility.
  Electronic voting systems have seen limited use within Australian elections,
most prominently for State Elections in Victoria (2014), New South Wales (2015)
and Western Australia (2017), along with trials of electronic voting systems in
federal elections in 2007.
  This thesis presents an analysis of the iVote electronic voting system used
for the 2017 Western Australian State Election (iVote WA), outlining a number
of security risks introduced by the use of cloud-based distributed denial of
service mitigation. In addition, this thesis presents the results of a
cross-sectional survey of Australian voters regarding levels of trust for three
voting systems: the existing paper-based system used for Australian federal
elections, the iVote WA system, and the vVote system used for the 2014
Victorian State Election.
  The analysis of iVote, combined with the survey results, are used to inform a
recommendation for future research and public policy regarding the use of
electronic voting systems in Australian federal elections.",1805.02202v1,cs.CR,2018-05-06 12:41:10+00:00,[arxiv.Result.Author('Mark Eldridge')],
364,A Peered Bulletin Board for Robust Use in Verifiable Voting Systems,"The Web Bulletin Board (WBB) is a key component of verifiable election
systems. It is used in the context of election verification to publish evidence
of voting and tallying that voters and officials can check, and where
challenges can be launched in the event of malfeasance. In practice, the
election authority has responsibility for implementing the web bulletin board
correctly and reliably, and will wish to ensure that it behaves correctly even
in the presence of failures and attacks. To ensure robustness, an
implementation will typically use a number of peers to be able to provide a
correct service even when some peers go down or behave dishonestly. In this
paper we propose a new protocol to implement such a Web Bulletin Board,
motivated by the needs of the vVote verifiable voting system. Using a
distributed algorithm increases the complexity of the protocol and requires
careful reasoning in order to establish correctness. Here we use the Event-B
modelling and refinement approach to establish correctness of the peered design
against an idealised specification of the bulletin board behaviour. In
particular we show that for n peers, a threshold of t > 2n/3 peers behaving
correctly is sufficient to ensure correct behaviour of the bulletin board
distributed design. The algorithm also behaves correctly even if honest or
dishonest peers temporarily drop out of the protocol and then return. The
verification approach also establishes that the protocols used within the
bulletin board do not interfere with each other. This is the first time a
peered web bulletin board suite of protocols has been formally verified.",1401.4151v1,cs.CR,2014-01-16 20:11:42+00:00,"[arxiv.Result.Author('Chris Culnane'), arxiv.Result.Author('Steve Schneider')]",
365,Natural Strategic Abilities in Voting Protocols,"Security properties are often focused on the technological side of the
system. One implicitly assumes that the users will behave in the right way to
preserve the property at hand. In real life, this cannot be taken for granted.
In particular, security mechanisms that are difficult and costly to use are
often ignored by the users, and do not really defend the system against
possible attacks.
  Here, we propose a graded notion of security based on the complexity of the
user's strategic behavior. More precisely, we suggest that the level to which a
security property $\varphi$ is satisfied can be defined in terms of (a) the
complexity of the strategy that the voter needs to execute to make $\varphi$
true, and (b) the resources that the user must employ on the way. The simpler
and cheaper to obtain $\varphi$, the higher the degree of security.
  We demonstrate how the idea works in a case study based on an electronic
voting scenario. To this end, we model the vVote implementation of the \Pret
voting protocol for coercion-resistant and voter-verifiable elections. Then, we
identify ""natural"" strategies for the voter to obtain receipt-freeness, and
measure the voter's effort that they require. We also look at how hard it is
for the coercer to compromise the election through a randomization attack.",2007.12424v2,cs.MA,2020-07-24 09:28:07+00:00,"[arxiv.Result.Author('Wojciech Jamroga'), arxiv.Result.Author('Damian Kurpiewski'), arxiv.Result.Author('Vadim Malvone')]",
366,vVote: a Verifiable Voting System,"The Pret a Voter cryptographic voting system was designed to be flexible and
to offer voters a familiar and easy voting experience. In this paper we present
a case study of our efforts to adapt Pret a Voter to the idiosyncrasies of
elections in the Australian state of Victoria. This technical report includes
general background, user experience and details of the cryptographic protocols
and human processes. We explain the problems, present solutions, then analyse
their security properties and explain how they tie in to other design
decisions. We hope this will be an interesting case study on the application of
end-to-end verifiable voting protocols to real elections.
  A preliminary version of this paper appeared as the 10th February 2014
version of ""Draft Technical Report for VEC vVote System"".
  The team involved in developing the vVote design described in this report
were: Craig Burton, Chris Culnane, James Heather, Rui Joaquim, Peter Y. A.
Ryan, Steve Schneider and Vanessa Teague.",1404.6822v4,cs.CR,2014-04-27 20:28:25+00:00,"[arxiv.Result.Author('Chris Culnane'), arxiv.Result.Author('Peter Y. A. Ryan'), arxiv.Result.Author('Steve Schneider'), arxiv.Result.Author('Vanessa Teague')]",
367,Secure and Verifiable Electronic Voting in Practice: the use of vVote in the Victorian State Election,"The November 2014 Australian State of Victoria election was the first
statutory political election worldwide at State level which deployed an
end-to-end verifiable electronic voting system in polling places. This was the
first time blind voters have been able to cast a fully secret ballot in a
verifiable way, and the first time a verifiable voting system has been used to
collect remote votes in a political election. The code is open source, and the
output from the election is verifiable. The system took 1121 votes from these
particular groups, an increase on 2010 and with fewer polling places.",1504.07098v1,cs.CR,2015-04-27 14:08:24+00:00,"[arxiv.Result.Author('Craig Burton'), arxiv.Result.Author('Chris Culnane'), arxiv.Result.Author('Steve Schneider')]",
368,Automated Analysis of Voting Systems under an Active Intruder Model in CSP,"This article presents a novel intruder model for automated reasoning about
anonymity (vote-privacy) and secrecy properties of voting systems. We adapt the
lazy spy for this purpose, as it avoids the eagerness of pre-computation of
unnecessary deductions, reducing the required state space for the analysis.
This powerful intruder behaves as a Dolev-Yao intruder, which not only observes
a protocol run but also interacts with the protocol participants, overhears
communication channels, intercepts and spoofs any messages that he has learned
or generated from any prior knowledge.
  We make several important modifications in relation to existing channel types
and the deductive system. For the former, we define various channel types for
different threat models. For the latter, we construct a large deductive system
over the space of messages transmitted in the voting system model. The model
represents the first formal treatment of the vVote system, which was used in
November 2014, in state elections in Victoria, Australia.",1705.00795v1,cs.CR,2017-05-02 04:53:07+00:00,"[arxiv.Result.Author('Murat Moran'), arxiv.Result.Author('James Heather')]",
369,A Trustworthy Electronic Voting System for Australian Federal Elections,"The existing system for determining election results in Australia is, for the
most part, secure, accurate and understandable by the average voter. This
thesis explores the design of electronic voting systems designed to achieve
these same goals, while also improving on the existing system in the areas of
counting speed, accuracy, tamper resistance and accessibility.
  Electronic voting systems have seen limited use within Australian elections,
most prominently for State Elections in Victoria (2014), New South Wales (2015)
and Western Australia (2017), along with trials of electronic voting systems in
federal elections in 2007.
  This thesis presents an analysis of the iVote electronic voting system used
for the 2017 Western Australian State Election (iVote WA), outlining a number
of security risks introduced by the use of cloud-based distributed denial of
service mitigation. In addition, this thesis presents the results of a
cross-sectional survey of Australian voters regarding levels of trust for three
voting systems: the existing paper-based system used for Australian federal
elections, the iVote WA system, and the vVote system used for the 2014
Victorian State Election.
  The analysis of iVote, combined with the survey results, are used to inform a
recommendation for future research and public policy regarding the use of
electronic voting systems in Australian federal elections.",1805.02202v1,cs.CR,2018-05-06 12:41:10+00:00,[arxiv.Result.Author('Mark Eldridge')],
370,A Peered Bulletin Board for Robust Use in Verifiable Voting Systems,"The Web Bulletin Board (WBB) is a key component of verifiable election
systems. It is used in the context of election verification to publish evidence
of voting and tallying that voters and officials can check, and where
challenges can be launched in the event of malfeasance. In practice, the
election authority has responsibility for implementing the web bulletin board
correctly and reliably, and will wish to ensure that it behaves correctly even
in the presence of failures and attacks. To ensure robustness, an
implementation will typically use a number of peers to be able to provide a
correct service even when some peers go down or behave dishonestly. In this
paper we propose a new protocol to implement such a Web Bulletin Board,
motivated by the needs of the vVote verifiable voting system. Using a
distributed algorithm increases the complexity of the protocol and requires
careful reasoning in order to establish correctness. Here we use the Event-B
modelling and refinement approach to establish correctness of the peered design
against an idealised specification of the bulletin board behaviour. In
particular we show that for n peers, a threshold of t > 2n/3 peers behaving
correctly is sufficient to ensure correct behaviour of the bulletin board
distributed design. The algorithm also behaves correctly even if honest or
dishonest peers temporarily drop out of the protocol and then return. The
verification approach also establishes that the protocols used within the
bulletin board do not interfere with each other. This is the first time a
peered web bulletin board suite of protocols has been formally verified.",1401.4151v1,cs.CR,2014-01-16 20:11:42+00:00,"[arxiv.Result.Author('Chris Culnane'), arxiv.Result.Author('Steve Schneider')]",
371,Natural Strategic Abilities in Voting Protocols,"Security properties are often focused on the technological side of the
system. One implicitly assumes that the users will behave in the right way to
preserve the property at hand. In real life, this cannot be taken for granted.
In particular, security mechanisms that are difficult and costly to use are
often ignored by the users, and do not really defend the system against
possible attacks.
  Here, we propose a graded notion of security based on the complexity of the
user's strategic behavior. More precisely, we suggest that the level to which a
security property $\varphi$ is satisfied can be defined in terms of (a) the
complexity of the strategy that the voter needs to execute to make $\varphi$
true, and (b) the resources that the user must employ on the way. The simpler
and cheaper to obtain $\varphi$, the higher the degree of security.
  We demonstrate how the idea works in a case study based on an electronic
voting scenario. To this end, we model the vVote implementation of the \Pret
voting protocol for coercion-resistant and voter-verifiable elections. Then, we
identify ""natural"" strategies for the voter to obtain receipt-freeness, and
measure the voter's effort that they require. We also look at how hard it is
for the coercer to compromise the election through a randomization attack.",2007.12424v2,cs.MA,2020-07-24 09:28:07+00:00,"[arxiv.Result.Author('Wojciech Jamroga'), arxiv.Result.Author('Damian Kurpiewski'), arxiv.Result.Author('Vadim Malvone')]",
372,Risk Automatic Prediction for Social Economy Companies using Camels,"Governments have to supervise and inspect social economy enterprises (SEEs).
However, inspecting all SEEs is not possible due to the large number of SEEs
and the low number of inspectors in general. We proposed a prediction model
based on a machine learning approach. The method was trained with the random
forest algorithm with historical data provided by each SEE. Three consecutive
periods of data were concatenated. The proposed method uses these periods as
input data and predicts the risk of each SEE in the fourth period. The model
achieved 76\% overall accuracy. In addition, it obtained good accuracy in
predicting the high risk of a SEE. We found that the legal nature and the
variation of the past-due portfolio are good predictors of the future risk of a
SEE. Thus, the risk of a SEE in a future period can be predicted by a
supervised machine learning method. Predicting the high risk of a SEE improves
the daily work of each inspector by focusing only on high-risk SEEs.",2210.05052v1,cs.LG,2022-10-10 23:50:02+00:00,"[arxiv.Result.Author('Joseph Gallego-Mejia'), arxiv.Result.Author('Daniela Martin-Vega'), arxiv.Result.Author('Fabio Gonzalez')]",
373,Triviality and the (Supersymmetric) See-Saw,"For the D=5 Majorana neutrino mass operator to have a see-saw ultraviolet
completion that is viable up to the Planck scale, the see-saw scale is bounded
above due to triviality limits on the see-saw couplings. For supersymmetric
see-saw models, with realistic neutrino mass textures, we compare constraints
on the see-saw scale from triviality bounds, with those arising from
experimental limits on induced charged-lepton flavour violation, for both the
CMSSM and for models with split supersymmetry.",hep-ph/0603053v2,hep-ph,2006-03-07 16:47:09+00:00,"[arxiv.Result.Author('Bruce A. Campbell'), arxiv.Result.Author('David W. Maybury')]","JHEP 0704:077,2007"
374,Astronomical Seeing at Maidanak Observatory during the year 2018,"Astronomical seeing measurements were carried out at Maidanak observatory
during the period from August to November 2018 using DIMM (Differential Image
Motion Monitor). The median value of seeing for the entire period was
determined as 0.54 arcseconds. This value was compared to the seeing data of
the period 1996-2002.",2012.08110v1,astro-ph.IM,2020-12-15 06:19:17+00:00,"[arxiv.Result.Author('Y. A. Tillayev'), arxiv.Result.Author('A. M. Azimov'), arxiv.Result.Author('A. R. Hafizov')]",
375,"The see-saw mechanism: neutrino mixing, leptogenesis and lepton flavor violation","The see-saw mechanism to generate small neutrino masses is reviewed. After
summarizing our current knowledge about the low energy neutrino mass matrix we
consider reconstructing the see-saw mechanism. Low energy neutrino physics is
not sufficient to reconstruct see-saw, a feature which we refer to as ``see-saw
degeneracy''. Indirect tests of see-saw are leptogenesis and lepton flavor
violation in supersymmetric scenarios, which together with neutrino mass and
mixing define the framework of see-saw phenomenology. Several examples are
given, both phenomenological and GUT-related. Variants of the see-saw mechanism
like the type II or triplet see-saw are also discussed. In particular, we
compare many general aspects regarding the dependence of LFV on low energy
neutrino parameters in the extreme cases of a dominating conventional see-saw
term or a dominating triplet term. For instance, the absence of mu -> e gamma
or tau -> e gamma in the pure triplet case means that CP is conserved in
neutrino oscillations. Scanning models, we also find that among the decays mu
-> e gamma, tau -> e gamma and tau -> mu gamma the latter one has the largest
branching ratio in (i) SO(10) type I see-saw models and in (ii) scenarios in
which the triplet term dominates in the neutrino mass matrix.",0804.3925v3,hep-ph,2008-04-24 13:44:29+00:00,[arxiv.Result.Author('Werner Rodejohann')],"Pramana 72:217-227,2009"
376,Statistics of turbulence profile at Cerro Tololo,"Results of 3-month continuous monitoring of turbulence profile and seeing at
Cerro Tololo (Chile) in May-July 2002 are presented. Some 28000 low-resolution
profiles were measured by a new MASS single-star turbulence monitor,
accompanied by seeing data from DIMM. The median seeing was 0.95 arcseconds.
The first 500 m contribute 60% to the total seeing, the free-atmosphere median
seeing was 0.55 arcseconds. Free-atmosphere seeing is almost never better than
0.15 arcseconds because there is always some turbulence above 12 km. A 4-day
period of calm upper atmosphere with a stable free-atmosphere seeing of 0.2-0.3
arcseconds was noted. A gain in resolution from adaptive compensation of ground
layer will be 1.7 times typically and 2-3 times during such calm periods.
Correlations of the free-atmosphere turbulence with the wind speed at
tropopause and of the ground-layer turbulence with ground wind are studied.
Temporal evolution of turbulence is characterized by recurrent bursts, their
typical duration increases from 15 minutes in low layers to 1-2 hours in high
layers. The large data base of turbulence profiles can be used to test
meso-scale modeling of astronomical seeing.",astro-ph/0209432v1,astro-ph,2002-09-20 16:32:20+00:00,"[arxiv.Result.Author('A. Tokovinin'), arxiv.Result.Author('S. Baumont'), arxiv.Result.Author('J. Vasquez')]",Mon.Not.Roy.Astron.Soc. 340 (2003) 52
377,The Spectrum of HD 3651B: An Extrasolar Nemesis?,This article has been withdrawn; see astro-ph/0611542,astro-ph/0609556v2,astro-ph,2006-09-20 00:36:50+00:00,[arxiv.Result.Author('Adam J. Burgasser')],
378,Criticality versus q in the 2+1-dimensional $Z_q$ clock model,"Paper has been withdrawn, see comment.",cond-mat/0212255v2,cond-mat.supr-con,2002-12-11 17:32:31+00:00,"[arxiv.Result.Author('J. Hove'), arxiv.Result.Author('A. Sudbo')]",
379,Closedness of the Space of AHE Metrics on 4-Manifolds,See comment above.,math/0012167v2,math.DG,2000-12-18 19:05:21+00:00,[arxiv.Result.Author('Michael T. Anderson')],
380,A Machine Learning Approach to Correcting Atmospheric Seeing in Solar Flare Observations,"Current post-processing techniques for the correction of atmospheric seeing
in solar observations -- such as Speckle interferometry and Phase Diversity
methods -- have limitations when it comes to their reconstructive capabilities
of solar flare observations. This, combined with the sporadic nature of flares
meaning observers cannot wait until seeing conditions are optimal before taking
measurements, means that many ground-based solar flare observations are marred
with bad seeing. To combat this, we propose a method for dedicated flare seeing
correction based on training a deep neural network to learn to correct
artificial seeing from flare observations taken during good seeing conditions.
This model uses transfer learning, a novel technique in solar physics, to help
learn these corrections. Transfer learning is when another network already
trained on similar data is used to influence the learning of the new network.
Once trained, the model has been applied to two flare datasets: one from
AR12157 on 2014/09/06 and one from AR12673 on 2017/09/06. The results show good
corrections to images with bad seeing with a relative error assigned to the
estimate based on the performance of the model. Further discussion takes place
of improvements to the robustness of the error on these estimates.",2011.12814v1,astro-ph.SR,2020-11-25 15:17:26+00:00,"[arxiv.Result.Author('John A. Armstrong'), arxiv.Result.Author('Lyndsay Fletcher')]",
381,Daytime Seeing and Solar Limb Positions,"A method to measure the seeing from video made during drift-scan solar
transits is proposed. The limb of the Sun is projected over a regular grid
evenly spaced. The temporal dispersion of the time intervals among the contacts
between solar limb and grid's rows is proportional to the atmospheric seeing.
Seeing effects on the position of the inflexion point of the limb's luminosity
profile are calculated numerically with Fast Fourier Transform. Observational
examples from Locarno and Paris Observatories are presented to show the
asymmetric contributions of the seeing at the beginning and the end of each
drift-scan transit.",1106.2539v1,astro-ph.IM,2011-06-13 19:58:47+00:00,[arxiv.Result.Author('Costantino Sigismondi')],
382,The effect of aperture and seeing on the visibility of sunspots when using early modern and modern telescopes of modest size,"Using the convolution of seeing and diffraction, the relation between seeing
and aperture in the visibility of sunspots is explored. It is shown that even
telescopes with apertures smaller than 5 centimetres are significantly affected
by seeing. Although larger aperture instruments suffer more from seeing than
smaller ones, their level of detail always remains better under the same
atmospheric conditions.",2208.07244v3,astro-ph.SR,2022-08-15 15:00:51+00:00,"[arxiv.Result.Author('Nicolàs de Hilster'), arxiv.Result.Author('Siebren van der Werf')]",
383,Typical duration of good seeing sequences at Concordia,"Context: The winter seeing at Concordia is essentially bimodal, excellent or
quite poor, with relative proportions that depend on altitude above the snow
surface. This paper studies the temporal behavior of the good seeing sequences.
Aims: An efficient exploitation of extremely good seeing with an adaptive
optics system needs long integrations. It is then important to explore the
temporal distribution of the fraction of time providing excellent seeing.
Methods: Temporal windows of good seeing are created by a simple binary
process. Good or bad. Their autocorrelations are corrected for those of the
existing data sets, since these are not continuous, being often interrupted by
technical problems in addition to the adverse weather gaps. At the end these
corrected autocorrelations provide the typical duration of good seeing
sequences. This study has to be a little detailed as its results depend on the
season, summer or winter. Results: Using a threshold of 0.5 arcsec to define
the ""good seeing"", three characteristic numbers are found to describe the
temporal evolution of the good seeing windows. The first number is the mean
duration of an uninterrupted good seeing sequence: it is $\tau_0=7.5$ hours at
8 m above the ground (15 hours at 20 m). These sequences are randomly
distributed in time, with a negative exponential law of damping time
$\tau_1=29$ hours (at elevation 8 m and 20 m). The third number is the mean
time between two 29 hours episodes. It is T=10 days at 8 m high (5 days at 20
m).",1003.3583v1,astro-ph.IM,2010-03-18 14:09:06+00:00,"[arxiv.Result.Author('Eric Fossat'), arxiv.Result.Author('Eric Aristidi'), arxiv.Result.Author('Karim Agabi'), arxiv.Result.Author('Erick Bondoux'), arxiv.Result.Author('Zalpha Challita'), arxiv.Result.Author('Francois Jeanneaux'), arxiv.Result.Author('Djamel Mekarnia')]",
384,Excellent daytime seeing at Dome Fuji on the Antarctic plateau,"Context. Dome Fuji, the second highest region on the Antarctic plateau, is
expected to have some of the best astronomical seeing on Earth. However, site
testing at Dome Fuji is still in its very early stages.
  Aims. To investigate the astronomical seeing in the free atmosphere above
Dome Fuji, and to determine the height of the surface boundary layer.
  Methods. A Differential Image Motion Monitor was used to measure the seeing
in the visible (472 nm) at a height of 11 m above the snow surface at Dome Fuji
during the austral summer of 2012/2013.
  Results. Seeing below 0.2'' has been observed. The seeing often has a local
minimum of ~0.3'' near 18 h local time. Some periods of excellent seeing, 0.3''
or smaller, were also observed, sometimes extending for several hours at local
midnight. The median seeing is higher, at 0.52''---this large value is believed
to be caused by periods when the telescope was within the turbulent boundary
layer.
  Conclusions. The diurnal variation of the daytime seeing at Dome Fuji is
similar to that reported for Dome C, and the height of the surface boundary
layer is consistent with previous simulations for Dome Fuji. The free
atmosphere seeing is ~0.2'', and the height of the surface boundary layer can
be as low as ~11 m.",1305.5109v1,astro-ph.IM,2013-05-22 12:38:07+00:00,"[arxiv.Result.Author('H. Okita'), arxiv.Result.Author('T. Ichikawa'), arxiv.Result.Author('M. C. B. Ashley'), arxiv.Result.Author('N. Takato')]",
385,Night-time measurements of astronomical seeing at Dome A in Antarctica,"Seeing, the angular size of stellar images blurred by atmospheric turbulence,
is a critical parameter used to assess the quality of astronomical sites.
Median values at the best mid-latitude sites are generally in the range of
0.6--0.8\,arcsec. Sites on the Antarctic plateau are characterized by
comparatively-weak turbulence in the free-atmosphere above a strong but thin
boundary layer. The median seeing at Dome C is estimated to be 0.23--0.36
arcsec above a boundary layer that has a typical height of 30\,m. At Dome A and
F, the only previous seeing measurements were made during daytime. Here we
report the first direct measurements of night-time seeing at Dome A, using a
Differential Image Motion Monitor. Located at a height of just 8\,m, it
recorded seeing as low as 0.13\,arcsec, and provided seeing statistics that are
comparable to those for a 20\,m height at Dome C. It indicates that the
boundary layer was below 8\,m 31\% of the time. At such times the median seeing
was 0.31\,arcsec, consistent with free-atmosphere seeing. The seeing and
boundary layer thickness are found to be strongly correlated with the
near-surface temperature gradient. The correlation confirms a median thickness
of approximately 14\,m for the boundary layer at Dome A, as found from a sonic
radar. The thinner boundary layer makes it less challenging to locate a
telescope above it, thereby giving greater access to the free-atmosphere.",2007.15365v1,astro-ph.IM,2020-07-30 10:36:24+00:00,"[arxiv.Result.Author('Bin Ma'), arxiv.Result.Author('Zhaohui Shang'), arxiv.Result.Author('Yi Hu'), arxiv.Result.Author('Keliang Hu'), arxiv.Result.Author('Yongjiang Wang'), arxiv.Result.Author('Xu Yang'), arxiv.Result.Author('Michael C. B. Ashley'), arxiv.Result.Author('Paul Hickson'), arxiv.Result.Author('Peng Jiang')]","Nature 583, 771-774 (2020)"
386,Auditing Indian Elections,"Indian Electronic Voting Machines (EVMs) will be fitted with printers that
produce Voter-Verifiable Paper Audit Trails (VVPATs) in time for the 2019
general election. VVPATs provide evidence that each vote was recorded as the
voter intended, without having to trust the perfection or security of the EVMs.
  However, confidence in election results requires more: VVPATs must be
preserved inviolate and then actually used to check the reported election
result in a trustworthy way that the public can verify. A full manual tally
from the VVPATs could be prohibitively expensive and time-consuming; moreover,
it is difficult for the public to determine whether a full hand count was
conducted accurately. We show how Risk-Limiting Audits (RLAs) could provide
high confidence in Indian election results. Compared to full hand recounts,
RLAs typically require manually inspecting far fewer VVPATs when the outcome is
correct, and are much easier for the electorate to observe in adequate detail
to determine whether the result is trustworthy.",1901.03108v2,cs.CR,2019-01-10 11:38:41+00:00,"[arxiv.Result.Author('Vishal Mohanty'), arxiv.Result.Author('Nicholas Akinyokun'), arxiv.Result.Author('Andrew Conway'), arxiv.Result.Author('Chris Culnane'), arxiv.Result.Author('Philip B. Stark'), arxiv.Result.Author('Vanessa Teague')]",
387,An AI-Powered VVPAT Counter for Elections in India,"The Election Commission of India has introduced Voter Verified Paper Audit
Trail since 2019. This mechanism has increased voter confidence at the time of
casting the votes. However, physical verification of the VVPATs against the
party level counts from the EVMs is done only in 5 (randomly selected) machines
per constituency. The time required to conduct physical verification becomes a
bottleneck in scaling this activity for 100% of machines in all constituencies.
We proposed an automated counter powered by image processing and machine
learning algorithms to speed up the process and address this issue.",2212.11124v1,cs.CV,2022-12-09 14:59:40+00:00,"[arxiv.Result.Author('Prasath Murugesan'), arxiv.Result.Author('Shamshu Dharwez Saganvali')]",
388,"Electryo, In-person Voting with Transparent Voter Verifiability and Eligibility Verifiability","Selene is an e-voting protocol that allows voters to directly check their
individual vote, in cleartext, in the final tally via a tracker system, while
providing good coercion mitigation. This is in contrast to conventional,
end-to-end verifiable schemes in which the voter verifies the presence of an
encryption of her vote on the bulletin board. The Selene mechanism can be
applied to many e-voting schemes, but here we present an application to the
polling station context, resulting in a voter-verifiable electronic tally with
a paper audit trail. The system uses a smartcard-based public key system to
provide the individual verification and universal eligibility verifiability.
The paper record contains an encrypted link to the voter's identity, requiring
stronger assumptions on ballot privacy than normal paper voting, but with the
benefit of providing good auditability and dispute resolution as well as
supporting (comparison) risk limiting audits.",2105.14783v1,cs.CR,2021-05-31 08:22:29+00:00,"[arxiv.Result.Author('Peter B. Roenne'), arxiv.Result.Author('Peter Y. A Ryan'), arxiv.Result.Author('Marie-Laure Zollinger')]",
389,SOBA: Secrecy-preserving Observable Ballot-level Audit,"SOBA is an approach to election verification that provides observers with
justifiably high confidence that the reported results of an election are
consistent with an audit trail (""ballots""), which can be paper or electronic.
SOBA combines three ideas: (1) publishing cast vote records (CVRs) separately
for each contest, so that anyone can verify that each reported contest outcome
is correct, if the CVRs reflect voters' intentions with sufficient accuracy;
(2) shrouding a mapping between ballots and the CVRs for those ballots to
prevent the loss of privacy that could occur otherwise; (3) assessing the
accuracy with which the CVRs reflect voters' intentions for a collection of
contests while simultaneously assessing the integrity of the shrouded mapping
between ballots and CVRs by comparing randomly selected ballots to the CVRs
that purport to represent them. Step (1) is related to work by the Humboldt
County Election Transparency Project, but publishing CVRs separately for
individual contests rather than images of entire ballots preserves privacy.
Step (2) requires a cryptographic commitment from elections officials.
Observers participate in step (3), which relies on the ""super-simple
simultaneous single-ballot risk-limiting audit."" Step (3) is designed to reveal
relatively few ballots if the shrouded mapping is proper and the CVRs
accurately reflect voter intent. But if the reported outcomes of the contests
differ from the outcomes that a full hand count would show, step (3) is
guaranteed to have a large chance of requiring all the ballots to be counted by
hand, thereby limiting the risk that an incorrect outcome will become official
and final.",1105.5803v2,stat.AP,2011-05-29 17:00:20+00:00,"[arxiv.Result.Author('Josh Benaloh'), arxiv.Result.Author('Douglas Jones'), arxiv.Result.Author('Eric Lazarus'), arxiv.Result.Author('Mark Lindeman'), arxiv.Result.Author('Philip B. Stark')]",
390,"STAR-Vote: A Secure, Transparent, Auditable, and Reliable Voting System","In her 2011 EVT/WOTE keynote, Travis County, Texas County Clerk Dana
DeBeauvoir described the qualities she wanted in her ideal election system to
replace their existing DREs. In response, in April of 2012, the authors,
working with DeBeauvoir and her staff, jointly architected STAR-Vote, a voting
system with a DRE-style human interface and a ""belt and suspenders"" approach to
verifiability. It provides both a paper trail and end-to-end cryptography using
COTS hardware. It is designed to support both ballot-level risk-limiting
audits, and auditing by individual voters and observers. The human interface
and process flow is based on modern usability research. This paper describes
the STAR-Vote architecture, which could well be the next-generation voting
system for Travis County and perhaps elsewhere.",1211.1904v1,cs.CR,2012-11-08 17:06:33+00:00,"[arxiv.Result.Author('Josh Benaloh'), arxiv.Result.Author('Mike Byrne'), arxiv.Result.Author('Philip Kortum'), arxiv.Result.Author('Neal McBurnett'), arxiv.Result.Author('Olivier Pereira'), arxiv.Result.Author('Philip B. Stark'), arxiv.Result.Author('Dan S. Wallach')]",
391,OpenVoting: Making E2E-V voting transparent and recoverable,"End-to-end verifiable (E2E-V) voting systems have been around for some time.
However, their adoption in large elections is poor, seemingly because of the
inaccessibility of their underlying complex cryptography to the general
electorate. Meanwhile, risk-limiting audits based on voter-verified paper
records (VVPR) have been effective in bringing easy-to-understand verifiability
and recoverability to electronic voting, but they generally require the
electorate to trust the post-election custody chain of the paper trail.
  In this paper, we propose \emph{OpenVoting}, a novel polling booth voting
protocol that publicly demonstrates a one-to-one correspondence between the
cryptographically-secured electronic vote records and the easily understandable
paper records, while protecting individual voter secrecy and polling
booth-level voting statistics. This one-to-one correspondence helps each
provide a check for the other, improves overall transparency, and also enables
efficient and principled recovery in case of tally mismatches, by pinpointing
mismatching votes and their associated polling booths. We propose a novel
distributed zero-knowledge proof (ZKP) that facilitates the above.
  To an ordinary voter \emph{OpenVoting} looks just like an old fashioned paper
based voting system, with minimal additional cognitive overload.",1908.09557v6,cs.CR,2019-08-26 09:32:54+00:00,"[arxiv.Result.Author('Prashant Agrawal'), arxiv.Result.Author('Kabir Tomer'), arxiv.Result.Author('Abhinav Nakarmi'), arxiv.Result.Author('Mahabir Prasad Jhanwar'), arxiv.Result.Author('Subodh Sharma'), arxiv.Result.Author('Subhashis Banerjee')]",
392,Electt: running auditable and verifiable elections in untrusted environments,"We present a system for running auditable and verifiable elections in
untrusted environments. Votes are anonymous since the order of candidates on a
ballot sheet is random. Tellers see only the position of the candidate. Voters
can check their vote. An election is auditable using blockchain log.
Threshold-encryption, which is used to implement the quorum, prevents a
deadlock from occurring if a minority of candidates or observers tries to
sabotage the election. Candidates and observers can indicate that the election
was free and fair by exposing their keys, which are used by the system to
decrypt each vote. Ballot sheets are encrypted by onion-routing, which has a
layer with the key of the election instance, so it's impossible for a quorum to
decode the results before they have announced their decision by exposing their
keys. A register of voters ensures that only verified voters can vote without
compromising their identity. If there any doubts about the identity of a voter,
their vote can be excluded from the election, if a quorum agrees. This system
is designed to scale from one instance to a distributed system that runs over
an unlimited number of instances, which can be achieved using cloud instances
or smartphones belonging to voters or tellers.",2011.10902v2,cs.CR,2020-11-22 00:58:34+00:00,[arxiv.Result.Author('Kirill A. Korinsky')],
393,Boardroom Voting: Verifiable Voting with Ballot Privacy Using Low-Tech Cryptography in a Single Room,"A boardroom election is an election that takes place in a single room -- the
boardroom -- in which all voters can see and hear each other. We present an
initial exploration of boardroom elections with ballot privacy and voter
verifiability that use only ""low-tech cryptography"" without using computers to
mark or collect ballots. Specifically, we define the problem, introduce several
building blocks, and propose a new protocol that combines these blocks in novel
ways. Our new building blocks include ""foldable ballots"" that can be rotated to
hide the alignment of ballot choices with voting marks, and ""visual secrets""
that are easy to remember and use but hard to describe. Although closely seated
participants in a boardroom election have limited privacy, the protocol ensures
that no one can determine how others voted. Moreover, each voter can verify
that their ballot was correctly cast, collected, and counted, without being
able to prove how they voted, providing assurance against undue influence.
Low-tech cryptography is useful in situations where constituents do not trust
computer technology, and it avoids the complex auditing requirements of
end-to-end cryptographic voting systems such as Pr\^{e}t-\`{a}-Voter. This
paper's building blocks and protocol are meant to be a proof of concept that
might be tested for usability and improved.",2007.14916v2,cs.CR,2020-07-29 15:40:51+00:00,"[arxiv.Result.Author('Enka Blanchard'), arxiv.Result.Author('Ted Selker'), arxiv.Result.Author('Alan T. Sherman')]",
394,"D-DEMOS: A distributed, end-to-end verifiable, internet voting system","E-voting systems have emerged as a powerful technology for improving
democracy by reducing election cost, increasing voter participation, and even
allowing voters to directly verify the entire election procedure. Prior
internet voting systems have single points of failure, which may result in the
compromise of availability, voter secrecy, or integrity of the election
results. In this paper, we present the design, implementation, security
analysis, and evaluation of D-DEMOS, a complete e-voting system that is
distributed, privacy-preserving and end-to-end verifiable. Our system includes
a fully asynchronous vote collection subsystem that provides immediate
assurance to the voter her vote was recorded as cast, without requiring
cryptographic operations on behalf of the voter. We also include a distributed,
replicated and fault-tolerant Bulletin Board component, that stores all
necessary election-related information, and allows any party to read and verify
the complete election process. Finally, we also incorporate trustees, i.e.,
individuals who control election result production while guaranteeing privacy
and end-to-end-verifiability as long as their strong majority is honest. Our
system is the first e-voting system whose voting operation is human verifiable,
i.e., a voter can vote over the web, even when her web client stack is
potentially unsafe, without sacrificing her privacy, and still be assured her
vote was recorded as cast. Additionally, a voter can outsource election
auditing to third parties, still without sacrificing privacy. Finally, as the
number of auditors increases, the probability of election fraud going
undetected is diminished exponentially. We provide a model and security
analysis of the system. We implement a prototype of the complete system, we
measure its performance experimentally, and we demonstrate its ability to
handle large-scale elections.",1507.06812v2,cs.CR,2015-07-24 11:29:12+00:00,"[arxiv.Result.Author('Nikos Chondros'), arxiv.Result.Author('Bingsheng Zhang'), arxiv.Result.Author('Thomas Zacharias'), arxiv.Result.Author('Panos Diamantopoulos'), arxiv.Result.Author('Stathis Maneas'), arxiv.Result.Author('Christos Patsonakis'), arxiv.Result.Author('Alex Delis'), arxiv.Result.Author('Aggelos Kiayias'), arxiv.Result.Author('Mema Roussopoulos')]",
395,Non(c)esuch Ballot-Level Risk-Limiting Audits for Precinct-Count Voting Systems,"Risk-limiting audits (RLAs) guarantee a high probability of correcting
incorrect reported outcomes before the outcomes are certified. The most
efficient use ballot-level comparison, comparing the voting system's
interpretation of individual ballot cards sampled at random (cast-vote records,
CVRs) from a trustworthy paper trail to a human interpretation of the same
cards. Such comparisons require the voting system to create and export CVRs in
a way that can be linked to the individual ballots the CVRs purport to
represent. Such links can be created by keeping the ballots in the order in
which they are scanned or by printing a unique serial number on each ballot.
But for precinct-count systems (PCOS), these strategies may compromise vote
anonymity: the order in which ballots are cast may identify the voters who cast
them. Printing a unique pseudo-random number (""cryptographic nonce"") on each
ballot card after the voter last touches it could reduce such privacy risks.
But what if the system does not in fact print a unique number on each ballot or
does not accurately report the numbers it printed? This paper gives two ways to
conduct an RLA so that even if the system does not print a genuine nonce on
each ballot or misreports the nonces it used, the audit's risk limit is not
compromised (however, the anonymity of votes might be compromised). One method
allows untrusted technology to be used to imprint and to retrieve ballot cards.
The method is adaptive: if the technology behaves properly, this protection
does not increase the audit workload. But if the imprinting or retrieval system
misbehaves, the sample size the RLA requires to confirm the reported results
when the results are correct is generally larger than if the imprinting and
retrieval were accurate.",2207.01362v1,cs.CR,2022-07-04 12:35:42+00:00,[arxiv.Result.Author('Philip B. Stark')],
396,Verifiable Elections with Commitment Consistent Encryption -- A Primer,"This note provides an introduction to the PPATS Commitment Consistent
Encryption (CCE) scheme proposed by Cuvelier, Pereira and Peters and its use in
the design of end-to-end verifiable elections with a perfectly private audit
trail. These elections can be verified using audit data that will never leak
any information about the vote, even if all the private keys of the elections
are compromised, or if the cryptographic assumptions are broken.",1412.7358v1,cs.CR,2014-12-23 13:35:21+00:00,[arxiv.Result.Author('Olivier Pereira')],
397,Towards Privacy-assured and Lightweight On-chain Auditing of Decentralized Storage,"How to audit outsourced data in centralized storage like cloud is
well-studied, but it is largely under-explored for the rising decentralized
storage network (DSN) that bodes well for a billion-dollar market. To realize
DSN as a usable service in a truly decentralized manner, the blockchain comes
in handy -- to record and verify audit trails in forms of proof of storage, and
based on that, to handle fair payments with necessary dispute resolution.
  Leaving the audit trails on the blockchain offers transparency and fairness,
yet it 1) sacrifices privacy, as they may leak information about the data under
audit, and 2) overwhelms on-chain resources, as they may be practically large
in size and expensive to verify. Prior auditing designs in centralized settings
are not directly applicable here. A handful of proposals targeting DSN cannot
satisfactorily address these issues either.
  We present an auditing solution that addresses on-chain privacy and
efficiency, from a synergy of homomorphic linear authenticators with polynomial
commitments for succinct proofs, and the sigma protocol for provable privacy.
The solution results in, per audit, 288-byte proof written to the blockchain,
and constant verification cost. It can sustain long-term operation and easily
scale to thousands of users on Ethereum.",2005.05531v3,cs.CR,2020-05-12 03:14:09+00:00,"[arxiv.Result.Author('Yuefeng Du'), arxiv.Result.Author('Huayi Duan'), arxiv.Result.Author('Anxin Zhou'), arxiv.Result.Author('Cong Wang'), arxiv.Result.Author('Man Ho Au'), arxiv.Result.Author('Qian Wang')]",
398,Explicit Auditing,"The Calculus of Audited Units (CAU) is a typed lambda calculus resulting from
a computational interpretation of Artemov's Justification Logic under the
Curry-Howard isomorphism; it extends the simply typed lambda calculus by
providing audited types, inhabited by expressions carrying a trail of their
past computation history. Unlike most other auditing techniques, CAU allows the
inspection of trails at runtime as a first-class operation, with applications
in security, debugging, and transparency of scientific computation.
  An efficient implementation of CAU is challenging: not only do the sizes of
trails grow rapidly, but they also need to be normalized after every beta
reduction. In this paper, we study how to reduce terms more efficiently in an
untyped variant of CAU by means of explicit substitutions and explicit auditing
operations, finally deriving a call-by-value abstract machine.",1808.00486v1,cs.LO,2018-08-01 18:03:02+00:00,"[arxiv.Result.Author('Wilmer Ricciotti'), arxiv.Result.Author('James Cheney')]",
399,Auditing for Core Stability in Participatory Budgeting,"We consider the participatory budgeting problem where each of $n$ voters
specifies additive utilities over $m$ candidate projects with given sizes, and
the goal is to choose a subset of projects (i.e., a committee) with total size
at most $k$. Participatory budgeting mathematically generalizes multiwinner
elections, and both have received great attention in computational social
choice recently. A well-studied notion of group fairness in this setting is
core stability: Each voter is assigned an ""entitlement"" of $\frac{k}{n}$, so
that a subset $S$ of voters can pay for a committee of size at most $|S| \cdot
\frac{k}{n}$. A given committee is in the core if no subset of voters can pay
for another committee that provides each of them strictly larger utility. This
provides proportional representation to all voters in a strong sense.
  In this paper, we study the following auditing question: Given a committee
computed by some preference aggregation method, how close is it to the core?
Concretely, how much does the entitlement of each voter need to be scaled down
by, so that the core property subsequently holds? As our main contribution, we
present computational hardness results for this problem, as well as a
logarithmic approximation algorithm via linear program rounding. We show that
our analysis is tight against the linear programming bound. Additionally, we
consider two related notions of group fairness that have similar audit
properties. The first is Lindahl priceability, which audits the closeness of a
committee to a market clearing solution. We show that this is related to the
linear programming relaxation of auditing the core, leading to efficient exact
and approximation algorithms for auditing. The second is a novel weakening of
the core that we term the sub-core, and we present computational results for
auditing this notion as well.",2209.14468v1,cs.GT,2022-09-28 23:13:06+00:00,"[arxiv.Result.Author('Kamesh Munagala'), arxiv.Result.Author('Yiheng Shen'), arxiv.Result.Author('Kangning Wang')]",
400,TellTable Spreadsheet Audit: from Technical Possibility to Operating Prototype,"At the 2003 EuSpRIG meeting, we presented a framework and software
infrastructure to generate and analyse an audit trail for a spreadsheet file.
This report describes the results of a pilot implementation of this software
(now called TellTable; see www.telltable.com), along with developments in the
server infrastructure and availability, extensions to other ""Office Suite""
files, integration of the audit tool into the server interface, and related
developments, licensing and reports. We continue to seek collaborators and
partners in what is primarily an open-source project with some shared-source
components.",0803.1751v1,cs.SE,2008-03-12 11:36:41+00:00,"[arxiv.Result.Author('John Nash'), arxiv.Result.Author('Andy Adler'), arxiv.Result.Author('Neil Smith')]","Proc. European Spreadsheet Risks Int. Grp. (EuSpRIG) 2004
  45-55ISBN 1 902724 94 1"
401,Risk-Limiting Tallies,"Many voter-verifiable, coercion-resistant schemes have been proposed, but
even the most carefully designed systems necessarily leak information via the
announced result. In corner cases, this may be problematic. For example, if all
the votes go to one candidate then all vote privacy evaporates. The mere
possibility of candidates getting no or few votes could have implications for
security in practice: if a coercer demands that a voter cast a vote for such an
unpopular candidate, then the voter may feel obliged to obey, even if she is
confident that the voting system satisfies the standard coercion resistance
definitions. With complex ballots, there may also be a danger of ""Italian""
style (aka ""signature"") attacks: the coercer demands the voter cast a ballot
with a specific, identifying pattern. Here we propose an approach to tallying
end-to-end verifiable schemes that avoids revealing all the votes but still
achieves whatever confidence level in the announced result is desired. Now a
coerced voter can claim that the required vote must be amongst those that
remained shrouded. Our approach is based on the well-established notion of
Risk-Limiting Audits, but here applied to the tally rather than to the audit.
We show that this approach counters coercion threats arising in extreme tallies
and ""Italian"" attacks. We illustrate our approach by applying it to the Selene
scheme, and we extend the approach to Risk-Limiting Verification, where not all
vote trackers are revealed, thereby enhancing the coercion mitigation
properties of Selene.",1908.04947v1,cs.CR,2019-08-14 04:23:10+00:00,"[arxiv.Result.Author('Wojciech Jamroga'), arxiv.Result.Author('Peter B. Roenne'), arxiv.Result.Author('Peter Y. A. Ryan'), arxiv.Result.Author('Philip B. Stark')]",
402,On the security of ballot marking devices,"A recent debate among election experts has considered whether electronic
ballot marking devices (BMDs) have adequate security against the risks of
malware. A malicious BMD might produce a printed ballot that disagrees with a
voter's actual intent, with the hope that voters would be unlikely to detect
this subterfuge. This essay considers how an election administrator can create
reasonable auditing procedures to gain confidence that their fleet of BMDs is
operating correctly, allowing voters to benefit from the usability and
accessibility features of BMDs while the overall election still benefits from
the same security and reliability properties we expect from hand-marked paper
ballots.",1908.01897v2,cs.CR,2019-08-05 23:04:16+00:00,[arxiv.Result.Author('Dan S. Wallach')],
403,EMA: Auditing Data Removal from Trained Models,"Data auditing is a process to verify whether certain data have been removed
from a trained model. A recently proposed method (Liu et al. 20) uses
Kolmogorov-Smirnov (KS) distance for such data auditing. However, it fails
under certain practical conditions. In this paper, we propose a new method
called Ensembled Membership Auditing (EMA) for auditing data removal to
overcome these limitations. We compare both methods using benchmark datasets
(MNIST and SVHN) and Chest X-ray datasets with multi-layer perceptrons (MLP)
and convolutional neural networks (CNN). Our experiments show that EMA is
robust under various conditions, including the failure cases of the previously
proposed method. Our code is available at: https://github.com/Hazelsuko07/EMA.",2109.03675v2,cs.LG,2021-09-08 14:22:02+00:00,"[arxiv.Result.Author('Yangsibo Huang'), arxiv.Result.Author('Xiaoxiao Li'), arxiv.Result.Author('Kai Li')]",
404,Reversibility and Composition of Rewriting in Hierarchies,"In this paper, we study how graph transformations based on sesqui-pushout
rewriting can be reversed and how the composition of rewrites can be
constructed. We illustrate how such reversibility and composition can be used
to design an audit trail system for individual graphs and graph hierarchies.
This provides us with a compact way to maintain the history of updates of an
object, including its multiple versions. The main application of the designed
framework is an audit trail of updates to knowledge represented by hierarchies
of graphs. Therefore, we introduce the notion of rule hierarchy that represents
a transformation of the entire hierarchy, study how rule hierarchies can be
applied to hierarchies and analyse the conditions under which this application
is reversible. We then present a theory for constructing the composition of
consecutive hierarchy rewrites. The prototype audit trail system for
transformations in hierarchies of simple graphs with attributes is implemented
as part of the ReGraph Python library.",2012.01661v1,cs.LO,2020-12-03 02:29:28+00:00,"[arxiv.Result.Author('Russ Harmer'), arxiv.Result.Author('Eugenia Oshurko')]","EPTCS 330, 2020, pp. 145-162"
405,Phrase-Verified Voting: Verifiable Low-Tech Remote Boardroom Voting,"We present Phrase-Verified Voting, a voter-verifiable remote voting system
assembled from commercial off-the-shelf software for small private elections.
The system is transparent and enables each voter to verify that the tally
includes their ballot selection without requiring any understanding of
cryptography. This paper describes the system and its use in fall 2020, to vote
remotely in promotion committees in a university. Each voter fills out a form
in the cloud with their vote V (YES, NO, ABSTAIN) and a passphrase P-two words
entered by the voter. The system generates a verification prompt of the (P,V)
pairs and a tally of the votes, organized to help visualize how the votes add
up. After the polls close, each voter verifies that this table lists their
(P,V) pair and that the tally is computed correctly. The system is especially
appropriate for any small group making sensitive decisions. Because the system
would not prevent a coercer from demanding that their victim use a specified
passphrase, it is not designed for applications where such malfeasance would be
likely or go undetected. Results from 43 voters show that the system was
well-accepted, performed effectively for its intended purpose, and introduced
users to the concept of voter-verified elections. Compared to the commonly-used
alternatives of paper ballots or voting by email, voters found the system
easier to use, and that it provided greater privacy and outcome integrity.",2103.07180v1,cs.CR,2021-03-12 09:59:55+00:00,"[arxiv.Result.Author('Enka Blanchard'), arxiv.Result.Author('Ryan Robucci'), arxiv.Result.Author('Ted Selker'), arxiv.Result.Author('Alan Sherman')]",
406,A mathematical evaluation of vote transfer systems,"The paper builds a general model of vote transfer systems on the basis of the
current Hungarian electoral rules. It combines single-seat districts and list
mandates with three possible compensation rule for 'wasted' votes in
constituencies: no compensation (direct vote transfer, DVT), compensation for
votes cast for losing party candidates (positive vote transfer, PVT) and
compensation for all votes that are not necessary to win the district (negative
vote transfer, NVT).
  The model is studied in the case of two parties. When the number of votes for
the majority party follows a uniform distribution in each district, DVT results
in the greatest expected seat share, however, application of PVT, and,
especially, NVT increases the probability of winning the election. The
trade-off between vote transfer formulas and the number of list mandates
reveals that the majority party should use an appropriately calibrated NVT
system if it focuses on these two variables.",1607.02879v3,cs.GT,2016-07-11 09:58:58+00:00,[arxiv.Result.Author('László Csató')],
407,Measures of Partisan Bias for Legislating Fair Elections,"Several measures of partisan bias are reviewed for single member districts
with two dominant parties. These include variants of the simple bias that
considers only deviation of seats from 50% at statewide 50% vote. Also included
are equalization of losing votes and equalization of wasted votes, both of
which apply directly when the statewide vote is not 50% and which require, not
just partisan symmetry, but specific forms of the seats-votes curve. A new
measure of bias is introduced, based on the geometric area between the
seats-vote curve and the symmetrically inverted seats-votes curve. These
measures are applied to recent Pennsylvania congressional elections and to
abstract models of the seats-votes curves. The numerical values obtained from
the various measures of bias are compared and contrasted. Each bias measure has
merits for different seats-votes curves and for different elections, but all
essentially agree for most cases when applied to measure only partisan bias,
not conflated with competitiveness. This supports the inclusion of partisan
fairness as a fundamental element for election law reform, and some options are
discussed.",1505.06749v1,physics.soc-ph,2015-05-25 20:27:21+00:00,[arxiv.Result.Author('John F. Nagle')],Election Law Journal 14 (2015) 346-360
408,A Relative Efficiency Gap formula for measuring Political Gerrymandering,"The efficiency gap formula was introduced in to measure political
gerrymandering. It played a key role in the Gill v. Whitford case whose appeal
is currently before the Supreme Court, but it was very recently shown by
Bernstein and Duchin to have some problematic mathematical properties. We
propose a new relative version of the efficiency gap formula that inherits its
desirable but not its undesirable features. Instead of measuring the difference
between the number of votes wasted by the two parties, we measure the
difference between the proportions of their votes that the two parties wasted.",1710.11086v2,physics.soc-ph,2017-10-30 17:32:54+00:00,[arxiv.Result.Author('Kristopher Tapp')],
409,Recognition of Cardiac MRI Orientation via Deep Neural Networks and a Method to Improve Prediction Accuracy,"In most medical image processing tasks, the orientation of an image would
affect computing result. However, manually reorienting images wastes time and
effort. In this paper, we study the problem of recognizing orientation in
cardiac MRI and using deep neural network to solve this problem. For multiple
sequences and modalities of MRI, we propose a transfer learning strategy, which
adapts our proposed model from a single modality to multiple modalities. We
also propose a prediction method that uses voting. The results shows that deep
neural network is an effective way in recognition of cardiac MRI orientation
and the voting prediction method could improve accuracy.",2211.07088v2,eess.IV,2022-11-14 03:35:15+00:00,[arxiv.Result.Author('Houxin Zhou')],
410,Conversational Markers of Constructive Discussions,"Group discussions are essential for organizing every aspect of modern life,
from faculty meetings to senate debates, from grant review panels to papal
conclaves. While costly in terms of time and organization effort, group
discussions are commonly seen as a way of reaching better decisions compared to
solutions that do not require coordination between the individuals (e.g.
voting)---through discussion, the sum becomes greater than the parts. However,
this assumption is not irrefutable: anecdotal evidence of wasteful discussions
abounds, and in our own experiments we find that over 30% of discussions are
unproductive.
  We propose a framework for analyzing conversational dynamics in order to
determine whether a given task-oriented discussion is worth having or not. We
exploit conversational patterns reflecting the flow of ideas and the balance
between the participants, as well as their linguistic choices. We apply this
framework to conversations naturally occurring in an online collaborative world
exploration game developed and deployed to support this research. Using this
setting, we show that linguistic cues and conversational patterns extracted
from the first 20 seconds of a team discussion are predictive of whether it
will be a wasteful or a productive one.",1604.07407v1,cs.CL,2016-04-25 20:00:02+00:00,"[arxiv.Result.Author('Vlad Niculae'), arxiv.Result.Author('Cristian Danescu-Niculescu-Mizil')]",
411,Alleviating partisan gerrymandering: can math and computers help to eliminate wasted votes?,"Partisan gerrymandering is a major cause for voter disenfranchisement in
United States. However, convincing US courts to adopt specific measures to
quantify gerrymandering has been of limited success to date. Recently,
Stephanopoulos and McGhee introduced a new and precise measure of partisan
gerrymandering via the so-called ""efficiency gap"" that computes the absolutes
difference of wasted votes between two political parties in a two-party system.
Quite importantly from a legal point of view, this measure was found legally
convincing enough in a US appeals court in a case that claims that the
legislative map of the state of Wisconsin was gerrymandered; the case is now
pending in US Supreme Court. In this article, we show the following:
  (a) We provide interesting mathematical and computational complexity
properties of the problem of minimizing the efficiency gap measure. To the best
of our knowledge, these are the first non-trivial theoretical and algorithmic
analyses of this measure of gerrymandering.
  (b) We provide a simple and fast algorithm that can ""un-gerrymander"" the
district maps for the states of Texas, Virginia, Wisconsin and Pennsylvania by
bring their efficiency gaps to acceptable levels from the current unacceptable
levels. Our work thus shows that, notwithstanding the general worst case
approximation hardness of the efficiency gap measure as shown by us,finding
district maps with acceptable levels of efficiency gaps is a computationally
tractable problem from a practical point of view. Based on these empirical
results, we also provide some interesting insights into three practical issues
related the efficiency gap measure.
  We believe that, should the US Supreme Court uphold the decision of lower
courts, our research work and software will provide a crucial supporting hand
to remove partisan gerrymandering.",1804.10577v1,cs.CY,2018-04-27 16:12:11+00:00,"[arxiv.Result.Author('Tanima Chatterjee'), arxiv.Result.Author('Bhaskar DasGupta'), arxiv.Result.Author('Laura Palmieri'), arxiv.Result.Author('Zainab Al-Qurashi'), arxiv.Result.Author('Anastasios Sidiropoulos')]",
412,Analog quantum error correction with encoding a qubit into an oscillator,"To implement fault-tolerant quantum computation with continuous variables,
Gottesman-Kitaev-Preskill (GKP) qubits have been recognized as an important
technological element. However, the analog outcome of GKP qubits, which
includes beneficial information to improve the error tolerance, has been
wasted, because the GKP qubits have been treated as only discrete variables. In
this paper, we propose a hybrid quantum error correction approach that combines
digital information with the analog information of the GKP qubits using the
maximum-likelihood method. As an example, we demonstrate that the three-qubit
bit-flip code can correct double errors, whereas the conventional method based
on majority voting on the binary measurement outcome can correct only a single
error. As another example, a concatenated code known as Knill's C4/C6 code can
achieve the hashing bound for the quantum capacity of the Gaussian quantum
channel. To the best of our knowledge, this approach is the first attempt to
draw both digital and analog information from a single quantum state to improve
quantum error correction performance.",1706.03011v3,quant-ph,2017-06-09 15:55:59+00:00,"[arxiv.Result.Author('Kosuke Fukui'), arxiv.Result.Author('Akihisa Tomita'), arxiv.Result.Author('Atsushi Okamoto')]","Phys. Rev. Lett. 119, 180507 (2017)"
413,Strategy-proof Popular Mechanisms,"We consider the allocation of indivisible objects when agents have
preferences over their own allocations, but share the ownership of the
resources to be distributed. Examples might include seats in public schools,
faculty offices, and time slots in public tennis courts. Given an allocation,
groups of agents who would prefer an alternative allocation might challenge it.
An assignment is popular if it is not challenged by another one. By assuming
that agents' ability to challenge allocations can be represented by weighted
votes, we characterize the conditions under which popular allocations might
exist and when these can be implemented via strategy-proof mechanisms. Serial
dictatorships that use orderings consistent with the agents' weights are not
only strategy-proof and Pareto efficient, but also popular, whenever these
assignments exist. We also provide a new characterization for serial
dictatorships as the only mechanisms that are popular, strategy-proof,
non-wasteful, and satisfy a consistency condition.",2012.01004v2,econ.TH,2020-12-02 07:44:43+00:00,"[arxiv.Result.Author('Mustafa Oğuz Afacan'), arxiv.Result.Author('Inácio Bó')]",
414,Generating Single Peaked Votes,"We discuss how to generate singled peaked votes uniformly from the Impartial
Culture model.",1503.02766v1,cs.GT,2015-03-10 04:15:23+00:00,[arxiv.Result.Author('Toby Walsh')],
415,Normalized Range Voting Broadly Resists Control,"We study the behavior of Range Voting and Normalized Range Voting with
respect to electoral control. Electoral control encompasses attempts from an
election chair to alter the structure of an election in order to change the
outcome. We show that a voting system resists a case of control by proving that
performing that case of control is computationally infeasible. Range Voting is
a natural extension of approval voting, and Normalized Range Voting is a simple
variant which alters each vote to maximize the potential impact of each voter.
We show that Normalized Range Voting has among the largest number of control
resistances among natural voting systems.",1005.5698v3,cs.GT,2010-05-31 15:34:11+00:00,[arxiv.Result.Author('Curtis Menton')],
416,Multi-agent based IoT smart waste monitoring and collection architecture,"Solid waste management is one of the existing challenges in urban areas and
it is becoming a critical issue due to rapid increase in population.
Appropriate solid waste management systems are important for improving the
environment and the well being of residents. In this paper, an Internet of
Things (IoT) architecture for real time waste monitoring and collection has
been proposed; able to improve and optimize solid waste collection in a city.
Netlogo Multiagent platform has been used to simulate real time monitoring and
smart decisions on waste management. Waste filling level in bins and truck
collection process are abstracted to a multiagent model and citizen are
involved by paying the price for waste collection services. Furthermore, waste
level data are updated and recorded continuously and are provided to decision
algorithms to determine the vehicle optimal route for waste collection to the
distributed bins in the city. Several simulation cases executed and results
validated. The presented solution gives substantial benefits to all waste
stakeholders by enabling the waste collection process to be more efficient.",1711.03966v1,cs.MA,2017-11-10 11:44:23+00:00,"[arxiv.Result.Author('Eunice David Likotiko'), arxiv.Result.Author('Devotha Nyambo'), arxiv.Result.Author('Joseph Mwangoka')]",
417,Strategic Voting Under Uncertainty About the Voting Method,"Much of the theoretical work on strategic voting makes strong assumptions
about what voters know about the voting situation. A strategizing voter is
typically assumed to know how other voters will vote and to know the rules of
the voting method. A growing body of literature explores strategic voting when
there is uncertainty about how others will vote. In this paper, we study
strategic voting when there is uncertainty about the voting method. We
introduce three notions of manipulability for a set of voting methods: sure,
safe, and expected manipulability. With the help of a computer program, we
identify voting scenarios in which uncertainty about the voting method may
reduce or even eliminate a voter's incentive to misrepresent her preferences.
Thus, it may be in the interest of an election designer who wishes to reduce
strategic voting to leave voters uncertain about which of several reasonable
voting methods will be used to determine the winners of an election.",1907.09110v1,cs.MA,2019-07-22 03:17:19+00:00,"[arxiv.Result.Author('Wesley H. Holliday'), arxiv.Result.Author('Eric Pacuit')]","EPTCS 297, 2019, pp. 252-272"
418,Vote Delegation and Misbehavior,"We study vote delegation with ""well-behaving"" and ""misbehaving"" agents and
compare it with conventional voting. Typical examples for vote delegation are
validation or governance tasks on blockchains. There is a majority of
well-behaving agents, but they may abstain or delegate their vote to other
agents since voting is costly. Misbehaving agents always vote. We compare
conventional voting allowing for abstention with vote delegation. Preferences
of voters are private information and a positive outcome is achieved if
well-behaving agents win. We illustrate that vote delegation leads to quite
different outcomes than conventional voting with abstention. In particular, we
obtain three insights: First, if the number of misbehaving voters, denoted by f
, is high, both voting methods fail to deliver a positive outcome. Second, if f
takes an intermediate value, conventional voting delivers a positive outcome,
while vote delegation fails with probability one. Third, if f is low,
delegation delivers a positive outcome with higher probability than
conventional voting. Finally, our results characterize worst-case outcomes that
can happen in a liquid democracy.",2102.08823v2,cs.GT,2021-02-17 15:32:32+00:00,"[arxiv.Result.Author('Hans Gersbach'), arxiv.Result.Author('Akaki Mamageishvili'), arxiv.Result.Author('Manvir Schneider')]",
419,CNN waste classification project report,"This report is about waste management project. We used CNN as classifier to
classify waste image captured from mobile phone. Our model can identify 6 waste
classes with highly accurate and our model is successfully transferred into IOS
platform as application by swift. In addition, this report also introduced some
basic project management from planning project to landing project, for instance
using agile development to develop this waste app.",2212.11050v1,cs.CY,2022-12-21 14:49:48+00:00,"[arxiv.Result.Author('Fei Wu'), arxiv.Result.Author('LiQin Zhang'), arxiv.Result.Author('An Tran')]",
420,Plasma filtering techniques for nuclear waste remediation,"Nuclear waste cleanup is challenged by the handling of feed stocks that are
both unknown and complex. Plasma filtering, operating on dissociated elements,
offers advantages over chemical methods in processing such wastes. The costs
incurred by plasma mass filtering for nuclear waste pretreatment, before
ultimate disposal, are similar to those for chemical pretreatment. However,
significant savings might be achieved in minimizing the waste mass. This
advantage may be realized over a large range of chemical waste compositions,
thereby addressing the heterogeneity of legacy nuclear waste.",1504.07180v1,physics.plasm-ph,2015-04-27 17:50:24+00:00,"[arxiv.Result.Author('Renaud Gueroult'), arxiv.Result.Author('David T. Hobbs'), arxiv.Result.Author('Nathaniel J. Fisch')]",
421,Voting Power of Teams Working Together,"Voting power determines the ""power"" of individuals who cast votes; their
power is based on their ability to influence the winning-ness of a coalition.
Usually each individual acts alone, casting either all or none of their votes
and is equally likely to do either. This paper extends this standard ""random
voting"" model to allow probabilistic voting, partial voting, and correlated
team voting. We extend the standard Banzhaf metric to account for these cases;
our generalization reduces to the standard metric under ""random voting"", This
new paradigm allows us to answer questions such as ""In the 2013 US Senate, how
much more unified would the Republicans have to be in order to have the same
power as the Democrats in attaining cloture?""",1312.3394v1,cs.GT,2013-12-12 03:19:17+00:00,[arxiv.Result.Author('Daniel Zwillinger')],
422,Suppressing technical noises in weak measurement by entanglement,"Postselected weak measurement has aroused broad interest for its distinctive
ability to amplify small physical quantities. However, the low postselection
efficiency to obtain a large weak value has been a big obstacle to its
application in practice, since it may waste resources, and reduce the
measurement precision. An improved protocol was proposed in [Phys. Rev. Lett.
113, 030401 (2014)] to make the postselected weak measurement dramatically more
efficient by using entanglement. Such a protocol can increase the Fisher
information of the measurement to approximately saturate the well-known
Heisenberg limit. In this paper, we review the entanglement-assisted protocol
of postselected weak measurement in detail, and study its robustness against
technical noises. We focus on readout errors. Readout errors can greatly
degrade the performance of postselected weak measurement, especially when the
readout error probability is comparable to the postselection probability. We
show that entanglement can significantly reduce the two main detrimental
effects of readout errors: inaccuracy in the measurement result, and the loss
of Fisher information. We extend the protocol by introducing a majority vote
scheme to postselection to further compensate for readout errors. With a proper
threshold, almost no Fisher information will be lost. These results demonstrate
the effectiveness of entanglement in protecting postselected weak measurement
against readout errors.",1504.07718v2,quant-ph,2015-04-29 04:21:11+00:00,"[arxiv.Result.Author('Shengshi Pang'), arxiv.Result.Author('Todd A. Brun')]","Phys. Rev. A 92, 012120 (2015)"
423,The Computational Impact of Partial Votes on Strategic Voting,"In many real world elections, agents are not required to rank all candidates.
We study three of the most common methods used to modify voting rules to deal
with such partial votes. These methods modify scoring rules (like the Borda
count), elimination style rules (like single transferable vote) and rules based
on the tournament graph (like Copeland) respectively. We argue that with an
elimination style voting rule like single transferable vote, partial voting
does not change the situations where strategic voting is possible. However,
with scoring rules and rules based on the tournament graph, partial voting can
increase the situations where strategic voting is possible. As a consequence,
the computational complexity of computing a strategic vote can change. For
example, with Borda count, the complexity of computing a strategic vote can
decrease or stay the same depending on how we score partial votes.",1405.7714v1,cs.GT,2014-05-28 12:13:52+00:00,"[arxiv.Result.Author('Nina Narodytska'), arxiv.Result.Author('Toby Walsh')]",
424,FASTEN: Fair and Secure Distributed Voting Using Smart Contracts,"Electing democratic representatives via voting has been a common mechanism
since the 17th century. However, these mechanisms raise concerns about
fairness, privacy, vote concealment, fair calculations of tally, and proxies
voting on their behalf for the voters. Ballot voting, and in recent times,
electronic voting via electronic voting machines (EVMs) improves fairness by
relying on centralized trust. Homomorphic encryption-based voting protocols
also assure fairness but cannot scale to large scale elections such as
presidential elections. In this paper, we leverage the blockchain technology of
distributing trust to propose a smart contract-based protocol, namely, \proto.
There are many existing protocols for voting using smart contracts. We observe
that these either are not scalable or leak the vote tally during the voting
stage, i.e., do not provide vote concealment. In contrast, we show that FASTEN
preserves voter's privacy ensures vote concealment, immutability, and avoids
double voting. We prove that the probability of privacy breaches is negligibly
small. Further, our cost analysis of executing FASTEN over Ethereum is
comparable to most of the existing cost of elections.",2102.10594v1,cs.CR,2021-02-21 12:29:23+00:00,"[arxiv.Result.Author('Sankarshan Damle'), arxiv.Result.Author('Sujit Gujar'), arxiv.Result.Author('Moin Hussain Moti')]",
425,Comparative Analysis of Multiple Deep CNN Models for Waste Classification,"Waste is a wealth in a wrong place. Our research focuses on analyzing
possibilities for automatic waste sorting and collecting in such a way that
helps it for further recycling process. Various approaches are being practiced
managing waste but not efficient and require human intervention. The automatic
waste segregation would fit in to fill the gap. The project tested well known
Deep Learning Network architectures for waste classification with dataset
combined from own endeavors and Trash Net. The convolutional neural network is
used for image classification. The hardware built in the form of dustbin is
used to segregate those wastes into different compartments. Without the human
exercise in segregating those waste products, the study would save the precious
time and would introduce the automation in the area of waste management.
Municipal solid waste is a huge, renewable source of energy. The situation is
win-win for both government, society and industrialists. Because of fine-tuning
of the ResNet18 Network, the best validation accuracy was found to be 87.8%.",2004.02168v2,cs.CV,2020-04-05 11:50:27+00:00,"[arxiv.Result.Author('Dipesh Gyawali'), arxiv.Result.Author('Alok Regmi'), arxiv.Result.Author('Aatish Shakya'), arxiv.Result.Author('Ashish Gautam'), arxiv.Result.Author('Surendra Shrestha')]","5th International Conference on Advanced Engineering and
  ICT-Convergence 2020"
426,Kings and serfs in oriented graphs,"In this paper, we extend the concept of kings and serfs in tournaments to
that of weak kings and weak serfs in oriented graphs. We obtain various results
on the existence of weak kings(weak serfs) in oriented graphs, and show the
existence of n-oriented graphs containing exactly k weak kings(weak serfs).
Also, we give the existence of n-oriented graphs containing exactly k weak
kings and exactly s weak serfs such that b weak kings from k are also weak
serfs.",math/0609203v1,math.CO,2006-09-07 11:12:19+00:00,"[arxiv.Result.Author('S. Pirzada'), arxiv.Result.Author('N. A. Shah')]",
427,On the double crossed product of weak Hopf algebras,"Given a weak distributive law between algebras underlying two weak
bialgebras, we present sufficient conditions under which the corresponding weak
wreath product algebra becomes a weak bialgebra with respect to the tensor
product coalgebra structure. When the weak bialgebras are weak Hopf algebras,
then the same conditions are shown to imply that the weak wreath product
becomes a weak Hopf algebra, too. Our sufficient conditions are capable to
describe most known examples, (in particular the Drinfel'd double of a weak
Hopf algebra).",1205.2163v1,math.QA,2012-05-10 06:13:35+00:00,"[arxiv.Result.Author('Gabriella Böhm'), arxiv.Result.Author('José Gómez-Torrecillas')]","AMS Contemp. Math. 585 (2013), 153-174"
428,Remarks on Gorenstein weak injective and weak flat modules,"In this paper, we introduce the notions of Gorenstein weak injective and weak
flat modules respectively in terms of weak injective and weak flat modules,
which is larger than classical classes of Gorenstein injective and flat
modules. In this new setting, we characterize rings over which all modules are
Gorenstein weak injective. Moreover, we also discuss a relation between weak
cosyzygy and Gorenstein weak cosyzygy of a module, and the stability of
Gorenstein weak injective modules.",1405.0648v3,math.RA,2014-05-04 05:19:39+00:00,"[arxiv.Result.Author('Tiwei Zhao'), arxiv.Result.Author('Yunge Xu')]",
429,Non-statistical Weak Measurements,"Non-statistical weak measurements yield weak values that are outside the
range of eigenvalues and are not rare, suggesting that weak values are a
property of every pre-and-post-selected ensemble. They also extend the
applicability and valid regime of weak values.",quant-ph/0607208v1,quant-ph,2006-07-28 17:05:22+00:00,"[arxiv.Result.Author('Jeff Tollaksen'), arxiv.Result.Author('Yakir Aharonov')]",
430,A framework for measuring weak values without weak interactions and its diagrammatic representation,"Weak values are typically obtained experimentally by performing weak
measurements, which involve weak interactions between the measured system and a
probe. However, the determination of weak values does not necessarily require
weak measurements, and several methods without weak system-probe interactions
have been developed previously. In this work, a framework for measuring weak
values is proposed to describe the relationship between various weak
measurement techniques in a unified manner. This framework, which uses a
probe-controlled system transformation instead of the weak system-probe
interaction, improves the understanding of the currently used weak value
measurement methods. Furthermore, a diagrammatic representation of the proposed
framework is introduced to intuitively identify the complex values obtained in
each measurement system. By using this diagram, a new method for measuring weak
values with a desired function can be systematically derived. As an example, a
scan-free and more efficient direct measurement method of wavefunctions than
the conventional techniques using weak measurements is developed.",1809.10393v1,quant-ph,2018-09-27 08:18:53+00:00,"[arxiv.Result.Author('Kazuhisa Ogawa'), arxiv.Result.Author('Osamu Yasuhiko'), arxiv.Result.Author('Hirokazu Kobayashi'), arxiv.Result.Author('Toshihiro Nakanishi'), arxiv.Result.Author('Akihisa Tomita')]","New J. Phys. 21, 043013 (2019)"
431,Weak mixing properties of vector sequences,"Notions of weak and uniformly weak mixing (to zero) are defined for bounded
sequences in arbitrary Banach spaces. Uniformly weak mixing for vector
sequences is characterized by mean ergodic convergence properties. For bounded
sequences, which satisfy a certain domination condition, it is shown that weak
mixing to zero is equivalent with uniformly weak mixing to zero.",math/0506554v1,math.FA,2005-06-28 17:32:21+00:00,[arxiv.Result.Author('L. Zsido')],
432,Measurements of non local weak values,"Some recent attempts at measuring non local weak values via local
measurements are discussed and shown to be less robust than standard weak
measurements. A method for measuring some non local weak values via non local
measurements (non local weak measurements) is introduced. The meaning of non
local weak values is discussed.",0902.4251v1,quant-ph,2009-02-24 22:11:19+00:00,"[arxiv.Result.Author('Aharon Brodutch'), arxiv.Result.Author('Lev Vaidman')]",J. Phys.: Conf. Ser. 174 012004 (2009)
433,Weak value controversy,"Recent controversy regarding the meaning and usefulness of weak values is
reviewed. It is argued that in spite of recent statistical arguments by Ferrie
and Combes, experiments with anomalous weak values provide a useful
amplification techniques for precision measurements of small effects in many
realistic situations. The statistical nature of weak vales was questioned.
Although measuring weak value requires an ensemble, it is argued that the weak
value, similarly to an eigenvalue, is a property of a single pre- and
post-selected quantum system.",1703.08870v1,quant-ph,2017-03-26 20:51:25+00:00,[arxiv.Result.Author('Lev Vaidman')],
434,Weighted weak group inverse for Hilbert space operators,"We present the weighted weak group inverse, which is a new generalized
inverse of operators between two Hilbert spaces, introduced to extend weak
group inverse for square matrices. Some characterizations and representations
of the weighted weak group inverse are investigated. We also apply these
results to define and study the weak group inverse for a Hilbert space
operator. Using the weak group inverse, we define and characterize various
binary relations.",1903.00915v1,math.FA,2019-03-03 13:57:50+00:00,"[arxiv.Result.Author('Dijana Mosic'), arxiv.Result.Author('Daochang Zhang')]",
435,Classifying (Weak) Coideal Subalgebras of Weak Hopf C*-Algebras,"We develop a general approach to the problem of classification of weak
coideal C*-subalgebras of weak Hopf C*-algebras. As an example, we consider
weak Hopf C*-algebras and their weak coideal C*-subalgebras associated with
Tambara Yamagami categories.",1904.07602v1,math.QA,2019-04-16 11:30:04+00:00,"[arxiv.Result.Author('Leonid Vainerman'), arxiv.Result.Author('Jean-Michel Vallin')]",Journal of algebra 550(2020) 333-57
436,Weak topologies for unbounded nets in CAT($0$) spaces,"Weak topologies that yield weak convergence for bounded sequences and nets in
CAT($0$) spaces have been studied in the past. We are here concerned with weak
topologies that yield weak convergence of unbounded sequences and nets. We
analyze two such topologies that generalize the weak topology on Hilbert spaces
and that agree with the strong topology on a CAT($0$) space if and only if the
space is locally compact.",2204.01291v1,math.MG,2022-04-04 07:54:33+00:00,"[arxiv.Result.Author('Philip Miller'), arxiv.Result.Author('Arian Berdellima'), arxiv.Result.Author('Max Wardetzky')]",
437,Studies on the Weak Charges of Particles,"In order to include massive neutrino theoretically, a new concept of the weak
charges of the particles, fundamental fermions, intermediate bosons and
hadrons, is introduced. A new conservation law of the weak charge is first
reported. According to the chirality of the weak charge the origin of parity
nonconservation in the weak interactions is reasonably explained. According to
the symmetry of the weak charge, an extension of the standard model is
proposed. In this scenario, all the three generations of neutrinos are massive
Dirac particles; both the right-handed neutrinos and the right-handed neutral
baryons have zero weak charge, and so they do not take part in the weak
interactions.",hep-ph/0201269v1,hep-ph,2002-01-29 15:38:38+00:00,[arxiv.Result.Author('Zhi-qiang Shi')],
438,Weak Finsler Strutures and the Funk Metric,"We discuss general notions of metrics and of Finsler structures which we call
weak metrics and weak Finsler structures. Any convex domain carries a canonical
weak Finsler structure, which we call its tautological weak Finsler structure.
We compute distances in the tautological weak Finsler structure of a domain and
we show that these are given by the so-called Funk weak metric. We conclude the
paper with a discussion of geodesics, of metric balls and of convexity
properties of the Funk weak metric.",0804.0705v1,math.DG,2008-04-04 12:06:19+00:00,"[arxiv.Result.Author('Athanase Papadopoulos'), arxiv.Result.Author('Marc Troyanov')]",
439,Kaplansky's Construction Type and Classification of Weak bialgebras and Weak Hopf algebras,"In this paper, we study weak bialgebras and weak Hopf algebras. These
algebras form a class wider than bialgebras respectively Hopf algebras. The
main results of this paper are Kaplansky's constructions type which lead to
weak bialgebras or weak Hopf algebras starting from a regular algebra or a
bialgebra. Also we provide a classification of 2-dimensional and 3-dimensional
weak bialgebras and weak Hopf algebras. We determine then the stabilizer group
and the representative of these classes, the action being that of the linear
group.",1001.2194v1,math.RA,2010-01-13 15:04:29+00:00,"[arxiv.Result.Author('Zoheir Chebel'), arxiv.Result.Author('Abdenacer Makhlouf')]",
440,Cohomological properties of different types of weak amenability,"In this paper, we deal with cohomological properties of weak amenability,
cyclic amenability, cyclic weak amenability and point amenability of Banach
algebras. We look at some hereditary properties of them and show that
continuous homomorphisms with dense range preserve cyclically weak amenability,
however, weak amenability and cyclically amenability are preserved under
certain conditions. We also study these cohomological properties of the
$\theta-$Lau product $A\times_\theta B$ and the projective tensor product
$A\hat{\otimes} B$. Finally, we investigate the cohomological properties of
$A^{**}$ and establish that cyclically weak amenability of $A^{**}$ implies
cyclically weak amenability of $A$. This result is true for point amenability
instead of cyclically weak amenability.",2210.03708v1,math.FA,2022-10-07 17:22:57+00:00,"[arxiv.Result.Author('M. J. Mehdipour'), arxiv.Result.Author('A. Rejali')]",
441,Weak type radial convolution operators on free group,"Radial convolution operators on free groups with nonnegative kernel of weak
type $(2,2)$ and of restricted weak type $(2,2)$ are characterized. Estimates
of weak type $(p,p)$ are obtained as well for $1<p<2.$",0704.1859v1,math.FA,2007-04-14 10:53:15+00:00,"[arxiv.Result.Author('T. Pytlik'), arxiv.Result.Author('R. Szwarc')]",
442,Weak Value and Weak Measurements,"The weak value of a variable O is a description of an effective interaction
with that variable in the limit of weak coupling. It is particularly important
for a pre- and post-selected quantum system.",0706.1348v1,quant-ph,2007-06-10 10:53:50+00:00,[arxiv.Result.Author('Lev Vaidman')],
443,Weak* continuous states on Banach algebras,"We prove that if a unital Banach algebra $A$ is the dual of a Banach space
$\pd{A}$, then the set of weak* continuous states is weak* dense in the set of
all states on $A$. Further, weak* continuous states linearly span $\pd{A}$.",0808.2037v1,math.FA,2008-08-14 17:48:14+00:00,[arxiv.Result.Author('Bojan Magajna')],
444,Theta liftings on weak Maass forms,"We construct theta liftings from half-integral weight weak Maass forms to
even integral weight weak Maass forms by using regularized theta integral.
Moreover it gives an extension of Niwa's theta liftings on harmonic weak Maass
forms. And we obtain the similar results to those by Niwa.",1101.3062v1,math.NT,2011-01-16 11:26:49+00:00,"[arxiv.Result.Author('YoungJu Choie'), arxiv.Result.Author('Subong Lim')]",
445,Excited weak bosons and their decays,"The weak bosons are bound states of fermions. Here the excitations of the
weak bosons are discussed. Especially we study the decays of these excited
states into weak bosons and photons.",1610.07709v2,hep-ph,2016-10-25 02:28:23+00:00,[arxiv.Result.Author('Harald Fritzsch')],
446,He white dwarfs with large H contamination: Convective mixing or accretion?,"White dwarfs are compact objects with atmospheres containing mainly light
elements, hydrogen or helium. Because of their surface high gravitational
field, heavy elements diffuse downwards in a very short timescale compared to
the evolutionary timescale, leaving the lightest ones on the top of the
envelope. This results in the main classification of white dwarfs as hydrogen
rich or helium rich. But many helium rich white dwarfs show also the presence
of hydrogen traces in their atmosphere, whose origin is still unsettled. Here
we study, by means of full evolutionary calculations, the case for a
representative model of the ""He-H-Z"" white dwarfs, a sub-group of helium rich
white dwarfs showing both heavy elements and a large amount of hydrogen in
their atmosphere. We find it impossible to explain its hydrogen atmospheric
content by the convective mixing of a primordial hydrogen present in the star.
We conclude that the most likely explanation is the accretion of hydrogen rich
material, presumably water-bearing, coming from a debris disk.",1812.09405v1,astro-ph.SR,2018-12-21 23:11:42+00:00,"[arxiv.Result.Author('F. C. Wachlin'), arxiv.Result.Author('G. Vauclair'), arxiv.Result.Author('S. Vauclair')]",
447,Limiting Risk by Turning Manifest Phantoms into Evil Zombies,"Drawing a random sample of ballots to conduct a risk-limiting audit generally
requires knowing how the ballots cast in an election are organized into groups,
for instance, how many containers of ballots there are in all and how many
ballots are in each container. A list of the ballot group identifiers along
with number of ballots in each group is called a ballot manifest. What if the
ballot manifest is not accurate? Surprisingly, even if ballots are known to be
missing from the manifest, it is not necessary to make worst-case assumptions
about those ballots--for instance, to adjust the margin by the number of
missing ballots--to ensure that the audit remains conservative. Rather, it
suffices to make worst-case assumptions about the individual randomly selected
ballots that the audit cannot find. This observation provides a simple
modification to some risk-limiting audit procedures that makes them
automatically become more conservative if the ballot manifest has errors. The
modification--phantoms to evil zombies (~2EZ)--requires only an upper bound on
the total number of ballots cast. ~2EZ makes the audit P-value stochastically
larger than it would be had the manifest been accurate, automatically requiring
more than enough ballots to be audited to offset the manifest errors. This
ensures that the true risk limit remains smaller than the nominal risk limit.
On the other hand, if the manifest is in fact accurate and the upper bound on
the total number of ballots equals the total according to the manifest, ~2EZ
has no effect at all on the number of ballots audited nor on the true risk
limit.",1207.3413v1,stat.AP,2012-07-14 11:26:02+00:00,"[arxiv.Result.Author('Jorge H. Banuelos'), arxiv.Result.Author('Philip B. Stark')]",
448,A Complete Enumeration of Ballot Permutations Avoiding Sets of Small Patterns,"Permutations whose prefixes contain at least as many ascents as descents are
called ballot permutations. Lin, Wang, and Zhao have previously enumerated
ballot permutations avoiding small patterns and have proposed the problem of
enumerating ballot permutations avoiding a pair of permutations of length $3$.
We completely enumerate ballot permutations avoiding two patterns of length $3$
and we relate these avoidance classes with their respective recurrence
relations and formulas, which leads to an interesting bijection between ballot
permutations avoiding $132$ and $312$ with left factors of Dyck paths. In
addition, we also conclude the Wilf-classification of ballot permutations
avoiding sets of two patterns of length $3$, and we then extend our results to
completely enumerate ballot permutations avoiding three patterns of length $3$.",2209.06087v1,math.CO,2022-09-13 15:37:45+00:00,[arxiv.Result.Author('Nathan Sun')],
449,Can Voters Detect Errors on Their Printed Ballots? Absolutely,"There is still debate on whether voters can detect malicious changes in their
printed ballot after making their selections on a Ballot Marking Device (BMD).
In this study, we altered votes on a voter's ballot after they had made their
selections on a BMD. We then required them to examine their ballots for any
changes from the slate they used to vote. Overall accuracy was exceptionally
high. Participants saw 1440 total contests, and of those 1440, there were a
total of 4 errors, so total accuracy was 99.8%. Participants were able to
perform with near-perfect accuracy regardless of ballot length, ballot type,
number of altered races, and location of altered races. Detection performance
was extremely robust. We conclude that with proper direction and resources,
voters can be near-perfect detectors of ballot changes on printed paper ballots
after voting with a BMD. This finding has significant implications for the
voting community as BMD use continues to grow. Research should now focus on
identifying administrative and behavioral methods that will prompt and
encourage voters to check their BMD-generated ballots before they drop them in
the ballot box.",2204.09780v1,cs.HC,2022-04-20 20:40:32+00:00,"[arxiv.Result.Author('Philip Kortum'), arxiv.Result.Author('Michael D. Byrne'), arxiv.Result.Author('Chidera O. Azubike'), arxiv.Result.Author('Laura E. Roty')]",
450,Ballot tilings and increasing trees,"We study enumerations of Dyck and ballot tilings, which are tilings of a
region determined by two Dyck or ballot paths. We give bijective proofs to two
formulae of enumerations of Dyck tilings through Hermite histories. We show
that one of the formulae is equal to a certain Kazhdan--Lusztig polynomial. For
a ballot tiling, we establish formulae which are analogues of formulae for Dyck
tilings. Especially, the generating functions have factorized expressions. The
key tool is a planted plane tree and its increasing labellings. We also
introduce a generalized perfect matching which is bijective to an Hermite
history for a ballot tiling. By combining these objects, we obtain various
expressions of a generating function of ballot tilings with a fixed lower path.",1705.06434v1,math-ph,2017-05-18 06:46:09+00:00,[arxiv.Result.Author('Keiichi Shigechi')],
451,Sophisticated Attacks on Decoy Ballots: The Devil's Menu and the Market for Lemons,"Decoy ballots do not count in election outcomes, but otherwise they are
indistinguishable from real ballots. By means of a game-theoretical model, we
show that decoy ballots may not provide effective protection against a
malevolent adversary trying to buy real ballots. If the citizenry is divided
into subgroups (or districts), the adversary can construct a so-called ""Devil's
Menu"" consisting of several prices. In equilibrium, the adversary can buy the
real ballots of any strict subset of districts at a price corresponding to the
willingness to sell on the part of the citizens holding such ballots. By
contrast, decoy voters are trapped into selling their ballots at a low, or even
negligible, price. Blowing up the adversary's budget by introducing decoy
ballots may thus turn out to be futile. The Devil's Menu can also be applied to
the well-known ""Lemons Problem"".",1712.05477v1,cs.GT,2017-12-14 23:54:58+00:00,"[arxiv.Result.Author('Hans Gersbach'), arxiv.Result.Author('Akaki Mamageishvili'), arxiv.Result.Author('Oriol Tejada')]",
452,The ATHENA Class of Risk-Limiting Ballot Polling Audits,"The main risk-limiting ballot polling audit in use today, BRAVO, is designed
for use when single ballots are drawn at random and a decision regarding
whether to stop the audit or draw another ballot is taken after each ballot
draw (ballot-by-ballot (B2) audits). On the other hand, real ballot polling
audits draw many ballots in a single round before determining whether to stop
(round-by-round (R2) audits). We show that BRAVO results in significant
inefficiency when directly applied to real R2 audits. We present the ATHENA
class of R2 stopping rules, which we show are risk-limiting if the round
schedule is pre-determined (before the audit begins). We prove that each rule
is at least as efficient as the corresponding BRAVO stopping rule applied at
the end of the round. We have open-source software libraries implementing most
of our results.
  We show that ATHENA halves the number of ballots required, for all state
margins in the 2016 US Presidential election and a first round with $90\%$
stopping probability, when compared to BRAVO (stopping rule applied at the end
of the round). We present simulation results supporting the 90% stopping
probability claims and our claims for the risk accrued in the first round.
Further, ATHENA reduces the number of ballots by more than a quarter for low
margins, when compared to the BRAVO stopping rule applied on ballots in
selection order. This implies that keeping track of the order when drawing
ballots R2 is not beneficial, because ATHENA is more efficient even without
information on selection order. These results are significant because current
approaches to real ballot polling election audits use the B2 BRAVO rules,
requiring about twice as much work on the part of election officials. Applying
the rules in selection order requires fewer ballots, but keeping track of the
order, and entering it into audit software, adds to the effort.",2008.02315v5,cs.CR,2020-08-05 18:47:37+00:00,"[arxiv.Result.Author('Filip Zagórski'), arxiv.Result.Author('Grant McClearn'), arxiv.Result.Author('Sarah Morin'), arxiv.Result.Author('Neal McBurnett'), arxiv.Result.Author('Poorvi L. Vora')]",
453,$k$-Cut: A Simple Approximately-Uniform Method for Sampling Ballots in Post-Election Audits,"We present an approximate sampling framework and discuss how risk-limiting
audits can compensate for these approximations, while maintaining their
""risk-limiting"" properties. Our framework is general and can compensate for
counting mistakes made during audits.
  Moreover, we present and analyze a simple approximate sampling
method,""$k$-cut"", for picking a ballot randomly from a stack, without counting.
Our method involves doing $k$ ""cuts"", each involving moving a random portion of
ballots from the top to the bottom of the stack, and then picking the ballot on
top. Unlike conventional methods of picking a ballot at random, $k$-cut does
not require identification numbers on the ballots or counting many ballots per
draw. We analyze how close the distribution of chosen ballots is to the uniform
distribution, and design different mitigation procedures. We show that $k=6$
cuts is enough for an risk-limiting election audit, based on empirical data,
which would provide a significant increase in efficiency.",1811.08811v3,cs.DS,2018-11-21 16:26:33+00:00,"[arxiv.Result.Author('Mayuri Sridhar'), arxiv.Result.Author('Ronald L. Rivest')]",
454,Bernoulli Ballot Polling: A Manifest Improvement for Risk-Limiting Audits,"We present a method and software for ballot-polling risk-limiting audits
(RLAs) based on Bernoulli sampling: ballots are included in the sample with
probability $p$, independently. Bernoulli sampling has several advantages: (1)
it does not require a ballot manifest; (2) it can be conducted independently at
different locations, rather than requiring a central authority to select the
sample from the whole population of cast ballots or requiring stratified
sampling; (3) it can start in polling places on election night, before margins
are known. If the reported margins for the 2016 U.S. Presidential election are
correct, a Bernoulli ballot-polling audit with a risk limit of 5% and a
sampling rate of $p_0 = 1\%$ would have had at least a 99% probability of
confirming the outcome in 42 states. (The other states were more likely to have
needed to examine additional ballots.) Logistical and security advantages that
auditing in the polling place affords may outweigh the cost of examining more
ballots than some other methods might require.",1812.06361v1,stat.AP,2018-12-15 21:57:23+00:00,"[arxiv.Result.Author('Kellie Ottoboni'), arxiv.Result.Author('Matthew Bernhard'), arxiv.Result.Author('J. Alex Halderman'), arxiv.Result.Author('Ronald L. Rivest'), arxiv.Result.Author('Philip B. Stark')]",
455,"Symmetric Dyck tilings, ballot tableaux and tree-like tableaux of shifted shapes","Symmetric Dyck tilings and ballot tilings are certain tilings in the region
surrounded by two ballot paths. We study the relations of combinatorial objects
which are bijective to symmetric Dyck tilings such as labeled trees, Hermite
histories, and perfect matchings. We also introduce two operations on labeled
trees for symmetric Dyck tilings: symmetric Dyck tiling strip (symDTS) and
symmetric Dyck tiling ribbon (symDTR). We give two definitions of Hermite
histories for symmetric Dyck tilings, and show that they are equivalent by use
of the correspondence between symDTS operation and an Hermite history. Since
ballot tilings form a subset in the set of symmetric Dyck tilings, we construct
an inclusive map from labeled trees for ballot tilings to labeled trees for
symmetric Dyck tilings. By this inclusive map, the results for symmetric Dyck
tilings can be applied to those of ballot tilings. We introduce and study the
notions of ballot tableaux and tree-like tableaux of shifted shapes, which are
generalizations of Dyck tableaux and tree-like tableaux, respectively. The
correspondence between ballot tableaux and tree-like tableaux of shifted shapes
is given by using the symDTR operation and the structure of labeled trees for
symmetric Dyck tilings.",2011.07296v1,math.CO,2020-11-14 13:15:14+00:00,[arxiv.Result.Author('Keiichi Shigechi')],
456,SOBA: Secrecy-preserving Observable Ballot-level Audit,"SOBA is an approach to election verification that provides observers with
justifiably high confidence that the reported results of an election are
consistent with an audit trail (""ballots""), which can be paper or electronic.
SOBA combines three ideas: (1) publishing cast vote records (CVRs) separately
for each contest, so that anyone can verify that each reported contest outcome
is correct, if the CVRs reflect voters' intentions with sufficient accuracy;
(2) shrouding a mapping between ballots and the CVRs for those ballots to
prevent the loss of privacy that could occur otherwise; (3) assessing the
accuracy with which the CVRs reflect voters' intentions for a collection of
contests while simultaneously assessing the integrity of the shrouded mapping
between ballots and CVRs by comparing randomly selected ballots to the CVRs
that purport to represent them. Step (1) is related to work by the Humboldt
County Election Transparency Project, but publishing CVRs separately for
individual contests rather than images of entire ballots preserves privacy.
Step (2) requires a cryptographic commitment from elections officials.
Observers participate in step (3), which relies on the ""super-simple
simultaneous single-ballot risk-limiting audit."" Step (3) is designed to reveal
relatively few ballots if the shrouded mapping is proper and the CVRs
accurately reflect voter intent. But if the reported outcomes of the contests
differ from the outcomes that a full hand count would show, step (3) is
guaranteed to have a large chance of requiring all the ballots to be counted by
hand, thereby limiting the risk that an incorrect outcome will become official
and final.",1105.5803v2,stat.AP,2011-05-29 17:00:20+00:00,"[arxiv.Result.Author('Josh Benaloh'), arxiv.Result.Author('Douglas Jones'), arxiv.Result.Author('Eric Lazarus'), arxiv.Result.Author('Mark Lindeman'), arxiv.Result.Author('Philip B. Stark')]",
457,"Voter Verification of BMD Ballots Is a Two-Part Question: Can They? Mostly, They Can. Do They? Mostly, They Don't","The question of whether or not voters actually verify ballots produced by
ballot marking devices (BMDs) is presently the subject of some controversy.
Recent studies (e.g., Bernhard, et al. 2020) suggest the verification rate is
low. What is not clear from previous research is whether this is more a result
of voters being unable to do so accurately or whether this is because voters
simply choose not to attempt verification in the first place. In order to
understand this problem, we conducted an experiment in which 108 participants
participated in a mock election where the BMD displayed the voters' true
choices, but then changed a subset of those choices on the printed ballot. The
design of the printed ballot, the length of the ballot, the number of changes
that were made to the ballot, the location of those changes, and the
instructions provided to the voters were manipulated as part of the experiment.
Results indicated that of those voters who chose to examine the printed ballot,
76% detected anomalies, indicating that voters can reliably detect errors on
their ballot if they will simply review it. This suggests that administrative
remedies, rather than attempts to alter fundamental human perceptual
capabilities, could be employed to encourage voters to check their ballots,
which could prove as an effective countermeasure.",2003.04997v1,cs.HC,2020-03-10 21:06:42+00:00,"[arxiv.Result.Author('Philip Kortum'), arxiv.Result.Author('Michael D. Byrne'), arxiv.Result.Author('Julie Whitmore')]",
458,Next Steps for the Colorado Risk-Limiting Audit (CORLA) Program,"Colorado conducted risk-limiting tabulation audits (RLAs) across the state in
2017, including both ballot-level comparison audits and ballot-polling audits.
Those audits only covered contests restricted to a single county; methods to
efficiently audit contests that cross county boundaries and combine ballot
polling and ballot-level comparisons have not been available.
  Colorado's current audit software (RLATool) needs to be improved to audit
these contests that cross county lines and to audit small contests efficiently.
  This paper addresses these needs. It presents extremely simple but
inefficient methods, more efficient methods that combine ballot polling and
ballot-level comparisons using stratified samples, and methods that combine
ballot-level comparison and variable-size batch comparison audits in a way that
does not require stratified sampling.
  We conclude with some recommendations, and illustrate our recommended method
using examples that compare them to existing approaches. Exemplar open-source
code and interactive Jupyter notebooks are provided that implement the methods
and allow further exploration.",1803.00698v1,stat.AP,2018-03-02 03:46:37+00:00,"[arxiv.Result.Author('Mark Lindeman'), arxiv.Result.Author('Neal McBurnett'), arxiv.Result.Author('Kellie Ottoboni'), arxiv.Result.Author('Philip B. Stark')]",
459,RAIRE: Risk-Limiting Audits for IRV Elections,"Risk-limiting post election audits guarantee a high probability of correcting
incorrect election results, independent of why the result was incorrect.
Ballot-polling audits select ballots at random and interpret those ballots as
evidence for and against the reported result, continuing this process until
either they support the recorded result, or they fall back to a full manual
recount. For elections with digitised scanning and counting of ballots, a
comparison audit compares randomly selected digital ballots with their paper
versions. Discrepancies are referred to as errors, and are used to build
evidence against or in support of the recorded result. Risk-limiting audits for
first-past-the-post elections are well understood, and used in some US
elections. We define a number of approaches to ballot-polling and comparison
risk-limiting audits for Instant Runoff Voting (IRV) elections. We show that
for almost all real elections we found, we can perform a risk-limiting audit by
looking at only a small fraction of the total ballots (assuming no errors were
made in the tallying and distribution of votes).",1903.08804v2,cs.DS,2019-03-20 06:19:10+00:00,"[arxiv.Result.Author('Michelle Blom'), arxiv.Result.Author('Peter J. Stuckey'), arxiv.Result.Author('Vanessa Teague')]",
460,"A decomposition of ballot permutations, pattern avoidance and Gessel walks","A permutation whose any prefix has no more descents than ascents is called a
ballot permutation. In this paper, we present a decomposition of ballot
permutations that enables us to construct a bijection between ballot
permutations and odd order permutations, which proves a set-valued extension of
a conjecture due to Spiro using the statistic of peak values. This bijection
also preserves the neighbors of the largest letter in permutations and thus
resolves a refinement of Spiro' s conjecture proposed by Wang and Zhang. Our
decomposition can be extended to well-labelled positive paths, a class of
generalized ballot permutations arising from polytope theory, that were
enumerated by Bernardi, Duplantier and Nadeau. We will also investigate the
enumerative aspect of ballot permutations avoiding a single pattern of length 3
and establish a connection between 213-avoiding ballot permutations and Gessel
walks.",2103.04599v1,math.CO,2021-03-08 08:37:08+00:00,"[arxiv.Result.Author('Zhicong Lin'), arxiv.Result.Author('David G. L. Wang'), arxiv.Result.Author('Tongyuan Zhao')]",
461,Ballot Length in Instant Runoff Voting,"Instant runoff voting (IRV) is an increasingly-popular alternative to
traditional plurality voting in which voters submit rankings over the
candidates rather than single votes. In practice, elections using IRV often
restrict the ballot length, the number of candidates a voter is allowed to rank
on their ballot. We theoretically and empirically analyze how ballot length can
influence the outcome of an election, given fixed voter preferences. We show
that there exist preference profiles over $k$ candidates such that up to $k-1$
different candidates win at different ballot lengths. We derive exact lower
bounds on the number of voters required for such profiles and provide a
construction matching the lower bound for unrestricted voter preferences.
Additionally, we characterize which sequences of winners are possible over
ballot lengths and provide explicit profile constructions achieving any
feasible winner sequence. We also examine how classic preference restrictions
influence our results--for instance, single-peakedness makes $k-1$ different
winners impossible but still allows at least $\Omega(\sqrt k)$. Finally, we
analyze a collection of 168 real-world elections, where we truncate rankings to
simulate shorter ballots. We find that shorter ballots could have changed the
outcome in one quarter of these elections. Our results highlight ballot length
as a consequential degree of freedom in the design of IRV elections.",2207.08958v3,cs.MA,2022-07-18 22:01:13+00:00,"[arxiv.Result.Author('Kiran Tomlinson'), arxiv.Result.Author('Johan Ugander'), arxiv.Result.Author('Jon Kleinberg')]",
462,A modular eballot system - V0.6,"We consider a reasonably simple voting system which can be implemented for
web-based ballots. Simplicity, modularity and the requirement of compatibility
with current web browsers leads to a system which satisfies a set of security
requirements for a ballot system which is not complete but sufficient in many
cases. Due to weak-eligibility and vote-selling, this system cannot be used for
political or similar ballots.",cs/0611066v2,cs.CR,2006-11-15 09:33:58+00:00,[arxiv.Result.Author('Andrea Pasquinucci')],
463,A quantum secret ballot,"The paper concerns the protection of the secrecy of ballots, so that the
identity of the voters cannot be matched with their vote. To achieve this we
use an entangled quantum state to represent the ballots. Each ballot includes
the identity of the voter, explicitly marked on the ""envelope"" containing it.
Measuring the content of the envelope yields a random number which reveals no
information about the vote. However, the outcome of the elections can be
unambiguously decided after adding the random numbers from all envelopes. We
consider a few versions of the protocol and their complexity of implementation.",quant-ph/0602087v2,quant-ph,2006-02-09 21:03:35+00:00,"[arxiv.Result.Author('Shahar Dolev'), arxiv.Result.Author('Itamar Pitowsky'), arxiv.Result.Author('Boaz Tamir')]",
464,Ballot Paths Avoiding Depth Zero Patterns,"In a paper by Sapounakis, Tasoulas, and Tsikouras \cite{stt}, the authors
count the number of occurrences of patterns of length four in Dyck paths. In
this paper we specify in one direction and generalize in another. We only count
ballot paths that avoid a given pattern, where a ballot path stays weakly above
the diagonal $y=x$, starts at the origin, and takes steps from the set
$\{\uparrow ,\to \}=\{u,r\}$. A pattern is a finite string made from the same
step set; it is also a path. Notice that a ballot path ending at a point along
the diagonal is a Dyck path.",1004.2710v1,math.CO,2010-04-15 20:23:45+00:00,"[arxiv.Result.Author('Heinrich Niederhausen'), arxiv.Result.Author('Shaun Sullivan')]","Journal OF Combinatorial Mathematics and Combinatorial Computing,
  August 2010"
465,Exploiting re-voting in the Helios election system,"Election systems must ensure that representatives are chosen by voters.
Moreover, each voter should have equal influence. Traditionally, this has been
achieved by permitting voters to cast at most one ballot. More recently, this
has been achieved by tallying the last ballot cast by each voter. We show this
is not achieved by the Helios election system, because an adversary can cause a
ballot other than a voter's last to be tallied. Moreover, we show how the
adversary can choose the contents of such a ballot, thus the adversary can
unduly influence the selection of representatives.",1612.04099v3,cs.CR,2016-12-13 11:07:22+00:00,"[arxiv.Result.Author('Maxime Meyer'), arxiv.Result.Author('Ben Smyth')]",
466,Applying Dynkin's isomorphism: An alternative approach to understand the Markov property of the de Wijs process,"Dynkin's (Bull. Amer. Math. Soc. 3 (1980) 975-999) seminal work associates a
multidimensional transient symmetric Markov process with a multidimensional
Gaussian random field. This association, known as Dynkin's isomorphism, has
profoundly influenced the studies of Markov properties of generalized Gaussian
random fields. Extending Dykin's isomorphism, we study here a particular
generalized Gaussian Markov random field, namely, the de Wijs process that
originated in Georges Matheron's pioneering work on mining geostatistics and,
following McCullagh (Ann. Statist. 30 (2002) 1225-1310), is now receiving
renewed attention in spatial statistics. This extension of Dynkin's theory
associates the de Wijs process with the (recurrent) Brownian motion on the two
dimensional plane, grants us further insight into Matheron's kriging formula
for the de Wijs process and highlights previously unexplored relationships of
the central Markov models in spatial statistics with Markov processes on the
plane.",1507.07357v1,math.ST,2015-07-27 10:38:23+00:00,[arxiv.Result.Author('Debashis Mondal')],"Bernoulli 2015, Vol. 21, No. 3, 1289-1303"
467,Anisotropy of the Mobility of Pentacene from Frustration,"The bandstructure of pentacene is calculated using first-principles density
functional theory. A large anisotropy of the hole and electron effective masses
within the molecular planes is found. The band dispersion of the HOMO and the
LUMO is analyzed with the help of a tight-binding fit. The anisotropy is shown
to be intimately related to the herringbone structure.",cond-mat/0301078v1,cond-mat.mtrl-sci,2003-01-07 18:27:32+00:00,"[arxiv.Result.Author('Gilles A. de Wijs'), arxiv.Result.Author('Christine C. Mattheus'), arxiv.Result.Author('Robert A. de Groot'), arxiv.Result.Author('Thomas T. M. Palstra')]",
468,NMR shieldings from density functional perturbation theory: GIPAW versus all-electron calculations,"We present a benchmark of the density functional linear response calculation
of NMR shieldings within the Gauge-Including Projector-Augmented-Wave method
against all-electron Augmented-Plane-Wave$+$local-orbital and uncontracted
Gaussian basis set results for NMR shieldings in molecular and solid state
systems. In general, excellent agreement between the aforementioned methods is
obtained. Scalar relativistic effects are shown to be quite large for nuclei in
molecules in the deshielded limit. The small component makes up a substantial
part of the relativistic corrections.",1609.03770v1,cond-mat.mtrl-sci,2016-09-13 11:28:12+00:00,"[arxiv.Result.Author('G. A. de Wijs'), arxiv.Result.Author('R. Laskowski'), arxiv.Result.Author('P. Blaha'), arxiv.Result.Author('R. W. A. Havenith'), arxiv.Result.Author('G. Kresse'), arxiv.Result.Author('M. Marsman')]",
469,Band offsets at the crystalline/amorphous silicon interface from first-principles,"The band offsets between crystalline and hydrogenated amorphous silicon
(a-Si:H) are key parameters governing the charge transport in modern silicon
hetrojunction solar cells. They are an important input for macroscopic
simulators that are used to further optimize the solar cell. Past experimental
studies, using X-ray photoelectron spectroscopy (XPS) and capacitance-voltage
measurements, have yielded conflicting results on the band offset. Here we
present a computational study on the band offsets. It is based on atomistic
models and density-functional theory (DFT). The amorphous part of the interface
is obtained by relatively long DFT first-principles molecular-dynamics (MD)
runs at an elevated temperature on 30 statistically independent samples. In
order to obtain a realistic conduction band position the electronic structure
of the interface is calculated with a hybrid functional. We find a slight
asymmetry in the band offsets, where the offset in the valence band (0.30 eV)
is larger than in the conduction band (0.17 eV). Our results are in agreement
with the latest XPS measurements that report a valence band offset of 0.3 eV
[M. Liebhaber et al., Appl. Phys. Lett. 106, 031601 (2015)].",1610.04119v1,cond-mat.mtrl-sci,2016-10-13 15:11:39+00:00,"[arxiv.Result.Author('K. Jarolimek'), arxiv.Result.Author('E. Hazrati'), arxiv.Result.Author('R. A. de Groot'), arxiv.Result.Author('G. A. de Wijs')]","Phys. Rev. Applied 8, 014026 (2017)"
470,Low work function of the (1000) Ca2N surface,"Polymer diodes require cathodes that do not corrode the polymer but do have
low work function to minimize the electron injection barrier. First-principles
calculations demonstrate that the work function of the (1000) surface of the
compound Ca2N is half an eV lower than that of the elemental metal Ca (2.35 vs.
2.87 eV). Moreover its reactivity is expected to be smaller. This makes Ca2N an
interesting candidate to replace calcium as cathode material for polymer light
emitting diode devices.",cond-mat/0311130v3,cond-mat.mtrl-sci,2003-11-06 15:39:06+00:00,"[arxiv.Result.Author('M. A. Uijttewaal'), arxiv.Result.Author('G. A. de Wijs'), arxiv.Result.Author('R. A. de Groot')]",
471,Phonons and electron-phonon coupling in graphene-h-BN heterostructures,"First principle calculations of the phonons of graphene-BN heterostructures
are presented and compared to those of the constituents. We show that AA and
AB' stacking are not only energetically less favoured than AB but also
dynamically unstable. We have identified low energy flat phonon branches of BN
character with out of plane displacement and evaluated their coupling to
electrons in graphene.",1407.6642v1,cond-mat.mes-hall,2014-07-24 16:29:37+00:00,"[arxiv.Result.Author('Guus J. Slotman'), arxiv.Result.Author('Gilles A. de Wijs'), arxiv.Result.Author('Annalisa Fasolino'), arxiv.Result.Author('Mikhail I. Katsnelson')]",
472,Term Rewriting on GPUs,"We present a way to implement term rewriting on a GPU. We do this by letting
the GPU repeatedly perform a massively parallel evaluation of all subterms. We
find that if the term rewrite systems exhibit sufficient internal parallelism,
GPU rewriting substantially outperforms the CPU. Since we expect that our
implementation can be further optimized, and because in any case GPUs will
become much more powerful in the future, this suggests that GPUs are an
interesting platform for term rewriting. As term rewriting can be viewed as a
universal programming language, this also opens a route towards programming
GPUs by term rewriting, especially for irregular computations.",2009.07174v1,cs.DC,2020-09-15 15:23:25+00:00,"[arxiv.Result.Author('Johri van Eerd'), arxiv.Result.Author('Jan Friso Groote'), arxiv.Result.Author('Pieter Hijma'), arxiv.Result.Author('Jan Martens'), arxiv.Result.Author('Anton Wijs')]",
473,An O(m log n) Algorithm for Stuttering Equivalence and Branching Bisimulation,"We provide a new algorithm to determine stuttering equivalence with time
complexity $O(m \log n)$, where $n$ is the number of states and $m$ is the
number of transitions of a Kripke structure. This algorithm can also be used to
determine branching bisimulation in $O(m(\log |\mathit{Act}|+ \log n))$ time
where $\mathit{Act}$ is the set of actions in a labelled transition system.
Theoretically, our algorithm substantially improves upon existing algorithms
which all have time complexity $O(m n)$ at best. Moreover, it has better or
equal space complexity. Practical results confirm these findings showing that
our algorithm can outperform existing algorithms with orders of magnitude,
especially when the sizes of the Kripke structures are large. The importance of
our algorithm stretches far beyond stuttering equivalence and branching
bisimulation. The known $O(m n)$ algorithms were already far more efficient
(both in space and time) than most other algorithms to determine behavioural
equivalences (including weak bisimulation) and therefore it was often used as
an essential preprocessing step. This new algorithm makes this use of
stuttering equivalence and branching bisimulation even more attractive.",1601.01478v1,cs.LO,2016-01-07 10:58:36+00:00,"[arxiv.Result.Author('Jan Friso Groote'), arxiv.Result.Author('Anton Wijs')]",
474,Tunable spin transport in CrAs: role of correlation effects,"Correlation effects on the electronic structure of half-metallic CrAs in
zinc-blende structure are studied for different substrate lattice constants.
Depending on the substrate the spectral weight of the non-quasiparticle states
might be tuned from a well developed value in the case of InAs substrate to an
almost negligible contribution for the GaAs one. A piezoelectric material that
would allow the change in the substrate lattice parameters opens the
possibility for practical investigations of the switchable (tunable)
non-quasiparticle states. Since the latter are important for the tunneling
magnetoresistance and related phenomena it creates new opportunities in
spintronics.",cond-mat/0501279v1,cond-mat.str-el,2005-01-12 11:38:18+00:00,"[arxiv.Result.Author('L. Chioncel'), arxiv.Result.Author('M. I. Katsnelson'), arxiv.Result.Author('G. A. de Wijs'), arxiv.Result.Author('R. A. de Groot'), arxiv.Result.Author('A. I. Lichtenstein')]",
475,"Ab initio study of magnesium alanate, Mg(AlH4)2","Magnesium alanate Mg(AlH4)2 has recently raised interest as a potential
material for hydrogen storage. We apply ab initio calculations to characterize
structural, electronic and energetic properties of Mg(AlH4)2. Density
functional theory calculations within the generalized gradient approximation
(GGA) are used to optimize the geometry and obtain the electronic structure.
The latter is also studied by quasi-particle calculations at the GW level.
Mg(AlH4)2 is a large band gap insulator with a fundamental band gap of 6.5 eV.
The hydrogen atoms are bonded in AlH4 complexes, whose states dominate both the
valence and the conduction bands. On the basis of total energies, the formation
enthalpy of Mg(AlH4)2 with respect to bulk magnesium, bulk aluminum and
hydrogen gas is 0.17 eV/H2 (at T = 0). Including corrections due to the zero
point vibrations of the hydrogen atoms this number decreases to 0.10 eV/H2. The
enthalpy of the dehydrogenation reaction Mg(AlH4)2 -> MgH2 +2Al+3H2(g) is close
to zero, which impairs the potential usefulness of magnesium alanate as a
hydrogen storage material.",cond-mat/0505623v1,cond-mat.mtrl-sci,2005-05-25 14:48:39+00:00,"[arxiv.Result.Author('M. J. van Setten'), arxiv.Result.Author('G. A. de Wijs'), arxiv.Result.Author('V. A. Popa'), arxiv.Result.Author('G. Brocks')]","Phys. Rev. B, 72, 073107 (2005)"
476,Electronic structure and optical properties of lightweight metal hydrides,"We study the electronic structures and dielectric functions of the simple
hydrides LiH, NaH, MgH2 and AlH3, and the complex hydrides Li3AlH6, Na3AlH6,
LiAlH4, NaAlH4 and Mg(AlH4)2, using first principles density functional theory
and GW calculations. All these compounds are large gap insulators with GW
single particle band gaps varying from 3.5 eV in AlH3 to 6.5 eV in the MAlH4
compounds. The valence bands are dominated by the hydrogen atoms, whereas the
conduction bands have mixed contributions from the hydrogens and the metal
cations. The electronic structure of the aluminium compounds is determined
mainly by aluminium hydride complexes and their mutual interactions. Despite
considerable differences between the band structures and the band gaps of the
various compounds, their optical responses are qualitatively similar. In most
of the spectra the optical absorption rises sharply above 6 eV and has a strong
peak around 8 eV. The quantitative differences in the optical spectra are
interpreted in terms of the structure and the electronic structure of the
compounds.",cond-mat/0609189v1,cond-mat.mtrl-sci,2006-09-08 07:20:38+00:00,"[arxiv.Result.Author('M. J. van Setten'), arxiv.Result.Author('V. A. Popa'), arxiv.Result.Author('G. A. de Wijs'), arxiv.Result.Author('G. Brocks')]","Phys. Rev. B, 75, 035204 (2007)"
477,A model for the formation energies of alanates and boranates,"We develop a simple model for the formation energies (FEs) of alkali and
lkaline earth alanates and boranates, based upon ionic bonding between metal
cations and (AlH4)- or (BH4)- anions. The FEs agree well with values obtained
from first principles calculations and with experimental FEs. The model shows
that details of the crystal structure are relatively unimportant. The small
size of the (BH4)- anion causes a strong bonding in the crystal, which makes
boranates more stable than alanates. Smaller alkali or alkaline earth cations
do not give an increased FE. They involve a larger ionization potential that
compensates for the increased crystal bonding.",cond-mat/0611665v1,cond-mat.mtrl-sci,2006-11-27 09:13:34+00:00,"[arxiv.Result.Author('M. J. van Setten'), arxiv.Result.Author('G. A. de Wijs'), arxiv.Result.Author('G. Brocks')]","J. Phys. Chem. C 111 No. 26, 9592 - 9594, (2007)"
478,Ab initio study on the effects of transition metal doping of Mg2NiH4,"Mg2NiH4 is a promising hydrogen storage material with fast (de)hydrogenation
kinetics. Its hydrogen desorption enthalpy, however, is too large for practical
applications. In this paper we study the effects of transition metal doping by
first-principles density functional theory calculations. We show that the
hydrogen desorption enthalpy can be reduced by ~0.1 eV/H2 if one in eight Ni
atoms is replaced by Cu or Fe. Replacing Ni by Co atoms, however, increases the
hydrogen desorption enthalpy. We study the thermodynamic stability of the
dopants in the hydrogenated and dehydrogenated phases. Doping with Co or Cu
leads to marginally stable compounds, whereas doping with Fe leads to an
unstable compound. The optical response of Mg2NiH4 is also substantially
affected by doping. The optical gap in Mg2NiH4 is ~1.7 eV. Doping with Co, Fe
or Cu leads to impurity bands that reduce the optical gap by up to 0.5 eV.",cond-mat/0703550v1,cond-mat.mtrl-sci,2007-03-21 13:36:25+00:00,"[arxiv.Result.Author('M. J. van Setten'), arxiv.Result.Author('G. A. de Wijs'), arxiv.Result.Author('G. Brocks')]","Phys. Rev. B 76, 075125 (2007)"
479,Interrelation of work function and surface stability: the case of BaAl4,"The relationship between the work function (Phi) and the surface stability of
compounds is, to our knowledge, unknown, but very important for applications
such as organic light-emitting diodes. This relation is studied using
first-principles calculations on various surfaces of BaAl4. The most stable
surface [Ba terminated (001)] has the lowest Phi (1.95 eV), which is lower than
that of any elemental metal including Ba. Adding barium to this surface neither
increases its stability nor lowers its work function. BaAl4 is also strongly
bound. These results run counter to the common perception that stability and a
low Phi are incompatible. Furthermore, a large anisotropy and a stable
low-work-function surface are predicted for intermetallic compounds with polar
surfaces.",physics/0505176v1,physics.comp-ph,2005-05-25 07:44:07+00:00,"[arxiv.Result.Author('M. A. Uijttewaal'), arxiv.Result.Author('G. A. de Wijs'), arxiv.Result.Author('R. A. de Groot'), arxiv.Result.Author('R. Coehoorn'), arxiv.Result.Author('V. van Elsbergen'), arxiv.Result.Author('C. H. L. Weijtens')]",
480,A first-principles study of the electronic structure and stability of Be(BH4)2,"Alanates and boranates are studied intensively because of their potential use
as hydrogen storage materials. In this paper we present a first-principles
study of the electronic structure and the energetics of beryllium boranate,
Be(BH4)2. From total energy calculations we show that - in contrast to the
other boranates and alanates - hydrogen desorption directly to the elements is
likely, and is at least competitive with desorption to the elemental hydride
(BeH2). The formation enthalpy of Be(BH4)2 is only -0.12 eV/H2 (at T=0K). This
low value can be rationalized by the participation of all atoms in the covalent
bonding, in contrast to the ionic bonding observed in other boranates. From
calculations of thermodynamic properties at finite temperature we estimate a
decomposition temperature of 162 K at a pressure of 1 bar.",0712.0907v1,cond-mat.mtrl-sci,2007-12-06 11:10:00+00:00,"[arxiv.Result.Author('M. J. van Setten'), arxiv.Result.Author('G. A. de Wijs'), arxiv.Result.Author('G. Brocks')]","Phys. Rev. B 77, 165115 (2008)"
481,Spin tunneling in junctions with disordered ferromagnets,"We provide compelling evidence to establish that, contrary to one's
elementary guess, the tunneling spin polarization (TSP) of amorphous CoFeB is
larger than that of highly textured fcc CoFeB. First principles atomic and
electronic structure calculations reveal striking agreement between the
measured TSP and the predicted s-electron spin polarization. Given the
disordered structure of the ternary alloy, not only do these results strongly
endorse our communal understanding of tunneling through AlOx, but they also
portray the key concepts that demand primary consideration in such complex
systems.",0712.2722v1,cond-mat.mtrl-sci,2007-12-17 14:19:03+00:00,"[arxiv.Result.Author('P. V. Paluskar'), arxiv.Result.Author('J. J. Attema'), arxiv.Result.Author('G. A. de Wijs'), arxiv.Result.Author('S. Fiddy'), arxiv.Result.Author('E. Snoeck'), arxiv.Result.Author('J. T. Kohlhepp'), arxiv.Result.Author('H. J. M. Swagten'), arxiv.Result.Author('R. A. de Groot'), arxiv.Result.Author('B. Koopmans')]",
482,First-principles study of the optical properties of MgxTi(1-x)H2,"The optical and electronic properties of Mg-Ti hydrides are studied using
first-principles density functional theory. Dielectric functions are calculated
for MgxTi(1-x)H2 with compositions x = 0.5, 0.75, and 0.875. The structure is
that of fluorite TiH2 where both Mg and Ti atoms reside at the Ti positions of
the lattice. In order to assess the effect of randomness in the Mg and Ti
occupations we consider both highly ordered structures, modeled with simple
unit cells of minimal size, and models of random alloys. These are simulated by
super cells containing up to 64 formula units (Z = 64). All compositions and
structural models turn out metallic, hence the dielectric functions contain
interband and intraband free electron contributions. The former are calculated
in the independent particle random phase approximation. The latter are modeled
based upon the intraband plasma frequencies, which are also calculated from
first-principles. Only for the models of the random alloys we obtain a black
state, i.e. low reflection and transmission in the energy range from 1 to 6 eV.",0804.0376v1,cond-mat.mtrl-sci,2008-04-02 15:30:12+00:00,"[arxiv.Result.Author('M. J. van Setten'), arxiv.Result.Author('S. Er'), arxiv.Result.Author('G. Brocks'), arxiv.Result.Author('R. A. de Groot'), arxiv.Result.Author('G. A. de Wijs')]","Physical Review B 79, 125117 (2009)"
483,Tunable Hydrogen Storage in Magnesium - Transition Metal Compounds,"Magnesium dihydride ($\mgh$) stores 7.7 weight % hydrogen, but it suffers
from a high thermodynamic stability and slow (de)hydrogenation kinetics.
Alloying Mg with lightweight transition metals (TM = Sc, Ti, V, Cr) aims at
improving the thermodynamic and kinetic properties. We study the structure and
stability of Mg$_x$TM$_{1-x}$H$_2$ compounds, $x=[0$-1], by first-principles
calculations at the level of density functional theory. We find that the
experimentally observed sharp decrease in hydrogenation rates for $x\gtrsim0.8$
correlates with a phase transition of Mg$_x$TM$_{1-x}$H$_2$ from a fluorite to
a rutile phase. The stability of these compounds decreases along the series Sc,
Ti, V, Cr. Varying the transition metal (TM) and the composition $x$, the
formation enthalpy of Mg$_x$TM$_{1-x}$H$_2$ can be tuned over the substantial
range 0-2 eV/f.u. Assuming however that the alloy Mg$_x$TM$_{1-x}$ does not
decompose upon dehydrogenation, the enthalpy associated with reversible
hydrogenation of compounds with a high magnesium content ($x=0.75$) is close to
that of pure Mg.",0810.2254v1,cond-mat.mtrl-sci,2008-10-13 15:35:46+00:00,"[arxiv.Result.Author('Suleyman Er'), arxiv.Result.Author('Dhirendra Tiwari'), arxiv.Result.Author('Gilles A. de Wijs'), arxiv.Result.Author('Geert Brocks')]","Phys. Rev. B 79, 024105 (2009)"
484,Hydrogen Storage by Polylithiated Molecules and Nanostructures,"We study polylithiated molecules as building blocks for hydrogen storage
materials, using first-principles calculations. $\clifour$ and $\olitwo$ bind
12 and 10 hydrogen molecules, respectively, with an average binding energy of
0.10 and 0.13 eV, leading to gravimetric densities of 37.8 and 40.3 weight % H.
Bonding between Li and C or O is strongly polar and $\hyd$ molecules attach to
the partially charged Li atoms without dissociating, which is favorable for
(de)hydrogenation kinetics. CLi$_n$ and OLi$_m$ molecules can be chemically
bonded to graphene sheets to hinder the aggregation of such molecules. In
particular B or Be doped graphene strongly bind the molecules without seriously
affecting the hydrogen binding energy. It still leads to a hydrogen storage
capacity in the range 5-8.5 wt % H.",0902.2339v1,cond-mat.mtrl-sci,2009-02-13 17:08:53+00:00,"[arxiv.Result.Author('Süleyman Er'), arxiv.Result.Author('Gilles A. de Wijs'), arxiv.Result.Author('Geert Brocks')]","J. Phys. Chem. C 113, 8997-9002 (2009)"
485,DFT Study of Planar Boron Sheets: A New Template for Hydrogen Storage,"We study the hydrogen storage properties of planar boron sheets and compare
them to those of graphene. The binding of molecular hydrogen to the boron sheet
(0.05 eV) is stronger than that to graphene. We find that dispersion of alkali
metal (AM = Li, Na, and K) atoms onto the boron sheet markedly increases
hydrogen binding energies and storage capacities. The unique structure of the
boron sheet presents a template for creating a stable lattice of strongly
bonded metal atoms with a large nearest neighbor distance. In contrast, AM
atoms dispersed on graphene tend to cluster to form a bulk metal. In particular
the boron-Li system is found to be a good candidate for hydrogen storage
purposes. In the fully loaded case this compound can contain up to 10.7 wt. %
molecular hydrogen with an average binding energy of 0.15 eV/H2.",0910.0929v1,cond-mat.mtrl-sci,2009-10-06 17:36:21+00:00,"[arxiv.Result.Author('Süleyman Er'), arxiv.Result.Author('Gilles A. de Wijs'), arxiv.Result.Author('Geert Brocks')]","J. Phys. Chem. C 113, 18962-18967 (2009)"
486,Winner-Take-All Autoencoders,"In this paper, we propose a winner-take-all method for learning hierarchical
sparse representations in an unsupervised fashion. We first introduce
fully-connected winner-take-all autoencoders which use mini-batch statistics to
directly enforce a lifetime sparsity in the activations of the hidden units. We
then propose the convolutional winner-take-all autoencoder which combines the
benefits of convolutional architectures and autoencoders for learning
shift-invariant sparse representations. We describe a way to train
convolutional autoencoders layer by layer, where in addition to lifetime
sparsity, a spatial sparsity within each feature map is achieved using
winner-take-all activation functions. We will show that winner-take-all
autoencoders can be used to to learn deep sparse representations from the
MNIST, CIFAR-10, ImageNet, Street View House Numbers and Toronto Face datasets,
and achieve competitive classification performance.",1409.2752v2,cs.LG,2014-09-09 14:38:43+00:00,"[arxiv.Result.Author('Alireza Makhzani'), arxiv.Result.Author('Brendan Frey')]",
487,Taking the Final Step to a Full Dichotomy of the Possible Winner Problem in Pure Scoring Rules,"The Possible Winner problem asks, given an election where the voters'
preferences over the candidates are specified only partially, whether a
designated candidate can become a winner by suitably extending all the votes.
Betzler and Dorn [1] proved a result that is only one step away from a full
dichotomy of this problem for the important class of pure scoring rules in the
case of unweighted voters and an unbounded number of candidates: Possible
Winner is NP-complete for all pure scoring rules except plurality, veto, and
the scoring rule with vector (2,1,...,1,0), but is solvable in polynomial time
for plurality and veto. We take the final step to a full dichotomy by showing
that Possible Winner is NP-complete also for the scoring rule with vector
(2,1,...,1,0).",1108.4436v2,cs.CC,2011-08-22 20:49:21+00:00,"[arxiv.Result.Author('Dorothea Baumeister'), arxiv.Result.Author('Joerg Rothe')]",
488,The $k$-Cap Process on Geometric Random Graphs,"The $k$-cap (or $k$-winners-take-all) process on a graph works as follows: in
each iteration, exactly $k$ vertices of the graph are in the cap (i.e.,
winners); the next round winners are the vertices that have the highest total
degree to the current winners, with ties broken randomly. This natural process
is a simple model of firing activity in the brain. We study its convergence on
geometric random graphs, revealing rather surprising behavior.",2203.12680v2,math.PR,2022-03-23 19:07:14+00:00,"[arxiv.Result.Author('Mirabel Reid'), arxiv.Result.Author('Santosh S. Vempala')]",
489,The winner takes all: Volume-scavenging populations of networked droplets,"In this work we present and analyze a fluid-mechanical model of competition
(scavenging) amongst $N$ liquid droplets (individual competitors). The eventual
outcome of this competition depends sensitively on the average resource
(volume) per individual $\overline{V}$. For abundant resource,
$\overline{V}>1$, there is one winner only and that winner eventually scavenges
all or most of the resource. In the socio-economic realm this is is known as
the ""winner-take-all"" outcome: A disproportionately large reward falls to one
or a few winners, even though other competitors start out with comparable (or
even slightly more) resource and perform only marginally worse. The losing
competitors are not rewarded. For less than abundant resource,
$\overline{V}<1$, an outcome with resource that is evenly partitioned amongst
the $N$ droplets becomes possible. This is the ""all-share-evenly"" or
egalitarian outcome. For sufficiently scarce resource the egalitarian outcome
is the only one that can occur. In addition to predicting what kind and how
many winners, our analysis shows that once an individual's resource (droplet
volume) falls below a fixed threshold, that individual can neither recover nor
emerge as winner. This is the ""once down-and-out, always down-and-out"" outcome.
Selected simulations suggest that the winner depends sensitively on population
size and the ""trading friction"" or inefficiency of resource exchange between
individuals (liquid rheology). Of all feasible rest states (equilibria), only
certain ones are reachable (stable equilibria). Friction turns out to strongly
influence the time to reach an end state (stable equilibrium), in surprising
ways. Besides the end states, our analysis reveals an array of rest states,
ordered in hierarchies of more versus less costly (energetic) outcomes.",1808.04474v2,physics.flu-dyn,2018-08-13 21:26:54+00:00,"[arxiv.Result.Author('Thomas C. Hagen'), arxiv.Result.Author('Paul H. Steen')]",
490,Revisiting Winner Take All (WTA) Hashing for Sparse Datasets,"WTA (Winner Take All) hashing has been successfully applied in many large
scale vision applications. This hashing scheme was tailored to take advantage
of the comparative reasoning (or order based information), which showed
significant accuracy improvements. In this paper, we identify a subtle issue
with WTA, which grows with the sparsity of the datasets. This issue limits the
discriminative power of WTA. We then propose a solution for this problem based
on the idea of Densification which provably fixes the issue. Our experiments
show that Densified WTA Hashing outperforms Vanilla WTA both in image
classification and retrieval tasks consistently and significantly.",1612.01834v2,cs.CV,2016-12-06 14:51:37+00:00,"[arxiv.Result.Author('Beidi Chen'), arxiv.Result.Author('Anshumali Shrivastava')]",
491,K-Winners-Take-All Computation with Neural Oscillators,"Artificial spike-based computation, inspired by models of computation in the
central nervous system, may present significant performance advantages over
traditional methods for specific types of large scale problems. This paper
describes very simple network architectures for k-winners-take-all and
soft-winner-take-all computation using neural oscillators. Fast convergence is
achieved from arbitrary initial conditions, which makes the networks
particularly suitable to track time-varying inputs.",q-bio/0401001v1,q-bio.NC,2003-12-31 21:47:22+00:00,"[arxiv.Result.Author('Wei Wang'), arxiv.Result.Author('Jean-Jacques E. Slotine')]",
492,The Winner-Take-All Dilemma,"We consider collective decision making when the society consists of groups
endowed with voting weights. Each group chooses an internal rule that specifies
the allocation of its weight to the alternatives as a function of its members'
preferences. Under fairly general conditions, we show that the winner-take-all
rule is a dominant strategy, while the equilibrium is Pareto dominated,
highlighting the dilemma structure between optimality for each group and for
the whole society. We also develop a technique for asymptotic analysis and show
Pareto dominance of the proportional rule. Our numerical computation for the US
Electoral College verifies its sensibility.",2206.09574v1,econ.TH,2022-06-20 05:12:10+00:00,"[arxiv.Result.Author('Kazuya Kikuchi'), arxiv.Result.Author('Yukio Koriyama')]",
493,Winner-take-all selection in a neural system with delayed feedback,"We consider the effects of temporal delay in a neural feedback system with
excitation and inhibition. The topology of our model system reflects the
anatomy of the avian isthmic circuitry, a feedback structure found in all
classes of vertebrates. We show that the system is capable of performing a
`winner-take-all' selection rule for certain combinations of excitatory and
inhibitory feedback. In particular, we show that when the time delays are
sufficiently large a system with local inhibition and global excitation can
function as a `winner-take-all' network and exhibit oscillatory dynamics. We
demonstrate how the origin of the oscillations can be attributed to the finite
delays through a linear stability analysis.",0709.2684v1,physics.bio-ph,2007-09-17 17:57:51+00:00,"[arxiv.Result.Author('Sebastian F. Brandt'), arxiv.Result.Author('Ralf Wessel')]",Biol. Cybern. (2007) 97:221-228
494,Spectra of winner-take-all stochastic neural networks,"During the recent few years, in response to empirical findings suggesting
scale-free self-organisation phenomena emerging in complex nervous systems at a
mesoscale level, there has been significant search for suitable models and
theoretical explanations in neuroscientific literature, see the recent survey
by Bullmore and Sporns (2009). In Piekniewski and Schreiber (2008) we have
developed a simple and tractable mathematical model shedding some light on a
particular class of the afore-mentioned phenomena, namely on mesoscopic level
self-organisation of functional brain networks under fMRI imaging, where we
have achieved a high degree of agreement with existing empirical reports. Being
addressed to the neuroscientific community, our work Piekniewski and Schreiber
(2008) relied on semi-rigorous study of information flow structure in a class
of recurrent neural networks exhibiting asymptotic scale-free behaviour and
admitting a description in terms of the so-called winner-take-all dynamics. The
purpose of the present paper is to define and study these winner-take-all
networks with full mathematical rigour in context of their asymptotic spectral
properties, well known to be of interest for neuroscientific community. Our
main result is a limit theorem for spectra of the spike-flow graphs induced by
the winner-take-all dynamics. We provide an explicit characterisation of the
limit spectral measure expressed in terms of zeros of Bessel's J-function.",0810.3193v2,math.PR,2008-10-17 16:36:32+00:00,[arxiv.Result.Author('Tomasz Schreiber')],
495,Will the Winner Take All? Competing Influences in Social Networks Under Information Overload,"Influence competition finds its significance in many applications, such as
marketing, politics and public events like COVID-19. Existing work tends to
believe that the stronger influence will always win and dominate nearly the
whole network, i.e., ""winner takes all"". However, this finding somewhat
contradicts with our common sense that many competing products are actually
coexistent, e.g., Android vs. iOS. This contradiction naturally raises the
question: will the winner take all?
  To answer this question, we make a comprehensive study into influence
competition by identifying two factors frequently overlooked by prior art: (1)
the incomplete observation of real diffusion networks; (2) the existence of
information overload and its impact on user behaviors. To this end, we attempt
to recover possible diffusion links based on user similarities, which are
extracted by embedding users into a latent space. Following this, we further
derive the condition under which users will be overloaded, and formulate the
competing processes where users' behaviors differ before and after information
overload. By establishing the explicit expressions of competing dynamics, we
disclose that information overload acts as the critical ""boundary line"", before
which the ""winner takes all"" phenomenon will definitively occur, whereas after
information overload the share of influences gradually stabilizes and is
jointly affected by their initial spreading conditions, influence powers and
the advent of overload. Numerous experiments are conducted to validate our
theoretical results where favorable agreement is found. Our work sheds light on
the intrinsic driving forces behind real-world dynamics, thus providing useful
insights into effective information engineering.",2104.14086v1,cs.SI,2021-04-29 03:11:15+00:00,"[arxiv.Result.Author('Chen Feng'), arxiv.Result.Author('Jiahui Sun'), arxiv.Result.Author('Luiyi Fu')]",
496,Magnification Laws of Winner-Relaxing and Winner-Enhancing Kohonen Feature Maps,"Self-Organizing Maps are models for unsupervised representation formation of
cortical receptor fields by stimuli-driven self-organization in laterally
coupled winner-take-all feedforward structures. This paper discusses
modifications of the original Kohonen model that were motivated by a potential
function, in their ability to set up a neural mapping of maximal mutual
information. Enhancing the winner update, instead of relaxing it, results in an
algorithm that generates an infomax map corresponding to magnification exponent
of one. Despite there may be more than one algorithm showing the same
magnification exponent, the magnification law is an experimentally accessible
quantity and therefore suitable for quantitative description of neural
optimization principles.",cs/0701003v1,cs.NE,2006-12-30 11:48:32+00:00,[arxiv.Result.Author('Jens Christian Claussen')],"pp. 17-22 in : V. Capasso (Ed.): Mathematical Modeling & Computing
  in Biology and Medicine, Miriam Series, Progetto Leonardo, Bologna (2003)"
497,Identifying Possible Winners in Ranked Choice Voting Elections with Outstanding Ballots,"Several election districts in the US have recently moved to ranked-choice
voting (RCV) to decide the results of local elections. RCV allows voters to
rank their choices, and the results are computed in rounds, eliminating one
candidate at a time. RCV ensures fairer elections and has been shown to
increase elected representation of women and people of color. A main drawback
of RCV is that the round-by-round process requires all the ballots to be
tallied before the results of an election can be calculated. With increasingly
large portions of ballots coming from absentee voters, RCV election outcomes
are not always apparent on election night, and can take several weeks to be
published, leading to a loss of trust in the electoral process from the public.
In this paper, we present an algorithm for efficiently computing possible
winners of RCV elections from partially known ballots and evaluate it on data
from the recent New York City Primary elections. We show that our techniques
allow to significantly narrow down the field of possible election winners, and
in some case identify the winner as soon as election night despite a number of
yet-unaccounted absentee ballots, providing more transparency in the electoral
process.",2206.12741v1,cs.CY,2022-06-25 22:08:15+00:00,"[arxiv.Result.Author('Alborz Jelvani'), arxiv.Result.Author('Amélie Marian')]",
498,Exploiting Spatio-Temporal Structure with Recurrent Winner-Take-All Networks,"We propose a convolutional recurrent neural network, with Winner-Take-All
dropout for high dimensional unsupervised feature learning in multi-dimensional
time series. We apply the proposedmethod for object recognition with temporal
context in videos and obtain better results than comparable methods in the
literature, including the Deep Predictive Coding Networks previously proposed
by Chalasani and Principe.Our contributions can be summarized as a scalable
reinterpretation of the Deep Predictive Coding Networks trained end-to-end with
backpropagation through time, an extension of the previously proposed
Winner-Take-All Autoencoders to sequences in time, and a new technique for
initializing and regularizing convolutional-recurrent neural networks.",1611.00050v2,cs.LG,2016-10-31 21:16:46+00:00,"[arxiv.Result.Author('Eder Santana'), arxiv.Result.Author('Matthew Emigh'), arxiv.Result.Author('Pablo Zegers'), arxiv.Result.Author('Jose C Principe')]",
499,A Winner-Take-All Approach to Emotional Neural Networks with Universal Approximation Property,"Here, we propose a brain-inspired winner-take-all emotional neural network
(WTAENN) and prove the universal approximation property for the novel
architecture. WTAENN is a single layered feedforward neural network that
benefits from the excitatory, inhibitory, and expandatory neural connections as
well as the winner-take-all (WTA) competitions in the human brain s nervous
system. The WTA competition increases the information capacity of the model
without adding hidden neurons. The universal approximation capability of the
proposed architecture is illustrated on two example functions, trained by a
genetic algorithm, and then applied to several competing recent and benchmark
problems such as in curve fitting, pattern recognition, classification and
prediction. In particular, it is tested on twelve UCI classification datasets,
a facial recognition problem, three real world prediction problems (2 chaotic
time series of geomagnetic activity indices and wind farm power generation
data), two synthetic case studies with constant and nonconstant noise variance
as well as k-selector and linear programming problems. Results indicate the
general applicability and often superiority of the approach in terms of higher
accuracy and lower model complexity, especially where low computational
complexity is imperative.",1511.02426v1,cs.AI,2015-11-08 01:37:14+00:00,[arxiv.Result.Author('E. Lotfi')],
500,Widening Disparity and its Suppression in a Stochastic Replicator Model,"Winner-take-all phenomena are observed in various competitive systems. We
find similar phenomena in replicator models with randomly fluctuating growth
rates. The disparity between winners and losers increases indefinitely, even if
all elements are statistically equivalent. A lognormal distribution describes
well the nonstationary time evolution. If a nonlinear load corresponding to
progressive taxation is introduced, a stationary distribution is obtained and
disparity widening is suppressed.",1602.02153v1,physics.soc-ph,2016-02-05 08:54:29+00:00,[arxiv.Result.Author('Hidetsugu Sakaguchi')],
501,Stochastic Local Winner-Takes-All Networks Enable Profound Adversarial Robustness,"This work explores the potency of stochastic competition-based activations,
namely Stochastic Local Winner-Takes-All (LWTA), against powerful
(gradient-based) white-box and black-box adversarial attacks; we especially
focus on Adversarial Training settings. In our work, we replace the
conventional ReLU-based nonlinearities with blocks comprising locally and
stochastically competing linear units. The output of each network layer now
yields a sparse output, depending on the outcome of winner sampling in each
block. We rely on the Variational Bayesian framework for training and
inference; we incorporate conventional PGD-based adversarial training arguments
to increase the overall adversarial robustness. As we experimentally show, the
arising networks yield state-of-the-art robustness against powerful adversarial
attacks while retaining very high classification rate in the benign case.",2112.02671v1,cs.LG,2021-12-05 20:00:10+00:00,"[arxiv.Result.Author('Konstantinos P. Panousis'), arxiv.Result.Author('Sotirios Chatzis'), arxiv.Result.Author('Sergios Theodoridis')]",
502,Determining Possible and Necessary Winners Given Partial Orders,"Usually a voting rule requires agents to give their preferences as linear
orders. However, in some cases it is impractical for an agent to give a linear
order over all the alternatives. It has been suggested to let agents submit
partial orders instead. Then, given a voting rule, a profile of partial orders,
and an alternative (candidate) c, two important questions arise: first, is it
still possible for c to win, and second, is c guaranteed to win? These are the
possible winner and necessary winner problems, respectively. Each of these two
problems is further divided into two sub-problems: determining whether c is a
unique winner (that is, c is the only winner), or determining whether c is a
co-winner (that is, c is in the set of winners). We consider the setting where
the number of alternatives is unbounded and the votes are unweighted. We
completely characterize the complexity of possible/necessary winner problems
for the following common voting rules: a class of positional scoring rules
(including Borda), Copeland, maximin, Bucklin, ranked pairs, voting trees, and
plurality with runoff.",1401.3876v1,cs.GT,2014-01-16 05:10:45+00:00,"[arxiv.Result.Author('Lirong Xia'), arxiv.Result.Author('Vincent Conitzer')]","Journal Of Artificial Intelligence Research, Volume 41, pages
  25-67, 2011"
503,"Coevolution Maintains Diversity in the Stochastic ""Kill the Winner"" Model","The ""Kill the Winner"" hypothesis is an attempt to address the problem of
diversity in biology. It argues that host-specific predators control the
population of each prey, preventing a winner from emerging and thus maintaining
the coexistence of all species in the system. We develop a stochastic model for
the ""Kill the Winner"" paradigm and show that the stable coexistence state of
the deterministic ""Kill the Winner"" model is destroyed by demographic
stochasticity, through a cascade of extinction events. We formulate an
individual-level stochastic model in which predator-prey coevolution promotes
high diversity of the ecosystem by generating a persistent population flux of
species.",1706.02666v1,physics.bio-ph,2017-06-08 16:19:37+00:00,"[arxiv.Result.Author('Chi Xue'), arxiv.Result.Author('Nigel Goldenfeld')]","Phys. Rev. Lett. 119, 268101 (2017)"
504,Analysis of a Poisson-picking symmetric winners-take-all game with randomized payoffs,"Winners-take-all situations introduce an incentive for agents to diversify
their behavior, since doing so will result in splitting an eventual price with
fewer people. At the same time, when the payoff of a process depends on a
parameter choice that is symmetric with respect to agents, all agents have the
incentive to choose the values of the parameter that lead to higher payoffs. We
explore the trade-off between these two considerations, with a focus on a
particular example. This example can be seen as a simple model for the
situation where a group of friends bet against each other about the top-scoring
team in a sports league. We obtain analytic characterizations of the symmetric
equilibria in the case of only 2 agents and in the case where there are only
two possible top-scorers. We also conduct some simulations beyond these cases,
and observe how does the pressure to diversify behavior evolve as the
parameters of the model change.",1906.03618v1,cs.GT,2019-06-09 11:29:15+00:00,[arxiv.Result.Author('Abel Molina')],
505,Combatting Gerrymandering with Social Choice: the Design of Multi-member Districts,"Every representative democracy must specify a mechanism under which voters
choose their representatives. The most common mechanism in the United States --
Winner takes all single-member districts -- both enables substantial partisan
gerrymandering and constrains `fair' redistricting, preventing proportional
representation in legislatures. We study the design of multi-member districts
(MMDs), in which each district elects multiple representatives, potentially
through a non-Winner takes all voting rule. We carry out large-scale empirical
analyses for the U.S. House of Representatives under MMDs with different social
choice functions, under algorithmically generated maps optimized for either
partisan benefit or proportionality. Doing so requires efficiently
incorporating predicted partisan outcomes -- under various multi-winner social
choice functions -- into an algorithm that optimizes over an ensemble of maps.
We find that with three-member districts using Single Transferable Vote,
fairness-minded independent commissions would be able to achieve proportional
outcomes in every state up to rounding, and advantage-seeking partisans would
have their power to gerrymander significantly curtailed. Simultaneously, such
districts would preserve geographic cohesion, an arguably important aspect of
representative democracies. In the process, we advance a rich research agenda
at the intersection of social choice and computational gerrymandering.",2107.07083v4,cs.GT,2021-07-15 02:29:46+00:00,"[arxiv.Result.Author('Nikhil Garg'), arxiv.Result.Author('Wes Gurnee'), arxiv.Result.Author('David Rothschild'), arxiv.Result.Author('David Shmoys')]",
506,"Transparent, Efficient, and Robust Word Embedding Access with WOMBAT","We present WOMBAT, a Python tool which supports NLP practitioners in
accessing word embeddings from code. WOMBAT addresses common research problems,
including unified access, scaling, and robust and reproducible preprocessing.
Code that uses WOMBAT for accessing word embeddings is not only cleaner, more
readable, and easier to reuse, but also much more efficient than code using
standard in-memory methods: a Python script using WOMBAT for evaluating seven
large word embedding collections (8.7M embedding vectors in total) on a simple
SemEval sentence similarity task involving 250 raw sentence pairs completes in
under ten seconds end-to-end on a standard notebook computer.",1807.00717v1,cs.CL,2018-07-02 14:47:30+00:00,"[arxiv.Result.Author('Mark-Christoph Müller'), arxiv.Result.Author('Michael Strube')]",
507,"The WOMBAT Challenge: A ""Hounds and Hares"" Exercise for Cosmology","The Wavelength-Oriented Microwave Background Analysis Team (WOMBAT) is
constructing microwave skymaps which will be more realistic than previous
simulations. Our foreground models represent a considerable improvement: where
spatial templates are available for a given foreground, we predict the flux and
spectral index of that component at each place on the sky and estimate the
uncertainties in these quantities. We will produce maps containing simulated
Cosmic Microwave Background anisotropies combined with all major expected
foreground components. The simulated maps will be provided to the cosmology
community as the WOMBAT Challenge, a ""hounds and hares"" exercise where such
maps can be analyzed to extract cosmological parameters by scientists who are
unaware of their input values. This exercise will test the efficacy of current
foreground subtraction, power spectrum analysis, and parameter estimation
techniques and will help identify the areas most in need of progress.",astro-ph/9812237v1,astro-ph,1998-12-11 21:04:27+00:00,"[arxiv.Result.Author('Eric Gawiser'), arxiv.Result.Author('Douglas Finkbeiner'), arxiv.Result.Author('Andrew Jaffe'), arxiv.Result.Author('Joanne C. Baker'), arxiv.Result.Author('Amedeo Balbi'), arxiv.Result.Author('Marc Davis'), arxiv.Result.Author('Shaul Hanany'), arxiv.Result.Author('William Holzapfel'), arxiv.Result.Author('Leonidas Moustakas'), arxiv.Result.Author('James Robinson'), arxiv.Result.Author('Evan Scannapieco'), arxiv.Result.Author('George F. Smoot'), arxiv.Result.Author('Joseph Silk')]",
508,WOMBAT & FORECAST: Making Realistic Maps of the Microwave Sky,"The Wavelength-Oriented Microwave Background Analysis Team (WOMBAT) is
constructing microwave maps which will be more realistic than previous
simulations. Our foreground models represent a considerable improvement: where
spatial templates are available for a given foreground, we predict the flux and
spectral index of that component at each place on the sky and estimate
uncertainties. We will produce maps containing simulated CMB anisotropy
combined with expected foregrounds. The simulated maps will be provided to the
community as the WOMBAT Challenge, so such maps can be analyzed to extract
cosmological parameters by scientists who are unaware of their input values.
This will test the efficacy of foreground subtraction, power spectrum analysis,
and parameter estimation techniques and help identify the areas most in need of
progress. These maps are also part of the FORECAST project, which allows
web-based access to the known foreground maps for the planning of CMB missions.",astro-ph/9903248v1,astro-ph,1999-03-17 00:23:49+00:00,"[arxiv.Result.Author('Andrew H. Jaffe'), arxiv.Result.Author('Eric Gawiser'), arxiv.Result.Author('Douglas Finkbeiner'), arxiv.Result.Author('Joanne C. Baker'), arxiv.Result.Author('Amedeo Balbi'), arxiv.Result.Author('Marc Davis'), arxiv.Result.Author('Shaul Hanany'), arxiv.Result.Author('William Holzapfel'), arxiv.Result.Author('Mark Krumholz'), arxiv.Result.Author('Leonidas Moustakas'), arxiv.Result.Author('James Robinson'), arxiv.Result.Author('Evan Scannapieco'), arxiv.Result.Author('George F. Smoot'), arxiv.Result.Author('Joseph Silk')]",
509,WOMBAT: A fully Bayesian global flux-inversion framework,"WOMBAT (the WOllongong Methodology for Bayesian Assimilation of Trace-gases)
is a fully Bayesian hierarchical statistical framework for flux inversion of
trace gases from flask, in situ, and remotely sensed data. WOMBAT extends the
conventional Bayesian-synthesis framework through the consideration of a
correlated error term, the capacity for online bias correction, and the
provision of uncertainty quantification on all unknowns that appear in the
Bayesian statistical model. We show, in an observing system simulation
experiment (OSSE), that these extensions are crucial when the data are indeed
biased and have errors that are correlated. Using the GEOS-Chem atmospheric
transport model, we show that WOMBAT is able to obtain posterior means and
uncertainties on non-fossil-fuel CO$_2$ fluxes from Orbiting Carbon
Observatory-2 (OCO-2) data that are comparable to those from the Model
Intercomparison Project (MIP) reported in Crowell et al. (2019, Atmos. Chem.
Phys., vol. 19). We also find that our predictions of out-of-sample retrievals
from the Total Column Carbon Observing Network are, for the most part, more
accurate than those made by the MIP participants. Subsequent versions of the
OCO-2 datasets will be ingested into WOMBAT as they become available.",2102.04004v1,stat.AP,2021-02-08 05:12:52+00:00,"[arxiv.Result.Author('Andrew Zammit-Mangion'), arxiv.Result.Author('Michael Bertolacci'), arxiv.Result.Author('Jenny Fisher'), arxiv.Result.Author('Ann Stavert'), arxiv.Result.Author('Matthew L. Rigby'), arxiv.Result.Author('Yi Cao'), arxiv.Result.Author('Noel Cressie')]",
510,A flowing plasma model to describe drift waves in a cylindrical helicon discharge,"A two-fluid model developed originally to describe wave oscillations in the
vacuum arc centrifuge, a cylindrical, rapidly rotating, low temperature and
confined plasma column, is applied to interpret plasma oscillations in a RF
generated linear magnetised plasma (WOMBAT), with similar density and field
strength. Compared to typical centrifuge plasmas, WOMBAT plasmas have slower
normalised rotation frequency, lower temperature and lower axial velocity.
Despite these differences, the two-fluid model provides a consistent
description of the WOMBAT plasma configuration and yields qualitative agreement
between measured and predicted wave oscillation frequencies with axial field
strength. In addition, the radial profile of the density perturbation predicted
by this model is consistent with the data. Parameter scans show that the
dispersion curve is sensitive to the axial field strength and the electron
temperature, and the dependence of oscillation frequency with electron
temperature matches the experiment. These results consolidate earlier claims
that the density and floating potential oscillations are a resistive drift
mode, driven by the density gradient. To our knowledge, this is the first
detailed physics model of flowing plasmas in the diffusion region away from the
RF source. Possible extensions to the model, including temperature
non-uniformity and magnetic field oscillations, are also discussed.",1102.4144v1,physics.plasm-ph,2011-02-21 06:16:56+00:00,"[arxiv.Result.Author('L. Chang'), arxiv.Result.Author('M. J. Hole'), arxiv.Result.Author('C. S. Cormac')]",
511,Towards Exascale Simulations of the ICM Dynamo with WENO-Wombat,"In galaxy clusters, modern radio interferometers observe non-thermal radio
sources with unprecedented spatial and spectral resolution. For the first time,
the new data allows to infer the structure of the intra-cluster magnetic fields
on small scales via Faraday tomography. This leap forward demands new numerical
models for the amplification of magnetic fields in cosmic structure formation -
the cosmological magnetic dynamo. Here we present a novel numerical approach to
astrophyiscal MHD simulations aimed to resolve this small-scale dynamo in
future cosmological simulations. As a first step, we implement a fifth order
WENO scheme in the new code WOMBAT. We show that this scheme doubles the
effective resolution of the simulation and is thus less expensive than common
second order schemes. WOMBAT uses a novel approach to parallelization and load
balancing developed in collaboration with performance engineers at Cray Inc.
This will allow us scale simulation to the exaflop regime and achieve kpc
resolution in future cosmological simulations of galaxy clusters. Here we
demonstrate the excellent scaling properties of the code and argue that
resolved simulations of the cosmological small scale dynamo within the whole
virial radius are possible in the next years.",1808.10633v2,physics.comp-ph,2018-08-31 08:38:44+00:00,"[arxiv.Result.Author('J. Donnert'), arxiv.Result.Author('H. Jang'), arxiv.Result.Author('P. Mendygral'), arxiv.Result.Author('G. Brunetti'), arxiv.Result.Author('D. Ryu'), arxiv.Result.Author('T. Jones')]",
512,"Inferring changes to the global carbon cycle with WOMBAT v2.0, a hierarchical flux-inversion framework","The natural cycles of the surface-to-atmosphere fluxes of carbon dioxide
(CO$_2$) and other important greenhouse gases are changing in response to human
influences. These changes need to be quantified to understand climate change
and its impacts, but this is difficult to do because natural fluxes occur over
large spatial and temporal scales. To infer trends in fluxes and identify phase
shifts and amplitude changes in flux seasonal cycles, we construct a
flux-inversion system that uses a novel spatially varying time-series
decomposition of the fluxes, while also accommodating physical constraints on
the fluxes. We incorporate these features into the Wollongong Methodology for
Bayesian Assimilation of Trace-gases (WOMBAT, Zammit-Mangion et al., Geosci.
Model Dev., 15, 2022), a hierarchical flux-inversion framework that yields
posterior distributions for all unknowns in the underlying model. We apply the
new method, which we call WOMBAT v2.0, to a mix of satellite observations of
CO$_2$ mole fraction from the Orbiting Carbon Observatory-2 (OCO-2) satellite
and direct measurements of CO$_2$ mole fraction from a variety of sources. We
estimate the changes to CO$_2$ fluxes that occurred from January 2015 to
December 2020, and compare our posterior estimates to those from an alternative
method based on a bottom-up understanding of the physical processes involved.
We find substantial trends in the fluxes, including that tropical ecosystems
trended from being a net source to a net sink of CO$_2$ over the study period.
We also find that the amplitude of the global seasonal cycle of ecosystem
CO$_2$ fluxes increased over the study period by 0.11 PgC/month (an increase of
8%), and that the seasonal cycle of ecosystem CO$_2$ fluxes in the northern
temperate and northern boreal regions shifted earlier in the year by 0.4-0.7
and 0.4-0.9 days, respectively (2.5th to 97.5th posterior percentiles).",2210.10479v1,physics.ao-ph,2022-10-19 11:39:28+00:00,"[arxiv.Result.Author('Michael Bertolacci'), arxiv.Result.Author('Andrew Zammit-Mangion'), arxiv.Result.Author('Andrew Schuh'), arxiv.Result.Author('Beata Bukosa'), arxiv.Result.Author('Jenny Fisher'), arxiv.Result.Author('Yi Cao'), arxiv.Result.Author('Aleya Kaushik'), arxiv.Result.Author('Noel Cressie')]",
513,WENO-Wombat: Scalable Fifth-Order Constrained-Transport Magnetohydrodynamics for Astrophysical Applications,"Due to increase in computing power, high-order Eulerian schemes will likely
become instrumental for the simulations of turbulence and magnetic field
amplification in astrophysical fluids in the next years. We present the
implementation of a fifth order weighted essentially non-oscillatory scheme for
constrained-transport magnetohydrodynamics into the code WOMBAT. We establish
the correctness of our implementation with an extensive number tests. We find
that the fifth order scheme performs as accurately as a common second order
scheme at half the resolution. We argue that for a given solution quality the
new scheme is more computationally efficient than lower order schemes in three
dimensions. We also establish the performance characteristics of the solver in
the WOMBAT framework. Our implementation fully vectorizes using flattened
arrays in thread-local memory. It performs at about 0.6 Million zones per
second per node on Intel Broadwell. We present scaling tests of the code up to
98 thousand cores on the Cray XC40 machine ""Hazel Hen"", with a sustained
performance of about 5 percent of peak at scale.",1812.04278v1,physics.comp-ph,2018-12-11 09:12:38+00:00,"[arxiv.Result.Author('J. M. F. Donnert'), arxiv.Result.Author('H. Jang'), arxiv.Result.Author('P. Mendygral'), arxiv.Result.Author('G. Brunetti'), arxiv.Result.Author('D. Ryu'), arxiv.Result.Author('T. W. Jones')]",
514,Big Bang Leftovers in the Microwave: Cosmology with the Cosmic Microwave Background Radiation,"We combine detections of anisotropy in the Cosmic Microwave Background
radiation with observations of inhomogeneity in the large-scale distribution of
galaxies to test the predictions of models of cosmological structure formation.
This combination probes spatial scales varying by three orders of magnitude,
including a significant region where the two types of data overlap. We examine
Cold Dark Matter models with adiabatic density perturbations, isocurvature
models, and a topological defects model. We set upper limits on the neutrino
mass and find the primordial power spectrum needed to reconcile an apparent
disagreement between structure formation observations and direct observations
of cosmological parameters.
  Present and future observations of Cosmic Microwave Background anisotropy
suffer from foreground contamination. We develop detailed predictions for
microwave emission from radio and infrared-bright galaxies and the
Sunyaev-Zeldovich effect from clusters. We present realistic simulations of the
microwave sky, produced as part of the `WOMBAT Challenge' exercise, and
introduce a pixel-space method for subtracting foreground contamination which
can be tested on these simulations.",astro-ph/0005365v1,astro-ph,2000-05-17 23:35:53+00:00,[arxiv.Result.Author('Eric Gawiser')],
515,WOMBAT: A Scalable and High Performance Astrophysical MHD Code,"We present a new code for astrophysical magneto-hydrodynamics specifically
designed and optimized for high performance and scaling on modern and future
supercomputers. We describe a novel hybrid OpenMP/MPI programming model that
emerged from a collaboration between Cray, Inc. and the University of
Minnesota. This design utilizes MPI-RMA optimized for thread scaling, which
allows the code to run extremely efficiently at very high thread counts ideal
for the latest generation of the multi-core and many-core architectures. Such
performance characteristics are needed in the era of ""exascale"" computing. We
describe and demonstrate our high-performance design in detail with the intent
that it may be used as a model for other, future astrophysical codes intended
for applications demanding exceptional performance.",1701.07452v1,astro-ph.IM,2017-01-25 19:07:47+00:00,"[arxiv.Result.Author('Peter Mendygral'), arxiv.Result.Author('Nick Radcliffe'), arxiv.Result.Author('Krishna Kandalla'), arxiv.Result.Author('David Porter'), arxiv.Result.Author(""Brian J. O'Neill""), arxiv.Result.Author('Chris Nolting'), arxiv.Result.Author('Paul Edmon'), arxiv.Result.Author('Julius M. F. Donnert'), arxiv.Result.Author('Thomas W. Jones')]",
516,"Transparent, Efficient, and Robust Word Embedding Access with WOMBAT","We present WOMBAT, a Python tool which supports NLP practitioners in
accessing word embeddings from code. WOMBAT addresses common research problems,
including unified access, scaling, and robust and reproducible preprocessing.
Code that uses WOMBAT for accessing word embeddings is not only cleaner, more
readable, and easier to reuse, but also much more efficient than code using
standard in-memory methods: a Python script using WOMBAT for evaluating seven
large word embedding collections (8.7M embedding vectors in total) on a simple
SemEval sentence similarity task involving 250 raw sentence pairs completes in
under ten seconds end-to-end on a standard notebook computer.",1807.00717v1,cs.CL,2018-07-02 14:47:30+00:00,"[arxiv.Result.Author('Mark-Christoph Müller'), arxiv.Result.Author('Michael Strube')]",
517,"The WOMBAT Challenge: A ""Hounds and Hares"" Exercise for Cosmology","The Wavelength-Oriented Microwave Background Analysis Team (WOMBAT) is
constructing microwave skymaps which will be more realistic than previous
simulations. Our foreground models represent a considerable improvement: where
spatial templates are available for a given foreground, we predict the flux and
spectral index of that component at each place on the sky and estimate the
uncertainties in these quantities. We will produce maps containing simulated
Cosmic Microwave Background anisotropies combined with all major expected
foreground components. The simulated maps will be provided to the cosmology
community as the WOMBAT Challenge, a ""hounds and hares"" exercise where such
maps can be analyzed to extract cosmological parameters by scientists who are
unaware of their input values. This exercise will test the efficacy of current
foreground subtraction, power spectrum analysis, and parameter estimation
techniques and will help identify the areas most in need of progress.",astro-ph/9812237v1,astro-ph,1998-12-11 21:04:27+00:00,"[arxiv.Result.Author('Eric Gawiser'), arxiv.Result.Author('Douglas Finkbeiner'), arxiv.Result.Author('Andrew Jaffe'), arxiv.Result.Author('Joanne C. Baker'), arxiv.Result.Author('Amedeo Balbi'), arxiv.Result.Author('Marc Davis'), arxiv.Result.Author('Shaul Hanany'), arxiv.Result.Author('William Holzapfel'), arxiv.Result.Author('Leonidas Moustakas'), arxiv.Result.Author('James Robinson'), arxiv.Result.Author('Evan Scannapieco'), arxiv.Result.Author('George F. Smoot'), arxiv.Result.Author('Joseph Silk')]",
518,WOMBAT & FORECAST: Making Realistic Maps of the Microwave Sky,"The Wavelength-Oriented Microwave Background Analysis Team (WOMBAT) is
constructing microwave maps which will be more realistic than previous
simulations. Our foreground models represent a considerable improvement: where
spatial templates are available for a given foreground, we predict the flux and
spectral index of that component at each place on the sky and estimate
uncertainties. We will produce maps containing simulated CMB anisotropy
combined with expected foregrounds. The simulated maps will be provided to the
community as the WOMBAT Challenge, so such maps can be analyzed to extract
cosmological parameters by scientists who are unaware of their input values.
This will test the efficacy of foreground subtraction, power spectrum analysis,
and parameter estimation techniques and help identify the areas most in need of
progress. These maps are also part of the FORECAST project, which allows
web-based access to the known foreground maps for the planning of CMB missions.",astro-ph/9903248v1,astro-ph,1999-03-17 00:23:49+00:00,"[arxiv.Result.Author('Andrew H. Jaffe'), arxiv.Result.Author('Eric Gawiser'), arxiv.Result.Author('Douglas Finkbeiner'), arxiv.Result.Author('Joanne C. Baker'), arxiv.Result.Author('Amedeo Balbi'), arxiv.Result.Author('Marc Davis'), arxiv.Result.Author('Shaul Hanany'), arxiv.Result.Author('William Holzapfel'), arxiv.Result.Author('Mark Krumholz'), arxiv.Result.Author('Leonidas Moustakas'), arxiv.Result.Author('James Robinson'), arxiv.Result.Author('Evan Scannapieco'), arxiv.Result.Author('George F. Smoot'), arxiv.Result.Author('Joseph Silk')]",
519,WOMBAT: A fully Bayesian global flux-inversion framework,"WOMBAT (the WOllongong Methodology for Bayesian Assimilation of Trace-gases)
is a fully Bayesian hierarchical statistical framework for flux inversion of
trace gases from flask, in situ, and remotely sensed data. WOMBAT extends the
conventional Bayesian-synthesis framework through the consideration of a
correlated error term, the capacity for online bias correction, and the
provision of uncertainty quantification on all unknowns that appear in the
Bayesian statistical model. We show, in an observing system simulation
experiment (OSSE), that these extensions are crucial when the data are indeed
biased and have errors that are correlated. Using the GEOS-Chem atmospheric
transport model, we show that WOMBAT is able to obtain posterior means and
uncertainties on non-fossil-fuel CO$_2$ fluxes from Orbiting Carbon
Observatory-2 (OCO-2) data that are comparable to those from the Model
Intercomparison Project (MIP) reported in Crowell et al. (2019, Atmos. Chem.
Phys., vol. 19). We also find that our predictions of out-of-sample retrievals
from the Total Column Carbon Observing Network are, for the most part, more
accurate than those made by the MIP participants. Subsequent versions of the
OCO-2 datasets will be ingested into WOMBAT as they become available.",2102.04004v1,stat.AP,2021-02-08 05:12:52+00:00,"[arxiv.Result.Author('Andrew Zammit-Mangion'), arxiv.Result.Author('Michael Bertolacci'), arxiv.Result.Author('Jenny Fisher'), arxiv.Result.Author('Ann Stavert'), arxiv.Result.Author('Matthew L. Rigby'), arxiv.Result.Author('Yi Cao'), arxiv.Result.Author('Noel Cressie')]",
520,A flowing plasma model to describe drift waves in a cylindrical helicon discharge,"A two-fluid model developed originally to describe wave oscillations in the
vacuum arc centrifuge, a cylindrical, rapidly rotating, low temperature and
confined plasma column, is applied to interpret plasma oscillations in a RF
generated linear magnetised plasma (WOMBAT), with similar density and field
strength. Compared to typical centrifuge plasmas, WOMBAT plasmas have slower
normalised rotation frequency, lower temperature and lower axial velocity.
Despite these differences, the two-fluid model provides a consistent
description of the WOMBAT plasma configuration and yields qualitative agreement
between measured and predicted wave oscillation frequencies with axial field
strength. In addition, the radial profile of the density perturbation predicted
by this model is consistent with the data. Parameter scans show that the
dispersion curve is sensitive to the axial field strength and the electron
temperature, and the dependence of oscillation frequency with electron
temperature matches the experiment. These results consolidate earlier claims
that the density and floating potential oscillations are a resistive drift
mode, driven by the density gradient. To our knowledge, this is the first
detailed physics model of flowing plasmas in the diffusion region away from the
RF source. Possible extensions to the model, including temperature
non-uniformity and magnetic field oscillations, are also discussed.",1102.4144v1,physics.plasm-ph,2011-02-21 06:16:56+00:00,"[arxiv.Result.Author('L. Chang'), arxiv.Result.Author('M. J. Hole'), arxiv.Result.Author('C. S. Cormac')]",
521,Towards Exascale Simulations of the ICM Dynamo with WENO-Wombat,"In galaxy clusters, modern radio interferometers observe non-thermal radio
sources with unprecedented spatial and spectral resolution. For the first time,
the new data allows to infer the structure of the intra-cluster magnetic fields
on small scales via Faraday tomography. This leap forward demands new numerical
models for the amplification of magnetic fields in cosmic structure formation -
the cosmological magnetic dynamo. Here we present a novel numerical approach to
astrophyiscal MHD simulations aimed to resolve this small-scale dynamo in
future cosmological simulations. As a first step, we implement a fifth order
WENO scheme in the new code WOMBAT. We show that this scheme doubles the
effective resolution of the simulation and is thus less expensive than common
second order schemes. WOMBAT uses a novel approach to parallelization and load
balancing developed in collaboration with performance engineers at Cray Inc.
This will allow us scale simulation to the exaflop regime and achieve kpc
resolution in future cosmological simulations of galaxy clusters. Here we
demonstrate the excellent scaling properties of the code and argue that
resolved simulations of the cosmological small scale dynamo within the whole
virial radius are possible in the next years.",1808.10633v2,physics.comp-ph,2018-08-31 08:38:44+00:00,"[arxiv.Result.Author('J. Donnert'), arxiv.Result.Author('H. Jang'), arxiv.Result.Author('P. Mendygral'), arxiv.Result.Author('G. Brunetti'), arxiv.Result.Author('D. Ryu'), arxiv.Result.Author('T. Jones')]",
522,"Inferring changes to the global carbon cycle with WOMBAT v2.0, a hierarchical flux-inversion framework","The natural cycles of the surface-to-atmosphere fluxes of carbon dioxide
(CO$_2$) and other important greenhouse gases are changing in response to human
influences. These changes need to be quantified to understand climate change
and its impacts, but this is difficult to do because natural fluxes occur over
large spatial and temporal scales. To infer trends in fluxes and identify phase
shifts and amplitude changes in flux seasonal cycles, we construct a
flux-inversion system that uses a novel spatially varying time-series
decomposition of the fluxes, while also accommodating physical constraints on
the fluxes. We incorporate these features into the Wollongong Methodology for
Bayesian Assimilation of Trace-gases (WOMBAT, Zammit-Mangion et al., Geosci.
Model Dev., 15, 2022), a hierarchical flux-inversion framework that yields
posterior distributions for all unknowns in the underlying model. We apply the
new method, which we call WOMBAT v2.0, to a mix of satellite observations of
CO$_2$ mole fraction from the Orbiting Carbon Observatory-2 (OCO-2) satellite
and direct measurements of CO$_2$ mole fraction from a variety of sources. We
estimate the changes to CO$_2$ fluxes that occurred from January 2015 to
December 2020, and compare our posterior estimates to those from an alternative
method based on a bottom-up understanding of the physical processes involved.
We find substantial trends in the fluxes, including that tropical ecosystems
trended from being a net source to a net sink of CO$_2$ over the study period.
We also find that the amplitude of the global seasonal cycle of ecosystem
CO$_2$ fluxes increased over the study period by 0.11 PgC/month (an increase of
8%), and that the seasonal cycle of ecosystem CO$_2$ fluxes in the northern
temperate and northern boreal regions shifted earlier in the year by 0.4-0.7
and 0.4-0.9 days, respectively (2.5th to 97.5th posterior percentiles).",2210.10479v1,physics.ao-ph,2022-10-19 11:39:28+00:00,"[arxiv.Result.Author('Michael Bertolacci'), arxiv.Result.Author('Andrew Zammit-Mangion'), arxiv.Result.Author('Andrew Schuh'), arxiv.Result.Author('Beata Bukosa'), arxiv.Result.Author('Jenny Fisher'), arxiv.Result.Author('Yi Cao'), arxiv.Result.Author('Aleya Kaushik'), arxiv.Result.Author('Noel Cressie')]",
523,WENO-Wombat: Scalable Fifth-Order Constrained-Transport Magnetohydrodynamics for Astrophysical Applications,"Due to increase in computing power, high-order Eulerian schemes will likely
become instrumental for the simulations of turbulence and magnetic field
amplification in astrophysical fluids in the next years. We present the
implementation of a fifth order weighted essentially non-oscillatory scheme for
constrained-transport magnetohydrodynamics into the code WOMBAT. We establish
the correctness of our implementation with an extensive number tests. We find
that the fifth order scheme performs as accurately as a common second order
scheme at half the resolution. We argue that for a given solution quality the
new scheme is more computationally efficient than lower order schemes in three
dimensions. We also establish the performance characteristics of the solver in
the WOMBAT framework. Our implementation fully vectorizes using flattened
arrays in thread-local memory. It performs at about 0.6 Million zones per
second per node on Intel Broadwell. We present scaling tests of the code up to
98 thousand cores on the Cray XC40 machine ""Hazel Hen"", with a sustained
performance of about 5 percent of peak at scale.",1812.04278v1,physics.comp-ph,2018-12-11 09:12:38+00:00,"[arxiv.Result.Author('J. M. F. Donnert'), arxiv.Result.Author('H. Jang'), arxiv.Result.Author('P. Mendygral'), arxiv.Result.Author('G. Brunetti'), arxiv.Result.Author('D. Ryu'), arxiv.Result.Author('T. W. Jones')]",
524,Big Bang Leftovers in the Microwave: Cosmology with the Cosmic Microwave Background Radiation,"We combine detections of anisotropy in the Cosmic Microwave Background
radiation with observations of inhomogeneity in the large-scale distribution of
galaxies to test the predictions of models of cosmological structure formation.
This combination probes spatial scales varying by three orders of magnitude,
including a significant region where the two types of data overlap. We examine
Cold Dark Matter models with adiabatic density perturbations, isocurvature
models, and a topological defects model. We set upper limits on the neutrino
mass and find the primordial power spectrum needed to reconcile an apparent
disagreement between structure formation observations and direct observations
of cosmological parameters.
  Present and future observations of Cosmic Microwave Background anisotropy
suffer from foreground contamination. We develop detailed predictions for
microwave emission from radio and infrared-bright galaxies and the
Sunyaev-Zeldovich effect from clusters. We present realistic simulations of the
microwave sky, produced as part of the `WOMBAT Challenge' exercise, and
introduce a pixel-space method for subtracting foreground contamination which
can be tested on these simulations.",astro-ph/0005365v1,astro-ph,2000-05-17 23:35:53+00:00,[arxiv.Result.Author('Eric Gawiser')],
525,WOMBAT: A Scalable and High Performance Astrophysical MHD Code,"We present a new code for astrophysical magneto-hydrodynamics specifically
designed and optimized for high performance and scaling on modern and future
supercomputers. We describe a novel hybrid OpenMP/MPI programming model that
emerged from a collaboration between Cray, Inc. and the University of
Minnesota. This design utilizes MPI-RMA optimized for thread scaling, which
allows the code to run extremely efficiently at very high thread counts ideal
for the latest generation of the multi-core and many-core architectures. Such
performance characteristics are needed in the era of ""exascale"" computing. We
describe and demonstrate our high-performance design in detail with the intent
that it may be used as a model for other, future astrophysical codes intended
for applications demanding exceptional performance.",1701.07452v1,astro-ph.IM,2017-01-25 19:07:47+00:00,"[arxiv.Result.Author('Peter Mendygral'), arxiv.Result.Author('Nick Radcliffe'), arxiv.Result.Author('Krishna Kandalla'), arxiv.Result.Author('David Porter'), arxiv.Result.Author(""Brian J. O'Neill""), arxiv.Result.Author('Chris Nolting'), arxiv.Result.Author('Paul Edmon'), arxiv.Result.Author('Julius M. F. Donnert'), arxiv.Result.Author('Thomas W. Jones')]",
526,Risk Automatic Prediction for Social Economy Companies using Camels,"Governments have to supervise and inspect social economy enterprises (SEEs).
However, inspecting all SEEs is not possible due to the large number of SEEs
and the low number of inspectors in general. We proposed a prediction model
based on a machine learning approach. The method was trained with the random
forest algorithm with historical data provided by each SEE. Three consecutive
periods of data were concatenated. The proposed method uses these periods as
input data and predicts the risk of each SEE in the fourth period. The model
achieved 76\% overall accuracy. In addition, it obtained good accuracy in
predicting the high risk of a SEE. We found that the legal nature and the
variation of the past-due portfolio are good predictors of the future risk of a
SEE. Thus, the risk of a SEE in a future period can be predicted by a
supervised machine learning method. Predicting the high risk of a SEE improves
the daily work of each inspector by focusing only on high-risk SEEs.",2210.05052v1,cs.LG,2022-10-10 23:50:02+00:00,"[arxiv.Result.Author('Joseph Gallego-Mejia'), arxiv.Result.Author('Daniela Martin-Vega'), arxiv.Result.Author('Fabio Gonzalez')]",
527,Triviality and the (Supersymmetric) See-Saw,"For the D=5 Majorana neutrino mass operator to have a see-saw ultraviolet
completion that is viable up to the Planck scale, the see-saw scale is bounded
above due to triviality limits on the see-saw couplings. For supersymmetric
see-saw models, with realistic neutrino mass textures, we compare constraints
on the see-saw scale from triviality bounds, with those arising from
experimental limits on induced charged-lepton flavour violation, for both the
CMSSM and for models with split supersymmetry.",hep-ph/0603053v2,hep-ph,2006-03-07 16:47:09+00:00,"[arxiv.Result.Author('Bruce A. Campbell'), arxiv.Result.Author('David W. Maybury')]","JHEP 0704:077,2007"
528,Astronomical Seeing at Maidanak Observatory during the year 2018,"Astronomical seeing measurements were carried out at Maidanak observatory
during the period from August to November 2018 using DIMM (Differential Image
Motion Monitor). The median value of seeing for the entire period was
determined as 0.54 arcseconds. This value was compared to the seeing data of
the period 1996-2002.",2012.08110v1,astro-ph.IM,2020-12-15 06:19:17+00:00,"[arxiv.Result.Author('Y. A. Tillayev'), arxiv.Result.Author('A. M. Azimov'), arxiv.Result.Author('A. R. Hafizov')]",
529,"The see-saw mechanism: neutrino mixing, leptogenesis and lepton flavor violation","The see-saw mechanism to generate small neutrino masses is reviewed. After
summarizing our current knowledge about the low energy neutrino mass matrix we
consider reconstructing the see-saw mechanism. Low energy neutrino physics is
not sufficient to reconstruct see-saw, a feature which we refer to as ``see-saw
degeneracy''. Indirect tests of see-saw are leptogenesis and lepton flavor
violation in supersymmetric scenarios, which together with neutrino mass and
mixing define the framework of see-saw phenomenology. Several examples are
given, both phenomenological and GUT-related. Variants of the see-saw mechanism
like the type II or triplet see-saw are also discussed. In particular, we
compare many general aspects regarding the dependence of LFV on low energy
neutrino parameters in the extreme cases of a dominating conventional see-saw
term or a dominating triplet term. For instance, the absence of mu -> e gamma
or tau -> e gamma in the pure triplet case means that CP is conserved in
neutrino oscillations. Scanning models, we also find that among the decays mu
-> e gamma, tau -> e gamma and tau -> mu gamma the latter one has the largest
branching ratio in (i) SO(10) type I see-saw models and in (ii) scenarios in
which the triplet term dominates in the neutrino mass matrix.",0804.3925v3,hep-ph,2008-04-24 13:44:29+00:00,[arxiv.Result.Author('Werner Rodejohann')],"Pramana 72:217-227,2009"
530,Statistics of turbulence profile at Cerro Tololo,"Results of 3-month continuous monitoring of turbulence profile and seeing at
Cerro Tololo (Chile) in May-July 2002 are presented. Some 28000 low-resolution
profiles were measured by a new MASS single-star turbulence monitor,
accompanied by seeing data from DIMM. The median seeing was 0.95 arcseconds.
The first 500 m contribute 60% to the total seeing, the free-atmosphere median
seeing was 0.55 arcseconds. Free-atmosphere seeing is almost never better than
0.15 arcseconds because there is always some turbulence above 12 km. A 4-day
period of calm upper atmosphere with a stable free-atmosphere seeing of 0.2-0.3
arcseconds was noted. A gain in resolution from adaptive compensation of ground
layer will be 1.7 times typically and 2-3 times during such calm periods.
Correlations of the free-atmosphere turbulence with the wind speed at
tropopause and of the ground-layer turbulence with ground wind are studied.
Temporal evolution of turbulence is characterized by recurrent bursts, their
typical duration increases from 15 minutes in low layers to 1-2 hours in high
layers. The large data base of turbulence profiles can be used to test
meso-scale modeling of astronomical seeing.",astro-ph/0209432v1,astro-ph,2002-09-20 16:32:20+00:00,"[arxiv.Result.Author('A. Tokovinin'), arxiv.Result.Author('S. Baumont'), arxiv.Result.Author('J. Vasquez')]",Mon.Not.Roy.Astron.Soc. 340 (2003) 52
531,The Spectrum of HD 3651B: An Extrasolar Nemesis?,This article has been withdrawn; see astro-ph/0611542,astro-ph/0609556v2,astro-ph,2006-09-20 00:36:50+00:00,[arxiv.Result.Author('Adam J. Burgasser')],
532,Criticality versus q in the 2+1-dimensional $Z_q$ clock model,"Paper has been withdrawn, see comment.",cond-mat/0212255v2,cond-mat.supr-con,2002-12-11 17:32:31+00:00,"[arxiv.Result.Author('J. Hove'), arxiv.Result.Author('A. Sudbo')]",
533,Closedness of the Space of AHE Metrics on 4-Manifolds,See comment above.,math/0012167v2,math.DG,2000-12-18 19:05:21+00:00,[arxiv.Result.Author('Michael T. Anderson')],
534,A Machine Learning Approach to Correcting Atmospheric Seeing in Solar Flare Observations,"Current post-processing techniques for the correction of atmospheric seeing
in solar observations -- such as Speckle interferometry and Phase Diversity
methods -- have limitations when it comes to their reconstructive capabilities
of solar flare observations. This, combined with the sporadic nature of flares
meaning observers cannot wait until seeing conditions are optimal before taking
measurements, means that many ground-based solar flare observations are marred
with bad seeing. To combat this, we propose a method for dedicated flare seeing
correction based on training a deep neural network to learn to correct
artificial seeing from flare observations taken during good seeing conditions.
This model uses transfer learning, a novel technique in solar physics, to help
learn these corrections. Transfer learning is when another network already
trained on similar data is used to influence the learning of the new network.
Once trained, the model has been applied to two flare datasets: one from
AR12157 on 2014/09/06 and one from AR12673 on 2017/09/06. The results show good
corrections to images with bad seeing with a relative error assigned to the
estimate based on the performance of the model. Further discussion takes place
of improvements to the robustness of the error on these estimates.",2011.12814v1,astro-ph.SR,2020-11-25 15:17:26+00:00,"[arxiv.Result.Author('John A. Armstrong'), arxiv.Result.Author('Lyndsay Fletcher')]",
535,Daytime Seeing and Solar Limb Positions,"A method to measure the seeing from video made during drift-scan solar
transits is proposed. The limb of the Sun is projected over a regular grid
evenly spaced. The temporal dispersion of the time intervals among the contacts
between solar limb and grid's rows is proportional to the atmospheric seeing.
Seeing effects on the position of the inflexion point of the limb's luminosity
profile are calculated numerically with Fast Fourier Transform. Observational
examples from Locarno and Paris Observatories are presented to show the
asymmetric contributions of the seeing at the beginning and the end of each
drift-scan transit.",1106.2539v1,astro-ph.IM,2011-06-13 19:58:47+00:00,[arxiv.Result.Author('Costantino Sigismondi')],
536,News Recommendation with Candidate-aware User Modeling,"News recommendation aims to match news with personalized user interest.
Existing methods for news recommendation usually model user interest from
historical clicked news without the consideration of candidate news. However,
each user usually has multiple interests, and it is difficult for these methods
to accurately match a candidate news with a specific user interest. In this
paper, we present a candidate-aware user modeling method for personalized news
recommendation, which can incorporate candidate news into user modeling for
better matching between candidate news and user interest. We propose a
candidate-aware self-attention network that uses candidate news as clue to
model candidate-aware global user interest. In addition, we propose a
candidate-aware CNN network to incorporate candidate news into local behavior
context modeling and learn candidate-aware short-term user interest. Besides,
we use a candidate-aware attention network to aggregate previously clicked news
weighted by their relevance with candidate news to build candidate-aware user
representation. Experiments on real-world datasets show the effectiveness of
our method in improving news recommendation performance.",2204.04726v1,cs.IR,2022-04-10 17:02:29+00:00,"[arxiv.Result.Author('Tao Qi'), arxiv.Result.Author('Fangzhao Wu'), arxiv.Result.Author('Chuhan Wu'), arxiv.Result.Author('Yongfeng Huang')]",
537,Favorite-Candidate Voting for Eliminating the Least Popular Candidate in Metric Spaces,"We study single-candidate voting embedded in a metric space, where both
voters and candidates are points in the space, and the distances between voters
and candidates specify the voters' preferences over candidates. In the voting,
each voter is asked to submit her favorite candidate. Given the collection of
favorite candidates, a mechanism for eliminating the least popular candidate
finds a committee containing all candidates but the one to be eliminated. Each
committee is associated with a social value that is the sum of the costs
(utilities) it imposes (provides) to the voters. We design mechanisms for
finding a committee to optimize the social value. We measure the quality of a
mechanism by its distortion, defined as the worst-case ratio between the social
value of the committee found by the mechanism and the optimal one. We establish
new upper and lower bounds on the distortion of mechanisms in this
single-candidate voting, for both general metrics and well-motivated special
cases.",1911.12109v1,cs.GT,2019-11-27 12:36:39+00:00,"[arxiv.Result.Author('Xujin Chen'), arxiv.Result.Author('Minming Li'), arxiv.Result.Author('Chenhao Wang')]",
538,On the Distortion of Voting with Multiple Representative Candidates,"We study positional voting rules when candidates and voters are embedded in a
common metric space, and cardinal preferences are naturally given by distances
in the metric space. In a positional voting rule, each candidate receives a
score from each ballot based on the ballot's rank order; the candidate with the
highest total score wins the election. The cost of a candidate is his sum of
distances to all voters, and the distortion of an election is the ratio between
the cost of the elected candidate and the cost of the optimum candidate. We
consider the case when candidates are representative of the population, in the
sense that they are drawn i.i.d. from the population of the voters, and analyze
the expected distortion of positional voting rules.
  Our main result is a clean and tight characterization of positional voting
rules that have constant expected distortion (independent of the number of
candidates and the metric space). Our characterization result immediately
implies constant expected distortion for Borda Count and elections in which
each voter approves a constant fraction of all candidates. On the other hand,
we obtain super-constant expected distortion for Plurality, Veto, and approving
a constant number of candidates. These results contrast with previous results
on voting with metric preferences: When the candidates are chosen
adversarially, all of the preceding voting rules have distortion linear in the
number of candidates or voters. Thus, the model of representative candidates
allows us to distinguish voting rules which seem equally bad in the worst case.",1711.07600v1,cs.GT,2017-11-21 01:51:25+00:00,"[arxiv.Result.Author('Yu Cheng'), arxiv.Result.Author('Shaddin Dughmi'), arxiv.Result.Author('David Kempe')]",
539,A New Optical Survey of Supernova Remnant Candidates in M31,"We present a survey of optically emitting supernova remnants (SNRs) in M31
based on H$\alpha$ and [SII] images in the Local Group Survey. Using these
images, we select objects that have [SII]:H$\alpha$ $>$ 0.4 and circular
shapes. We find 76 new SNR candidates. We also inspect 234 SNR candidates
presented in previous studies, finding that only 80 of them are SNR candidates
according to our criteria. Combining them with the new candidates, we produce a
master catalog of 156 SNR candidates in M31. We classify these SNR candidates
according to two criteria: the SNR progenitor type [Type Ia and core-collapse
(CC) SNRs] and the morphological type. Type Ia and CC SNR candidates make up
23% and 77%, respectively, of the total sample. Most of the CC SNR candidates
are concentrated in the spiral arms, while the Type Ia SNR candidates are
rather distributed over the entire galaxy, including the inner region. The CC
SNR candidates are brighter in H$\alpha$ and [SII] than the Type Ia SNR
candidates. We derive a cumulative size distribution of the SNR candidates,
finding that the distribution of the candidates with 17 $< D <$ 50 pc is fitted
well by a power law with the power law index $\alpha = 2.53\pm0.04$. This
indicates that most of the SNR candidates identified in this study appear to be
in the Sedov-Taylor phase. The [SII]:H$\alpha$ distribution of the SNR
candidates is bimodal, with peaks at [SII]:H$\alpha$ $\sim$ 0.4 and $\sim$ 0.9.
The properties of these SNR candidates vary little with the galactocentric
distance. The H$\alpha$ and [SII] surface brightnesses show a good correlation
with the X-ray luminosity of the SNR candidates that are center-bright. The SNR
candidates with X-ray counterparts have higher surface brightnesses in
H$\alpha$ and [SII] and smaller sizes than those without such counterparts.",1403.4335v1,astro-ph.GA,2014-03-18 04:04:42+00:00,"[arxiv.Result.Author('Jong Hwan Lee'), arxiv.Result.Author('Myung Gyoon Lee')]",
540,Probabilistic Evaluation of Candidates and Symptom Clustering for Multidisorder Diagnosis,"This paper derives a formula for computing the conditional probability of a
set of candidates, where a candidate is a set of disorders that explain a given
set of positive findings. Such candidate sets are produced by a recent method
for multidisorder diagnosis called symptom clustering. A symptom clustering
represents a set of candidates compactly as a cartesian product of differential
diagnoses. By evaluating the probability of a candidate set, then, a large set
of candidates can be validated or pruned simultaneously. The probability of a
candidate set is then specialized to obtain the probability of a single
candidate. Unlike earlier results, the equation derived here allows the
specification of positive, negative, and unknown symptoms and does not make
assumptions about disorders not in the candidate.",1304.1136v1,cs.AI,2013-03-27 13:59:43+00:00,[arxiv.Result.Author('Thomas D. Wu')],
541,New Candidates Welcome! Possible Winners with respect to the Addition of New Candidates,"In voting contexts, some new candidates may show up in the course of the
process. In this case, we may want to determine which of the initial candidates
are possible winners, given that a fixed number $k$ of new candidates will be
added. We give a computational study of this problem, focusing on scoring
rules, and we provide a formal comparison with related problems such as control
via adding candidates or cloning.",1111.3690v1,cs.AI,2011-11-15 23:41:11+00:00,"[arxiv.Result.Author('Yann Chevaleyre'), arxiv.Result.Author('Jérôme Lang'), arxiv.Result.Author('Nicolas Maudet'), arxiv.Result.Author('Jérôme Monnot'), arxiv.Result.Author('Lirong Xia')]","Mathematical Social Sciences 64(1), 2012"
542,Optical spectroscopy of (candidate) ultra-compact X-ray binaries,"We present (preliminary) results of our systematic spectroscopic study of
(candidate) ultra-compact X-ray binaries. Most candidates are confirmed and we
found the first optical spectra of (pure) carbon-oxygen accretion discs.",astro-ph/0412115v1,astro-ph,2004-12-06 09:47:34+00:00,"[arxiv.Result.Author('Gijs Nelemans'), arxiv.Result.Author('Peter Jonker')]",
543,Lightest U-parity Particle (LUP): a hidden sector dark matter candidate,"We introduce a new dark matter candidate, the lightest U-parity particle
(LUP). We suggest it as a good dark matter candidate especially in the R-parity
violating supersymmetric model.",0808.3600v1,hep-ph,2008-08-26 22:12:41+00:00,[arxiv.Result.Author('Hye-Sung Lee')],
544,Implementing and Evaluating Candidate-Based Invariant Generation,"The discovery of inductive invariants lies at the heart of static program
verification. Presently, many automatic solutions to inductive invariant
generation are inflexible, only applicable to certain classes of programs, or
unpredictable. An automatic technique that circumvents these deficiencies to
some extent is candidate-based invariant generation. This paper describes our
efforts to apply candidate-based invariant generation in GPUVerify, a static
checker for programs that run on GPUs. We study a set of GPU programs that
contain loops, drawn from a number of open source suites and vendor SDKs.
  We describe the methodology we used to incrementally improve the invariant
generation capabilities of GPUVerify to handle these benchmarks, through
candidate-based invariant generation, using cheap static analysis to speculate
potential program invariants. We also describe a set of experiments that we
used to examine the effectiveness of our rules for candidate generation,
assessing rules based on their generality (the extent to which they generate
candidate invariants), hit rate (the extent to which the generated candidates
hold), worth (the extent to which provable candidates actually help in allowing
verification to succeed), and influence (the extent to which the success of one
generation rule depends on candidates generated by another rule).
  The candidates produced by GPUVerify help to verify 231 of the 253 programs.
This increase in precision, however, makes GPUVerify sluggish: the more
candidates that are generated, the more time is spent determining which are
inductive invariants. To speed up this process, we have investigated four
under-approximating program analyses that aim to reject false candidates
quickly and a framework whereby these analyses can run in sequence or in
parallel.",1612.01198v2,cs.SE,2016-12-04 22:32:43+00:00,"[arxiv.Result.Author('Adam Betts'), arxiv.Result.Author('Nathan Chong'), arxiv.Result.Author('Pantazis Deligiannis'), arxiv.Result.Author('Alastair F. Donaldson'), arxiv.Result.Author('Jeroen Ketema')]",
545,Gravitational lens candidates in the E-CDFS,"We report ten lens candidates in the E-CDFS from the GEMS survey. Nine of the
systems are new detections and only one of the candidates is a known lens
system. For the most promising five systems including the known lens system, we
present results from preliminary lens mass modelling, which tests if the
candidates are plausible lens systems. Photometric redshifts of the candidate
lens galaxies are obtained from the COMBO-17 galaxy catalog. Stellar masses of
the candidate lens galaxies within the Einstein radius are obtained by using
the $z$-band luminosity and the $V-z$ color-based stellar mass-to-light ratios.
As expected, the lensing masses are found to be larger than the stellar masses
of the candidate lens galaxies. These candidates have similar dark matter
fractions as compared to lenses in SLACS and COSMOS. They also roughly follow
the halo mass-stellar mass relation predicted by the subhalo abundance matching
technique. One of the candidate lens galaxies qualifies as a LIRG and may not
be a true lens because the arc-like feature in the system is likely to be an
active region of star formation in the candidate lens galaxy. Amongst the five
best candidates, one is a confirmed lens system, one is a likely lens system,
two are less likely to be lenses and the status of one of the candidates is
ambiguous. Spectroscopic follow-up of these systems is still required to
confirm lensing and/or for more accurate determination of the lens masses and
mass density profiles.",1104.0931v1,astro-ph.CO,2011-04-05 20:00:03+00:00,"[arxiv.Result.Author('A. More'), arxiv.Result.Author('K. Jahnke'), arxiv.Result.Author('S. More'), arxiv.Result.Author('A. Gallazzi'), arxiv.Result.Author('E. F. Bell'), arxiv.Result.Author('M. Barden'), arxiv.Result.Author('B. Haeussler')]","Astrophys.J.734:69,2011"
546,Spectroscopic Observations of Twenty-one Faint Cataclysmic Variables Candidates,"We provide the first minimum light spectroscopic observations for 21
previously known or suspected faint cataclysmic variable candidates. The
sources were selected from the Downes et al. (2001) living edition catalog and
the identified candidates have minimum light magnitudes of V~18-22. We confirm
15 of the candidates to be cataclysmic variables.",astro-ph/0303022v1,astro-ph,2003-03-03 05:45:22+00:00,"[arxiv.Result.Author('E. Mason'), arxiv.Result.Author('S. B. Howell')]",Astron.Astrophys. 403 (2003) 699-708
547,Investigation of variable star candidates in the globular cluster NGC 5024 (M53),"We have performed a careful investigation of the 74 candidate variable stars
presented by Safanova & Stalin (2011). For this purpose we used our data base
of imaging and light curves from Arellano Ferro et al. (2011) and Arellano
Ferro et al. (2012). We find that two candidates are known variable stars,
eight candidates were discovered first by Arellano Ferro et al. (2011) but
would not have been known to Safanova & Stalin (2011) at the time of their
paper submission, while four candidates are new variables. Three of the new
variables are SX Phe type and one is a semi-regular red giant variable (SR
type). We also tentatively confirm the presence of true variability in two
other candidates and we are unable to investigate another four candidates
because they are not in our data base. However, we find that the remaining 54
candidate variable stars are spurious detections where systematic trends in the
light curves have been mistaken for true variability. We believe that the
erroneous detections are caused by the adoption of a very low detection
threshold used to identify these candidates.",1205.5112v1,astro-ph.SR,2012-05-23 07:33:54+00:00,"[arxiv.Result.Author('D. M. Bramich'), arxiv.Result.Author('A. Arellano Ferro'), arxiv.Result.Author('R. Figuera Jaimes'), arxiv.Result.Author('Sunetra Giridhar')]",
548,Elections with Few Voters: Candidate Control Can Be Easy,"We study the computational complexity of candidate control in elections with
few voters, that is, we consider the parameterized complexity of candidate
control in elections with respect to the number of voters as a parameter. We
consider both the standard scenario of adding and deleting candidates, where
one asks whether a given candidate can become a winner (or, in the destructive
case, can be precluded from winning) by adding or deleting few candidates, as
well as a combinatorial scenario where adding/deleting a candidate
automatically means adding or deleting a whole group of candidates. Considering
several fundamental voting rules, our results show that the parameterized
complexity of candidate control, with the number of voters as the parameter, is
much more varied than in the setting with many voters.",1411.7812v2,cs.AI,2014-11-28 11:09:04+00:00,"[arxiv.Result.Author('Jiehua Chen'), arxiv.Result.Author('Piotr Faliszewski'), arxiv.Result.Author('Rolf Niedermeier'), arxiv.Result.Author('Nimrod Talmon')]",
549,Candidate Generation with Binary Codes for Large-Scale Top-N Recommendation,"Generating the Top-N recommendations from a large corpus is computationally
expensive to perform at scale. Candidate generation and re-ranking based
approaches are often adopted in industrial settings to alleviate efficiency
problems. However it remains to be fully studied how well such schemes
approximate complete rankings (or how many candidates are required to achieve a
good approximation), or to develop systematic approaches to generate
high-quality candidates efficiently. In this paper, we seek to investigate
these questions via proposing a candidate generation and re-ranking based
framework (CIGAR), which first learns a preference-preserving binary embedding
for building a hash table to retrieve candidates, and then learns to re-rank
the candidates using real-valued ranking models with a candidate-oriented
objective. We perform a comprehensive study on several large-scale real-world
datasets consisting of millions of users/items and hundreds of millions of
interactions. Our results show that CIGAR significantly boosts the Top-N
accuracy against state-of-the-art recommendation models, while reducing the
query time by orders of magnitude. We hope that this work could draw more
attention to the candidate generation problem in recommender systems.",1909.05475v1,cs.IR,2019-09-12 06:37:01+00:00,"[arxiv.Result.Author('Wang-Cheng Kang'), arxiv.Result.Author('Julian McAuley')]",
550,"Comment on ""A Candidate for a Supergravity Anomaly""","In a recent paper (arXiv:2110.06213v1) it was argued that in 4D supergravity
a previously unknown candidate anomaly exists. In the present paper it is
pointed out that the existence of such a candidate would contradict results
available in the literature, and it is substantiated that there is no candidate
anomaly of the asserted form.",2112.07390v1,hep-th,2021-12-14 13:32:03+00:00,[arxiv.Result.Author('Friedemann Brandt')],
551,Two-Winner Election Using Favorite-Candidate Voting Rule,"We investigate two-winner election problem seeking to minimize the social
cost. We are interested in strategy-proof mechanisms where each voter only
reports a single candidate. In our model, candidates and voters are located in
Euclidean space and candidates' locations are known to the mechanism. The
quality of a mechanism is measured by its distortion, defined as the worst-case
ratio between the social cost achieved by the mechanism and the optimal one. We
find that the ratio between the maximum and minimum distances among every two
candidates plays a vital role in the distortion of mechanisms. When there are
three candidates, the problem is solved mainly by previous work. We mainly
focus on the problem with at least four candidates. When voters and candidates
are embedded in 1-dimensional space, we establish several lower bounds of the
distortion. When voters and candidates are embedded in at least 3-dimensional
space, we give a tight bound of the distortion.",2205.09386v1,cs.GT,2022-05-19 08:31:14+00:00,"[arxiv.Result.Author('Zeyu Ren'), arxiv.Result.Author('Zihe Wang')]",
552,One-Shot Neural Architecture Search via Self-Evaluated Template Network,"Neural architecture search (NAS) aims to automate the search procedure of
architecture instead of manual design. Even if recent NAS approaches finish the
search within days, lengthy training is still required for a specific
architecture candidate to get the parameters for its accurate evaluation.
Recently one-shot NAS methods are proposed to largely squeeze the tedious
training process by sharing parameters across candidates. In this way, the
parameters for each candidate can be directly extracted from the shared
parameters instead of training them from scratch. However, they have no sense
of which candidate will perform better until evaluation so that the candidates
to evaluate are randomly sampled and the top-1 candidate is considered the
best. In this paper, we propose a Self-Evaluated Template Network (SETN) to
improve the quality of the architecture candidates for evaluation so that it is
more likely to cover competitive candidates. SETN consists of two components:
(1) an evaluator, which learns to indicate the probability of each individual
architecture being likely to have a lower validation loss. The candidates for
evaluation can thus be selectively sampled according to this evaluator. (2) a
template network, which shares parameters among all candidates to amortize the
training cost of generated candidates. In experiments, the architecture found
by SETN achieves state-of-the-art performance on CIFAR and ImageNet benchmarks
within comparable computation costs. Code is publicly available on GitHub:
https://github.com/D-X-Y/AutoDL-Projects.",1910.05733v4,cs.CV,2019-10-13 11:25:40+00:00,"[arxiv.Result.Author('Xuanyi Dong'), arxiv.Result.Author('Yi Yang')]",
553,Evaluating the Performance of Clone Detection Tools in Detecting Cloned Co-change Candidates,"Co-change candidates are the group of code fragments that require a change if
any of these fragments experience a modification in a commit operation during
software evolution. The cloned co-change candidates are a subset of the
co-change candidates, and the members in this subset are clones of one another.
The cloned co-change candidates are usually created by reusing existing code
fragments in a software system. Detecting cloned co-change candidates is
essential for clone-tracking, and studies have shown that we can use clone
detection tools to find cloned co-change candidates. However, although several
studies evaluate clone detection tools for their accuracy in detecting cloned
fragments, we found no study that evaluates clone detection tools for detecting
cloned co-change candidates. In this study, we explore the dimension of code
clone research for detecting cloned co-change candidates. We compare the
performance of 12 different configurations of nine promising clone detection
tools in identifying cloned co-change candidates from eight open-source C and
Java-based subject systems of various sizes and application domains. A ranked
list and analysis of the results provides valuable insights and guidelines into
selecting and configuring a clone detection tool for identifying co-change
candidates and leads to a new dimension of code clone research into change
impact analysis.",2201.07996v1,cs.SE,2022-01-20 04:26:53+00:00,"[arxiv.Result.Author('Md Nadim'), arxiv.Result.Author('Manishankar Mondal'), arxiv.Result.Author('Chanchal K. Roy'), arxiv.Result.Author('Kevin Schneider')]",
554,The Top Ten List of Gravitational Lens Candidates from the HST Medium Deep Survey,"A total of 10 good candidates for gravitational lensing have been discovered
in the WFPC2 images from the HST Medium Deep Survey (MDS) and archival primary
observations. These candidate lenses are unique HST discoveries, i.e. they are
faint systems with sub-arcsecond separations between the lensing objects and
the lensed source images. Most of them are difficult objects for ground-based
spectroscopic confirmation or for measurement of the lens and source redshifts.
Seven are ``strong lens'' candidates which appear to have multiple images of
the source. Three are cases where the single image of the source galaxy has
been significantly distorted into an arc. The first two quadruply lensed
candidates were reported in Ratnatunga et al 1995 (ApJL, 453, L5) We report on
the subsequent eight candidates and describe them with simple models based on
the assumption of singular isothermal potentials. Residuals from the simple
models for some of the candidates indicate that a more complex model for the
potential will probably be required to explain the full structural detail of
the observations once they are confirmed to be lenses. We also discuss the
effective survey area which was searched for these candidate lens objects.",astro-ph/9902100v1,astro-ph,1999-02-06 00:19:02+00:00,"[arxiv.Result.Author('K. U. Ratnatunga'), arxiv.Result.Author('R. E. Griffiths'), arxiv.Result.Author('E. J. Ostrander')]",
555,Awareness of Voter Passion Greatly Improves the Distortion of Metric Social Choice,"We develop new voting mechanisms for the case when voters and candidates are
located in an arbitrary unknown metric space, and the goal is to choose a
candidate minimizing social cost: the total distance from the voters to this
candidate. Previous work has often assumed that only ordinal preferences of the
voters are known (instead of their true costs), and focused on minimizing
distortion: the quality of the chosen candidate as compared with the best
possible candidate. In this paper, we instead assume that a (very small) amount
of information is known about the voter preference strengths, not just about
their ordinal preferences. We provide mechanisms with much better distortion
when this extra information is known as compared to mechanisms which use only
ordinal information. We quantify tradeoffs between the amount of information
known about preference strengths and the achievable distortion. We further
provide advice about which type of information about preference strengths seems
to be the most useful. Finally, we conclude by quantifying the ideal candidate
distortion, which compares the quality of the chosen outcome with the best
possible candidate that could ever exist, instead of only the best candidate
that is actually in the running.",1906.10562v1,cs.AI,2019-06-25 14:25:12+00:00,"[arxiv.Result.Author('Ben Abramowitz'), arxiv.Result.Author('Elliot Anshelevich'), arxiv.Result.Author('Wennan Zhu')]",
556,Generalized Multiplicative Indices of Polycyclic Aromatic Hydrocarbons and Benzeniod Systems,"Many types of topological indices such as degree-based topological indices,
distance-based topological indices and counting related topological indices are
explored during past recent years. Among degree based topological indices,
Zagreb indices are the oldest one and studied well. In the paper, we define a
generalized multiplicative version of these indices and compute exact formulas
for Polycyclic Aromatic Hydrocarbons and Jagged-Rectangle Benzenoid Systems.",1705.01139v1,math.CO,2017-05-02 18:47:29+00:00,"[arxiv.Result.Author('V. R. Kulli'), arxiv.Result.Author('Branden Stone'), arxiv.Result.Author('Shaohui Wang'), arxiv.Result.Author('Bing Wei')]",
557,Indicator sequences and indicator topologies of Fort transformation groups,"In the following text we prove that there exists a Fort transformation group
with indicator sequence $(p_0,\ldots,p_n)$ if and only if $0=p_0\leq
p_1\leq\cdots\leq p_n=1$, moreover we characterize all possible indicator
topological spaces of Fort transformation groups.The text will study indicator
sequences and indicator topologies of Fort transformation semigroups too.",1708.09276v1,math.GN,2017-08-25 09:48:05+00:00,"[arxiv.Result.Author('Fatemah Ayatollah Zadeh Shirazi'), arxiv.Result.Author('Fatemeh Ebrahimifar')]",
558,Predicting some physicochemical properties of octane isomers: A topological approach using ev-degree and ve-degree Zagreb indices,"Topological indices have important role in theoretical chemistry for QSPR
researches. Among the all topological indices the Randi\'c and the Zagreb
indices have been used more considerably than any other topological indices in
chemical and mathematical literature. Most of the topological indices as in the
Randi\'c and the Zagreb indices are based on the degrees of the vertices of a
connected graph. Recently novel two degree concepts have been defined in graph
theory; ev-degrees and ve-degrees. In this study we define ev-degree Zagreb
index, ve-degree Zagreb indices and ve-degree Randi\'c index by using these new
graph invariants as parallel to their corresponding classical degree versions.
We compare these new group ev-degree and ve-degree indices with the other
well-known and most used topological indices in literature such as; Wiener,
Zagreb and Randi\'c indices by modelling some physicochemical properties of
octane isomers. We show that the ev-degree Zagreb index, the ve-degree Zagreb
and the ve-degree Randi\'c indices give better correlation than Wiener, Zagreb
and Randi\'c indices to predict the some specific physicochemical properties of
octanes. We investigate the relations between the second Zagreb index and
ev-degree and ve-degree Zagreb indices and some mathematical properties of
ev-degree and ve-degree Zagreb indices. Keywords:",1701.02859v1,math.CO,2017-01-11 06:15:14+00:00,[arxiv.Result.Author('Süleyman Ediz')],
559,The average representation - a cornucopia of power indices?,"For the classical power indices there is a disproportion between power and
relative weights, in general. We introduce two new indices, based on weighted
representations, which are proportional to suitable relative weights and which
also share several important properties of the classical power indices.
Imposing further restrictions on the set of representations may lead to a whole
family of such indices.",1405.0825v1,cs.GT,2014-05-05 08:47:45+00:00,"[arxiv.Result.Author('Serguei Kaniovski'), arxiv.Result.Author('Sascha Kurz')]",
560,Chromatic Topological Indices of Certain Cycle Related Graphs,"Topological indices are real numbers invariant under graph isomorphisms.
Chromatic analogue of topological indices has been introduced recently in
literature in 2017. Mainly, chromatic versions of Zagreb indices are studied
lately. This paper discusses the notion of chromatic topological and
irregularity indices of certain cycle related graphs.",1810.02875v1,math.GM,2018-09-22 02:07:36+00:00,"[arxiv.Result.Author('Smitha Rose'), arxiv.Result.Author('Sudev Naduvath')]",
561,On quantile oriented sensitivity analysis,"We propose to study quantile oriented sensitivity indices (QOSA indices) and
quantile oriented Shapley effects (QOSE). Some theoretical properties of QOSA
indices will be given and several calculations of QOSA indices and QOSE will
allow to better understand the behaviour and the interest of these indices.",2102.05895v1,stat.ME,2021-02-11 08:59:01+00:00,"[arxiv.Result.Author('Kevin Elie-Dit-Cosaque'), arxiv.Result.Author('Véronique Maume-Deschamps')]",
562,Some modifications to the SNIP journal impact indicator,"The SNIP (source normalized impact per paper) indicator is an indicator of
the citation impact of scientific journals. The indicator, introduced by Henk
Moed in 2010, is included in Elsevier's Scopus database. The SNIP indicator
uses a source normalized approach to correct for differences in citation
practices between scientific fields. The strength of this approach is that it
does not require a field classification system in which the boundaries of
fields are explicitly defined. In this paper, a number of modifications that
will be made to the SNIP indicator are explained, and the advantages of the
resulting revised SNIP indicator are pointed out. It is argued that the
original SNIP indicator has some counterintuitive properties, and it is shown
mathematically that the revised SNIP indicator does not have these properties.
Empirically, the differences between the original SNIP indicator and the
revised one turn out to be relatively small, although some systematic
differences can be observed. Relations with other source normalized indicators
proposed in the literature are discussed as well.",1209.0785v1,cs.DL,2012-09-04 20:04:08+00:00,"[arxiv.Result.Author('Ludo Waltman'), arxiv.Result.Author('Nees Jan van Eck'), arxiv.Result.Author('Thed N. van Leeuwen'), arxiv.Result.Author('Martijn S. Visser')]",
563,An Analysis of Quality Indicators Using Approximated Optimal Distributions in a Three-dimensional Objective Space,"Although quality indicators play a crucial role in benchmarking evolutionary
multi-objective optimization algorithms, their properties are still unclear.
One promising approach for understanding quality indicators is the use of the
optimal distribution of objective vectors that optimizes each quality
indicator. However, it is difficult to obtain the optimal distribution for each
quality indicator, especially when its theoretical property is unknown. Thus,
optimal distributions for most quality indicators have not been well
investigated. To address these issues, first, we propose a problem formulation
of finding the optimal distribution for each quality indicator on an arbitrary
Pareto front. Then, we approximate the optimal distributions for nine quality
indicators using the proposed problem formulation. We analyze the nine quality
indicators using their approximated optimal distributions on eight types of
Pareto fronts of three-objective problems. Our analysis demonstrates that
uniformly-distributed objective vectors over the entire Pareto front are not
optimal in many cases. Each quality indicator has its own optimal distribution
for each Pareto front. We also examine the consistency among the nine quality
indicators.",2009.12788v1,cs.NE,2020-09-27 08:30:43+00:00,"[arxiv.Result.Author('Ryoji Tanabe'), arxiv.Result.Author('Hisao Ishibuchi')]",
564,Feasibility of nowcasting SDG indicators: a comprehensive survey,"The 2030 Agenda and accompanying Sustainable Development Goals (SDGs) are
vital in guiding national and global policy. However, many of the SDG
indicators used to measure progress toward those goals suffer from long
publication lags. Nowcasting has the potential to address this problem and
generate more timely estimates of those indicators. This paper provides
resources for achieving that potential by 1) carrying out a comprehensive
nowcasting feasibility survey of all SDG indicators to assess their potential
to be nowcast, and 2) performing a case study of indicator 9.4.1 to illustrate
and shed light on the process of performing a nowcasting exercise. There exist
231 SDG indicators, but due to only examining Tier 1 indicators and the fact
that many indicators have multiple sub-indicators, 362 indicators and
sub-indicators were eventually surveyed. Of those 362, 150 were found highly
likely to be suitable candidates for nowcasting, 87 were found to be likely,
and 125 were found to be unsuitable.",2204.01482v1,cs.CY,2022-03-23 08:23:54+00:00,"[arxiv.Result.Author('Daniel Hopp'), arxiv.Result.Author('Emily Fu'), arxiv.Result.Author('Anu Peltola')]",
565,Boyd Indices of Orlicz-Lorentz Space,"Orlicz-Lorentz spaces provide a common generalization of Orlicz spaces and
Lorentz spaces. In this paper, we investigate their Boyd indices. Bounds on the
Boyd indices in terms of the Matuszewska-Orlicz indices of the defining
functions are given. Also, we give an example to show that the Boyd indices and
Zippin indices of an Orlicz-Lorentz space need not be equal, answering a
question of Maligranda. Finally, we show how the Boyd indices are related to
whether an Orlicz-Lorentz space is p-convex or q-concave.",math/9412215v2,math.FA,1994-12-19 16:15:12+00:00,[arxiv.Result.Author('Stephen J. Montgomery-Smith')],"Function Spaces, The Second Conference, Ed.: K. Jarosz, 321-334,
  Marcel Dekker, 1995"
566,Comprehensive indicator comparisons intelligible to non-experts: The case of two SNIP versions,"A framework is proposed for comparing different types of bibliometric
indicators, introducing the notion of an Indicator Comparison Report. It
provides a comprehensive overview of the main differences and similarities of
indicators. The comparison shows both the strong points and the limitations of
each of the indicators at stake, rather than over-promoting one indicator and
ignoring the benefits of alternative constructs. It focuses on base notions,
assumptions, and application contexts, which makes it more intelligible to
non-experts. As an illustration, a comparison report is presented for the
original and the modified SNIP (Source Normalized Impact per Paper) indicator
of journal citation impact.",1510.05128v1,cs.DL,2015-10-17 14:06:52+00:00,[arxiv.Result.Author('Henk F. Moed')],
567,Further results on degree based topological indices of certain chemical networks,"There are various topological indices such as degree based topological
indices, distance based topological indices and counting related topological
indices etc. These topological indices correlate certain physicochemical
properties such as boiling point, stability of chemical compounds. In this
paper, we compute the sum-connectivity index and multiplicative Zagreb indices
for certain networks of chemical importance like silicate networks, hexagonal
networks, oxide networks, and honeycomb networks. Moreover, a comparative study
using computer-based graphs has been made to clarify their nature for these
families of networks.",1605.00253v2,math.CO,2016-05-01 13:36:28+00:00,"[arxiv.Result.Author('Shaohui Wang'), arxiv.Result.Author('Jia-Bao Liu'), arxiv.Result.Author('Chunxiang Wang'), arxiv.Result.Author('Sakander Hayat')]",
568,State and group dynamics of world stock market by principal component analysis,"We study the dynamic interactions and structural changes in global financial
indices in the years 1998-2012. We apply a principal component analysis (PCA)
to cross-correlation coefficients of the stock indices. We calculate the
correlations between principal components (PCs) and each asset, known as PC
coefficients. A change in market state is identified as a change in the first
PC coefficients. Some indices do not show significant change of PCs in market
state during crises. The indices exposed to the invested capitals in the stock
markets are at the minimum level of risk. Using the first two PC coefficients,
we identify indices that are similar and more strongly correlated than the
others. We observe that the European indices form a robust group over the
observation period. The dynamics of the individual indices within the group
increase in similarity with time, and the dynamics of indices are more similar
during the crises. Furthermore, the group formation of indices changes position
in two-dimensional spaces due to crises. Finally, after a financial crisis, the
difference of PCs between the European and American indices narrows.",1503.00421v1,physics.soc-ph,2015-03-02 06:36:49+00:00,"[arxiv.Result.Author('Ashadun Nobi'), arxiv.Result.Author('Jae Woo Lee')]",
569,A new set of cluster driven composite development indicators,"Composite development indicators used in policy making often subjectively
aggregate a restricted set of indicators. We show, using dimensionality
reduction techniques, including Principal Component Analysis (PCA) and for the
first time information filtering and hierarchical clustering, that these
composite indicators miss key information on the relationship between different
indicators. In particular, the grouping of indicators via topics is not
reflected in the data at a global and local level. We overcome these issues by
using the clustering of indicators to build a new set of cluster driven
composite development indicators that are objective, data driven, comparable
between countries, and retain interpretabilty. We discuss their consequences on
informing policy makers about country development, comparing them with the top
PageRank indicators as a benchmark. Finally, we demonstrate that our new set of
composite development indicators outperforms the benchmark on a dataset
reconstruction task.",1911.11226v3,econ.GN,2019-11-25 20:43:25+00:00,"[arxiv.Result.Author('Anshul Verma'), arxiv.Result.Author('Orazio Angelini'), arxiv.Result.Author('Tiziana Di Matteo')]",
570,Indices of Vector Fields on Singular Varieties: An Overview,"Indices of vector fields on (complex analytic) singular varieties have been
considered by various authors from several different viewpoints. All these
indices coincide with the classical local index of Poincar\'e-Hopf when the
ambient variety is a smooth manifold. One has the Schwartz index, the local
Euler obstruction, the GSV-index, the virtual index and the homological index
(and probably other indices too). In this article we give a brief overview of
all these indices and the relations amongst them. We also indicate their
relations with the various generalisations to complex analytic singular
varieties of the Chern classes of complex manifolds.",math/0603582v1,math.AG,2006-03-24 17:17:35+00:00,[arxiv.Result.Author('Jose Seade')],
571,"Percentile rank scores are congruous indicators of relative performance, or aren't they?","Percentile ranks and the I3 indicator were introduced by Bornmann,
Leydesdorff, Mutz and Opthof. These two notions are based on the concept of
percentiles (or quantiles) for discrete data. As several definitions for these
notions exist we propose one that we think is suitable in this context. Next we
show that if the notion of relative congruous indicators is carefully defined
then percentile rank scores are congruous indicators of relative performance.
The I3 indicator is a strictly congruous indicator of absolute performance.",1108.1860v1,physics.data-an,2011-08-09 06:34:33+00:00,[arxiv.Result.Author('Ronald Rousseau')],
572,Computing Higher Indicators for the Double of a Symmetric Group,"In this paper we explicitly determine all indicators for the Drinfel'd
doubles of the symmetric group acting upon up to 10 objects. We explore when
distinct characters give exactly the same indicators and when the indicators
have a zero value. We find that the indicators are all non-negative integers,
which supports our conjecture that the indicators for the Drinfel'd double of
any symmetric group will be non-negative integers, just as they are for the
symmetric groups themselves.",1206.6908v1,math.RT,2012-06-28 22:07:46+00:00,[arxiv.Result.Author('Rebecca Courter')],
573,Graph Sensitive Indices for Comparing Clusterings,"This report discusses two new indices for comparing clusterings of a set of
points. The motivation for looking at new ways for comparing clusterings stems
from the fact that the existing clustering indices are based on set cardinality
alone and do not consider the positions of data points. The new indices,
namely, the Random Walk index (RWI) and Variation of Information with Neighbors
(VIN), are both inspired by the clustering metric Variation of Information
(VI). VI possesses some interesting theoretical properties which are also
desirable in a metric for comparing clusterings. We define our indices and
discuss some of their explored properties which appear relevant for a
clustering index. We also include the results of these indices on clusterings
of some example data sets.",1411.7582v1,cs.LG,2014-11-27 13:19:15+00:00,"[arxiv.Result.Author('Zaeem Hussain'), arxiv.Result.Author('Marina Meila')]",
574,Higher Maslov Indices,"We define Maslov--type indices associated to contact and symplectic
transformation groups. There are two such families of indices. The first class
of indices are maps from the homotopy groups of the contactomorphism or
symplectomorphism group to a quotient of the integers. These are based on a
generalization of the Maslov index. The second class of indices are maps from
the homotopy groups of the space of contact structures or the space of
cohomologous symplectic forms to the homotopy groups of a simple homogeneous
space. We provide a detailed construction and describe some properties of these
indices and their applications.",1602.00571v1,math.SG,2016-02-01 15:52:09+00:00,"[arxiv.Result.Author('Roger Casals'), arxiv.Result.Author('Viktor L. Ginzburg'), arxiv.Result.Author('Francisco Presas')]",
575,How to assess case-finding in chronic diseases: Comparison of different indices,"Recently, we have proposed a new illness-death model that comprises a state
of undiagnosed chronic disease preceding the diagnosed disease. Based on this
model, the question arises how case-finding can be assessed in the presence of
mortality from all these states. We simulate two scenarios of different
performance of case-finding and apply several indices to assess case-finding in
both scenarios. One of the prevalence based indices leads to wrong conclusions.
Some indices are partly insensitive to distinguish the quality of case-finding.
The incidence based indices perform well. If possible, incidence based indices
should be preferred.",1603.06142v1,q-bio.PE,2016-03-19 20:47:11+00:00,[arxiv.Result.Author('Ralph Brinks')],
576,Generating Single Peaked Votes,"We discuss how to generate singled peaked votes uniformly from the Impartial
Culture model.",1503.02766v1,cs.GT,2015-03-10 04:15:23+00:00,[arxiv.Result.Author('Toby Walsh')],
577,Normalized Range Voting Broadly Resists Control,"We study the behavior of Range Voting and Normalized Range Voting with
respect to electoral control. Electoral control encompasses attempts from an
election chair to alter the structure of an election in order to change the
outcome. We show that a voting system resists a case of control by proving that
performing that case of control is computationally infeasible. Range Voting is
a natural extension of approval voting, and Normalized Range Voting is a simple
variant which alters each vote to maximize the potential impact of each voter.
We show that Normalized Range Voting has among the largest number of control
resistances among natural voting systems.",1005.5698v3,cs.GT,2010-05-31 15:34:11+00:00,[arxiv.Result.Author('Curtis Menton')],
578,Strategic Voting Under Uncertainty About the Voting Method,"Much of the theoretical work on strategic voting makes strong assumptions
about what voters know about the voting situation. A strategizing voter is
typically assumed to know how other voters will vote and to know the rules of
the voting method. A growing body of literature explores strategic voting when
there is uncertainty about how others will vote. In this paper, we study
strategic voting when there is uncertainty about the voting method. We
introduce three notions of manipulability for a set of voting methods: sure,
safe, and expected manipulability. With the help of a computer program, we
identify voting scenarios in which uncertainty about the voting method may
reduce or even eliminate a voter's incentive to misrepresent her preferences.
Thus, it may be in the interest of an election designer who wishes to reduce
strategic voting to leave voters uncertain about which of several reasonable
voting methods will be used to determine the winners of an election.",1907.09110v1,cs.MA,2019-07-22 03:17:19+00:00,"[arxiv.Result.Author('Wesley H. Holliday'), arxiv.Result.Author('Eric Pacuit')]","EPTCS 297, 2019, pp. 252-272"
579,Vote Delegation and Misbehavior,"We study vote delegation with ""well-behaving"" and ""misbehaving"" agents and
compare it with conventional voting. Typical examples for vote delegation are
validation or governance tasks on blockchains. There is a majority of
well-behaving agents, but they may abstain or delegate their vote to other
agents since voting is costly. Misbehaving agents always vote. We compare
conventional voting allowing for abstention with vote delegation. Preferences
of voters are private information and a positive outcome is achieved if
well-behaving agents win. We illustrate that vote delegation leads to quite
different outcomes than conventional voting with abstention. In particular, we
obtain three insights: First, if the number of misbehaving voters, denoted by f
, is high, both voting methods fail to deliver a positive outcome. Second, if f
takes an intermediate value, conventional voting delivers a positive outcome,
while vote delegation fails with probability one. Third, if f is low,
delegation delivers a positive outcome with higher probability than
conventional voting. Finally, our results characterize worst-case outcomes that
can happen in a liquid democracy.",2102.08823v2,cs.GT,2021-02-17 15:32:32+00:00,"[arxiv.Result.Author('Hans Gersbach'), arxiv.Result.Author('Akaki Mamageishvili'), arxiv.Result.Author('Manvir Schneider')]",
580,Voting Power of Teams Working Together,"Voting power determines the ""power"" of individuals who cast votes; their
power is based on their ability to influence the winning-ness of a coalition.
Usually each individual acts alone, casting either all or none of their votes
and is equally likely to do either. This paper extends this standard ""random
voting"" model to allow probabilistic voting, partial voting, and correlated
team voting. We extend the standard Banzhaf metric to account for these cases;
our generalization reduces to the standard metric under ""random voting"", This
new paradigm allows us to answer questions such as ""In the 2013 US Senate, how
much more unified would the Republicans have to be in order to have the same
power as the Democrats in attaining cloture?""",1312.3394v1,cs.GT,2013-12-12 03:19:17+00:00,[arxiv.Result.Author('Daniel Zwillinger')],
581,The Computational Impact of Partial Votes on Strategic Voting,"In many real world elections, agents are not required to rank all candidates.
We study three of the most common methods used to modify voting rules to deal
with such partial votes. These methods modify scoring rules (like the Borda
count), elimination style rules (like single transferable vote) and rules based
on the tournament graph (like Copeland) respectively. We argue that with an
elimination style voting rule like single transferable vote, partial voting
does not change the situations where strategic voting is possible. However,
with scoring rules and rules based on the tournament graph, partial voting can
increase the situations where strategic voting is possible. As a consequence,
the computational complexity of computing a strategic vote can change. For
example, with Borda count, the complexity of computing a strategic vote can
decrease or stay the same depending on how we score partial votes.",1405.7714v1,cs.GT,2014-05-28 12:13:52+00:00,"[arxiv.Result.Author('Nina Narodytska'), arxiv.Result.Author('Toby Walsh')]",
582,FASTEN: Fair and Secure Distributed Voting Using Smart Contracts,"Electing democratic representatives via voting has been a common mechanism
since the 17th century. However, these mechanisms raise concerns about
fairness, privacy, vote concealment, fair calculations of tally, and proxies
voting on their behalf for the voters. Ballot voting, and in recent times,
electronic voting via electronic voting machines (EVMs) improves fairness by
relying on centralized trust. Homomorphic encryption-based voting protocols
also assure fairness but cannot scale to large scale elections such as
presidential elections. In this paper, we leverage the blockchain technology of
distributing trust to propose a smart contract-based protocol, namely, \proto.
There are many existing protocols for voting using smart contracts. We observe
that these either are not scalable or leak the vote tally during the voting
stage, i.e., do not provide vote concealment. In contrast, we show that FASTEN
preserves voter's privacy ensures vote concealment, immutability, and avoids
double voting. We prove that the probability of privacy breaches is negligibly
small. Further, our cost analysis of executing FASTEN over Ethereum is
comparable to most of the existing cost of elections.",2102.10594v1,cs.CR,2021-02-21 12:29:23+00:00,"[arxiv.Result.Author('Sankarshan Damle'), arxiv.Result.Author('Sujit Gujar'), arxiv.Result.Author('Moin Hussain Moti')]",
583,Exploring Weak Strategy-Proofness in Voting Theory,"Voting is the aggregation of individual preferences in order to select a
winning alternative. Selection of a winner is accomplished via a voting rule,
e.g., rank-order voting, majority rule, plurality rule, approval voting. Which
voting rule should be used? In social choice theory, desirable properties of
voting rules are expressed as axioms to be satisfied. This thesis focuses on
axioms concerning strategic manipulation by voters. Sometimes, voters may
intentionally misstate their true preferences in order to alter the outcome for
their own advantage. For example, in plurality rule, if a voter knows that
their top-choice candidate will lose, then they might instead vote for their
second-choice candidate just to avoid an even less desirable result. When no
coalition of voters can strategically manipulate, then the voting rule is said
to satisfy the axiom of Strategy-Proofness. A less restrictive axiom is Weak
Strategy-Proofness (as defined by Dasgupta and Maskin (2019)), which allows for
strategic manipulation by all but the smallest coalitions. Under certain
intuitive conditions, Dasgupta and Maskin (2019) proved that the only voting
rules satisfying Strategy-Proofness are rank-order voting and majority rule. In
my thesis, I generalize their result, by proving that rank-order voting and
majority rule are surprisingly still the only voting rules satisfying Weak
Strategy-Proofness.",2005.07521v1,econ.TH,2020-05-13 19:53:08+00:00,[arxiv.Result.Author('Anne Carlstein')],
584,Designing Stable Elections: A Survey,"We survey the design of elections that are resilient to attempted
interference by third parties. For example, suppose votes have been cast in an
election between two candidates, and then each vote is randomly changed with a
small probability, independently of the other votes. It is desirable to keep
the outcome of the election the same, regardless of the changes to the votes.
It is well known that the US electoral college system is about 5 times more
likely to have a changed outcome due to vote corruption, when compared to a
majority vote. In fact, Mossel, O'Donnell and Oleszkiewicz proved in 2005 that
the majority voting method is most stable to this random vote corruption, among
voting methods where each person has a small influence on the election. We
discuss some recent progress on the analogous result for elections between more
than two candidates. In this case, plurality should be most stable to
corruption in votes. We also survey results on adversarial election
manipulation (where an adversary can select particular votes to change, perhaps
in a non-random way), and we briefly discuss ranked choice voting methods
(where a vote is a ranked list of candidates).",2006.05460v2,math.PR,2020-06-09 18:59:48+00:00,[arxiv.Result.Author('Steven Heilman')],
585,Vulnerability analysis of three remote voting methods,"This article analyses three methods of remote voting in an uncontrolled
environment: postal voting, internet voting and hybrid voting. It breaks down
the voting process into different stages and compares their vulnerabilities
considering criteria that must be respected in any democratic vote:
confidentiality, anonymity, transparency, vote unicity and authenticity.
Whether for safety or reliability, each vulnerability is quantified by three
parameters: size, visibility and difficulty to achieve. The study concludes
that the automatisation of treatments combined with the dematerialisation of
the objects used during an election tends to substitute visible vulnerabilities
of a lesser magnitude by invisible and widespread vulnerabilities.",0908.1059v1,cs.CY,2009-08-07 14:02:40+00:00,"[arxiv.Result.Author('Chantal Enguehard'), arxiv.Result.Author('Rémi Lehn')]","XXI IPSA World Congress of Political Science, RC10 Electronic
  Democracy - Dilemmas of Change?, Santiago : Chile (2009)"
586,Equation of States for Elections,"In the US 2008 presidential election, Barack Obama was elected as the 44th
president of United States, winning the 53% of popular votes and 68% of
electoral votes; in the election of 2000, Al Gore lost the election receiving
49 % of electoral votes, although he had more popular votes. It is generally
believed that the electoral votes and the popular votes are correlated; however
the detailed quantitative relationship for these two quantities is unclear.
Here, we found an interesting relationship between fractions of electoral votes
and fractions of popular votes in the presidential elections of the United
States by examining the election results from 1932 to 2004. Moreover, this
curve could provide an interesting explanation for the results of other
elections that have taken place in Taiwan.",1211.1825v1,physics.soc-ph,2012-11-08 10:59:50+00:00,[arxiv.Result.Author('Bih-Yaw Jin')],
587,"Voting power and Qualified Majority Voting with a ""no vote"" option","In recent years, enlargement of the European Union has led to increased
interest in the allocation of voting weights to member states with hugely
differing population numbers. While the eventually agreed voting scheme lacks
any strict mathematical basis, the Polish government suggested a voting scheme
based on the Penrose definition of voting power, leading to an allocation of
voting weights proportional to the square root of the population (the
""Jagiellonian Compromise""). The Penrose definition of voting power is derived
from the citizens' freedom to vote either ""yes"" or ""no"". This paper defines a
corresponding voting power based on ""yes"", ""no"" and ""abstain"" options, and it
is found that this definition also leads to a square root law, and to the same
optimal vote allocation as the Penrose scheme.",0805.3251v2,math.GM,2008-05-21 10:57:28+00:00,[arxiv.Result.Author('Martin Kurth')],
588,Transparent Voting Platform Based on Permissioned Blockchain,"Since 2004, different research was handling the challenges in the centralized
voting systems, e-voting protocols and recently the decentralized voting. So
electronic voting puts forward some difficulties regarding the voter anonymity,
the secure casting of the votes and to prevent the voting process from
frauding. The Decentralized property of the technology called ""blockchain""
could have the solution for many of the challenges in voting research area and
brings a new secure mechanism of safe and transparent voting. In this paper, a
broad comparison between ongoing voting systems has studied by analyzing their
structure and the drawbacks that should consider in future to improve the whole
election process from keeping the privacy of the voter, casting a vote with the
possibility to check if it was counted correctly to publishing the results. The
result of the paper will give a new approach to extend the target of the
election from small scale to large scale despite the fact of Ethereum
limitation which can cast on the blockchain just five votes per minute. The
primary challenge is to find an answer for this question: ""How to balance
between voter privacy and transparency without breaking the important rule
where the voter can proof for a specific candidate that he voted for him in a
bribe situation?"".",1802.10134v1,cs.CY,2018-02-22 14:20:35+00:00,[arxiv.Result.Author('Nazim Faour')],
589,Distributed Voting/Ranking with Optimal Number of States per Node,"Considering a network with $n$ nodes, where each node initially votes for one
(or more) choices out of $K$ possible choices, we present a Distributed
Multi-choice Voting/Ranking (DMVR) algorithm to determine either the choice
with maximum vote (the voting problem) or to rank all the choices in terms of
their acquired votes (the ranking problem). The algorithm consolidates node
votes across the network by updating the states of interacting nodes using two
key operations, the union and the intersection. The proposed algorithm is
simple, independent from network size, and easily scalable in terms of the
number of choices $K$, using only $K\times 2^{K-1}$ nodal states for voting,
and $K\times K!$ nodal states for ranking. We prove the number of states to be
optimal in the ranking case, this optimality is conjectured to also apply to
the voting case. The time complexity of the algorithm is analyzed in complete
graphs. We show that the time complexity for both ranking and voting is
$O(\log(n))$ for given vote percentages, and is inversely proportional to the
minimum of the vote percentage differences among various choices.",1703.08838v1,cs.DC,2017-03-26 16:19:31+00:00,"[arxiv.Result.Author('Saber Salehkaleybar'), arxiv.Result.Author('Arsalan Sharif-Nassab'), arxiv.Result.Author('S. Jamaloddin Golestani')]",
590,Assessment Voting in Large Electorates,"We analyze Assessment Voting, a new two-round voting procedure that can be
applied to binary decisions in democratic societies. In the first round, a
randomly-selected number of citizens cast their vote on one of the two
alternatives at hand, thereby irrevocably exercising their right to vote. In
the second round, after the results of the first round have been published, the
remaining citizens decide whether to vote for one alternative or to ab- stain.
The votes from both rounds are aggregated, and the final outcome is obtained by
applying the majority rule, with ties being broken by fair randomization.
Within a costly voting framework, we show that large elec- torates will choose
the preferred alternative of the majority with high prob- ability, and that
average costs will be low. This result is in contrast with the literature on
one-round voting, which predicts either higher voting costs (when voting is
compulsory) or decisions that often do not represent the preferences of the
majority (when voting is voluntary).",1712.05470v2,econ.EM,2017-12-14 22:47:38+00:00,"[arxiv.Result.Author('Hans Gersbach'), arxiv.Result.Author('Akaki Mamageishvili'), arxiv.Result.Author('Oriol Tejada')]",
591,Heuristics in Multi-Winner Approval Voting,"In many real world situations, collective decisions are made using voting.
Moreover, scenarios such as committee or board elections require voting rules
that return multiple winners. In multi-winner approval voting (AV), an agent
may vote for as many candidates as they wish. Winners are chosen by tallying up
the votes and choosing the top-$k$ candidates receiving the most votes. An
agent may manipulate the vote to achieve a better outcome by voting in a way
that does not reflect their true preferences. In complex and uncertain
situations, agents may use heuristics to strategize, instead of incurring the
additional effort required to compute the manipulation which most favors them.
In this paper, we examine voting behavior in multi-winner approval voting
scenarios with complete information. We show that people generally manipulate
their vote to obtain a better outcome, but often do not identify the optimal
manipulation. Instead, voters tend to prioritize the candidates with the
highest utilities. Using simulations, we demonstrate the effectiveness of these
heuristics in situations where agents only have access to partial information.",1905.12104v2,cs.GT,2019-05-28 21:44:07+00:00,"[arxiv.Result.Author('Jaelle Scheuerman'), arxiv.Result.Author('Jason L. Harman'), arxiv.Result.Author('Nicholas Mattei'), arxiv.Result.Author('K. Brent Venable')]",
592,Quantum voting and its physical interpretation,"Voting is a game with a no-go theorem. New proofs of Arrow's impossibility
theorem are given based on quantum information theory. We show that the
Arrowian dictator is equivalent to the perfect cloning circuit. We present
\textit{Gedankenexperiment} of voting and Bell-like inequalities of voting. We
provide the thermodynamic interpretation of voting with Landauer's principle.",1912.05356v1,quant-ph,2019-12-09 14:37:35+00:00,[arxiv.Result.Author('Woong-seon Yoo')],
593,Self-consistency of voting implies majority vote,"Paper develops axiomatic characterization of the family of majority vote
rules in the way alternative to characterization of the majority vote given in
paper of Kenneth O. May in the 1952. This, similar but different, axiomatics
focuses on the consistency of the voting procedure. Both approaches are
compared. Relation to famous Kenneth J. Arrow's Impossibility Theorem is also
discussed.",1807.00170v1,cs.GT,2018-06-30 12:49:55+00:00,[arxiv.Result.Author('Artur Poplawski')],
594,Bucklin Voting is Broadly Resistant to Control,"Electoral control models ways of changing the outcome of an election via such
actions as adding/deleting/partitioning either candidates or voters. These
actions modify an election's participation structure and aim at either making a
favorite candidate win (""constructive control"") or prevent a despised candidate
from winning (""destructive control""), which yields a total of 22 standard
control scenarios. To protect elections from such control attempts,
computational complexity has been used to show that electoral control, though
not impossible, is computationally prohibitive. Among natural voting systems
with a polynomial-time winner problem, the two systems with the highest number
of proven resistances to control types (namely 19 out of 22) are
""sincere-strategy preference-based approval voting"" (SP-AV, a modification of a
system proposed by Brams and Sanver) and fallback voting. Both are hybrid
systems; e.g., fallback voting combines approval with Bucklin voting. In this
paper, we study the control complexity of Bucklin voting itself and show that
it behaves equally well in terms of control resistance for the 20 cases
investigated so far. As Bucklin voting is a special case of fallback voting,
all resistances shown for Bucklin voting in this paper strengthen the
corresponding resistance for fallback voting.",1005.4115v1,cs.CC,2010-05-22 09:12:04+00:00,"[arxiv.Result.Author('Gábor Erdélyi'), arxiv.Result.Author('Lena Piras'), arxiv.Result.Author('Jörg Rothe')]",
595,"Square root voting system, optimal threshold and π","The problem of designing an optimal weighted voting system for the two-tier
voting, applicable in the case of the Council of Ministers of the European
Union (EU), is investigated. Various arguments in favour of the square root
voting system, where the voting weights of member states are proportional to
the square root of their population are discussed and a link between this
solution and the random walk in the one-dimensional lattice is established. It
is known that the voting power of every member state is approximately equal to
its voting weight, if the threshold q for the qualified majority in the voting
body is optimally chosen. We analyze the square root voting system for a
generic 'union' of M states and derive in this case an explicit approximate
formula for the level of the optimal threshold: q \simeq 1/2+1/\sqrt{{\pi} M}.
The prefactor 1/\sqrt{{\pi}} appears here as a result of averaging over the
ensemble of unions with random populations.",1104.5213v2,physics.soc-ph,2011-04-27 18:51:41+00:00,"[arxiv.Result.Author('Karol Zyczkowski'), arxiv.Result.Author('Wojciech Slomczynski')]",
596,Generating Single Peaked Votes,"We discuss how to generate singled peaked votes uniformly from the Impartial
Culture model.",1503.02766v1,cs.GT,2015-03-10 04:15:23+00:00,[arxiv.Result.Author('Toby Walsh')],
597,Normalized Range Voting Broadly Resists Control,"We study the behavior of Range Voting and Normalized Range Voting with
respect to electoral control. Electoral control encompasses attempts from an
election chair to alter the structure of an election in order to change the
outcome. We show that a voting system resists a case of control by proving that
performing that case of control is computationally infeasible. Range Voting is
a natural extension of approval voting, and Normalized Range Voting is a simple
variant which alters each vote to maximize the potential impact of each voter.
We show that Normalized Range Voting has among the largest number of control
resistances among natural voting systems.",1005.5698v3,cs.GT,2010-05-31 15:34:11+00:00,[arxiv.Result.Author('Curtis Menton')],
598,Strategic Voting Under Uncertainty About the Voting Method,"Much of the theoretical work on strategic voting makes strong assumptions
about what voters know about the voting situation. A strategizing voter is
typically assumed to know how other voters will vote and to know the rules of
the voting method. A growing body of literature explores strategic voting when
there is uncertainty about how others will vote. In this paper, we study
strategic voting when there is uncertainty about the voting method. We
introduce three notions of manipulability for a set of voting methods: sure,
safe, and expected manipulability. With the help of a computer program, we
identify voting scenarios in which uncertainty about the voting method may
reduce or even eliminate a voter's incentive to misrepresent her preferences.
Thus, it may be in the interest of an election designer who wishes to reduce
strategic voting to leave voters uncertain about which of several reasonable
voting methods will be used to determine the winners of an election.",1907.09110v1,cs.MA,2019-07-22 03:17:19+00:00,"[arxiv.Result.Author('Wesley H. Holliday'), arxiv.Result.Author('Eric Pacuit')]","EPTCS 297, 2019, pp. 252-272"
599,Vote Delegation and Misbehavior,"We study vote delegation with ""well-behaving"" and ""misbehaving"" agents and
compare it with conventional voting. Typical examples for vote delegation are
validation or governance tasks on blockchains. There is a majority of
well-behaving agents, but they may abstain or delegate their vote to other
agents since voting is costly. Misbehaving agents always vote. We compare
conventional voting allowing for abstention with vote delegation. Preferences
of voters are private information and a positive outcome is achieved if
well-behaving agents win. We illustrate that vote delegation leads to quite
different outcomes than conventional voting with abstention. In particular, we
obtain three insights: First, if the number of misbehaving voters, denoted by f
, is high, both voting methods fail to deliver a positive outcome. Second, if f
takes an intermediate value, conventional voting delivers a positive outcome,
while vote delegation fails with probability one. Third, if f is low,
delegation delivers a positive outcome with higher probability than
conventional voting. Finally, our results characterize worst-case outcomes that
can happen in a liquid democracy.",2102.08823v2,cs.GT,2021-02-17 15:32:32+00:00,"[arxiv.Result.Author('Hans Gersbach'), arxiv.Result.Author('Akaki Mamageishvili'), arxiv.Result.Author('Manvir Schneider')]",
600,Voting Power of Teams Working Together,"Voting power determines the ""power"" of individuals who cast votes; their
power is based on their ability to influence the winning-ness of a coalition.
Usually each individual acts alone, casting either all or none of their votes
and is equally likely to do either. This paper extends this standard ""random
voting"" model to allow probabilistic voting, partial voting, and correlated
team voting. We extend the standard Banzhaf metric to account for these cases;
our generalization reduces to the standard metric under ""random voting"", This
new paradigm allows us to answer questions such as ""In the 2013 US Senate, how
much more unified would the Republicans have to be in order to have the same
power as the Democrats in attaining cloture?""",1312.3394v1,cs.GT,2013-12-12 03:19:17+00:00,[arxiv.Result.Author('Daniel Zwillinger')],
601,The Computational Impact of Partial Votes on Strategic Voting,"In many real world elections, agents are not required to rank all candidates.
We study three of the most common methods used to modify voting rules to deal
with such partial votes. These methods modify scoring rules (like the Borda
count), elimination style rules (like single transferable vote) and rules based
on the tournament graph (like Copeland) respectively. We argue that with an
elimination style voting rule like single transferable vote, partial voting
does not change the situations where strategic voting is possible. However,
with scoring rules and rules based on the tournament graph, partial voting can
increase the situations where strategic voting is possible. As a consequence,
the computational complexity of computing a strategic vote can change. For
example, with Borda count, the complexity of computing a strategic vote can
decrease or stay the same depending on how we score partial votes.",1405.7714v1,cs.GT,2014-05-28 12:13:52+00:00,"[arxiv.Result.Author('Nina Narodytska'), arxiv.Result.Author('Toby Walsh')]",
602,FASTEN: Fair and Secure Distributed Voting Using Smart Contracts,"Electing democratic representatives via voting has been a common mechanism
since the 17th century. However, these mechanisms raise concerns about
fairness, privacy, vote concealment, fair calculations of tally, and proxies
voting on their behalf for the voters. Ballot voting, and in recent times,
electronic voting via electronic voting machines (EVMs) improves fairness by
relying on centralized trust. Homomorphic encryption-based voting protocols
also assure fairness but cannot scale to large scale elections such as
presidential elections. In this paper, we leverage the blockchain technology of
distributing trust to propose a smart contract-based protocol, namely, \proto.
There are many existing protocols for voting using smart contracts. We observe
that these either are not scalable or leak the vote tally during the voting
stage, i.e., do not provide vote concealment. In contrast, we show that FASTEN
preserves voter's privacy ensures vote concealment, immutability, and avoids
double voting. We prove that the probability of privacy breaches is negligibly
small. Further, our cost analysis of executing FASTEN over Ethereum is
comparable to most of the existing cost of elections.",2102.10594v1,cs.CR,2021-02-21 12:29:23+00:00,"[arxiv.Result.Author('Sankarshan Damle'), arxiv.Result.Author('Sujit Gujar'), arxiv.Result.Author('Moin Hussain Moti')]",
603,Exploring Weak Strategy-Proofness in Voting Theory,"Voting is the aggregation of individual preferences in order to select a
winning alternative. Selection of a winner is accomplished via a voting rule,
e.g., rank-order voting, majority rule, plurality rule, approval voting. Which
voting rule should be used? In social choice theory, desirable properties of
voting rules are expressed as axioms to be satisfied. This thesis focuses on
axioms concerning strategic manipulation by voters. Sometimes, voters may
intentionally misstate their true preferences in order to alter the outcome for
their own advantage. For example, in plurality rule, if a voter knows that
their top-choice candidate will lose, then they might instead vote for their
second-choice candidate just to avoid an even less desirable result. When no
coalition of voters can strategically manipulate, then the voting rule is said
to satisfy the axiom of Strategy-Proofness. A less restrictive axiom is Weak
Strategy-Proofness (as defined by Dasgupta and Maskin (2019)), which allows for
strategic manipulation by all but the smallest coalitions. Under certain
intuitive conditions, Dasgupta and Maskin (2019) proved that the only voting
rules satisfying Strategy-Proofness are rank-order voting and majority rule. In
my thesis, I generalize their result, by proving that rank-order voting and
majority rule are surprisingly still the only voting rules satisfying Weak
Strategy-Proofness.",2005.07521v1,econ.TH,2020-05-13 19:53:08+00:00,[arxiv.Result.Author('Anne Carlstein')],
604,Designing Stable Elections: A Survey,"We survey the design of elections that are resilient to attempted
interference by third parties. For example, suppose votes have been cast in an
election between two candidates, and then each vote is randomly changed with a
small probability, independently of the other votes. It is desirable to keep
the outcome of the election the same, regardless of the changes to the votes.
It is well known that the US electoral college system is about 5 times more
likely to have a changed outcome due to vote corruption, when compared to a
majority vote. In fact, Mossel, O'Donnell and Oleszkiewicz proved in 2005 that
the majority voting method is most stable to this random vote corruption, among
voting methods where each person has a small influence on the election. We
discuss some recent progress on the analogous result for elections between more
than two candidates. In this case, plurality should be most stable to
corruption in votes. We also survey results on adversarial election
manipulation (where an adversary can select particular votes to change, perhaps
in a non-random way), and we briefly discuss ranked choice voting methods
(where a vote is a ranked list of candidates).",2006.05460v2,math.PR,2020-06-09 18:59:48+00:00,[arxiv.Result.Author('Steven Heilman')],
605,Vulnerability analysis of three remote voting methods,"This article analyses three methods of remote voting in an uncontrolled
environment: postal voting, internet voting and hybrid voting. It breaks down
the voting process into different stages and compares their vulnerabilities
considering criteria that must be respected in any democratic vote:
confidentiality, anonymity, transparency, vote unicity and authenticity.
Whether for safety or reliability, each vulnerability is quantified by three
parameters: size, visibility and difficulty to achieve. The study concludes
that the automatisation of treatments combined with the dematerialisation of
the objects used during an election tends to substitute visible vulnerabilities
of a lesser magnitude by invisible and widespread vulnerabilities.",0908.1059v1,cs.CY,2009-08-07 14:02:40+00:00,"[arxiv.Result.Author('Chantal Enguehard'), arxiv.Result.Author('Rémi Lehn')]","XXI IPSA World Congress of Political Science, RC10 Electronic
  Democracy - Dilemmas of Change?, Santiago : Chile (2009)"
606,Equation of States for Elections,"In the US 2008 presidential election, Barack Obama was elected as the 44th
president of United States, winning the 53% of popular votes and 68% of
electoral votes; in the election of 2000, Al Gore lost the election receiving
49 % of electoral votes, although he had more popular votes. It is generally
believed that the electoral votes and the popular votes are correlated; however
the detailed quantitative relationship for these two quantities is unclear.
Here, we found an interesting relationship between fractions of electoral votes
and fractions of popular votes in the presidential elections of the United
States by examining the election results from 1932 to 2004. Moreover, this
curve could provide an interesting explanation for the results of other
elections that have taken place in Taiwan.",1211.1825v1,physics.soc-ph,2012-11-08 10:59:50+00:00,[arxiv.Result.Author('Bih-Yaw Jin')],
607,"Voting power and Qualified Majority Voting with a ""no vote"" option","In recent years, enlargement of the European Union has led to increased
interest in the allocation of voting weights to member states with hugely
differing population numbers. While the eventually agreed voting scheme lacks
any strict mathematical basis, the Polish government suggested a voting scheme
based on the Penrose definition of voting power, leading to an allocation of
voting weights proportional to the square root of the population (the
""Jagiellonian Compromise""). The Penrose definition of voting power is derived
from the citizens' freedom to vote either ""yes"" or ""no"". This paper defines a
corresponding voting power based on ""yes"", ""no"" and ""abstain"" options, and it
is found that this definition also leads to a square root law, and to the same
optimal vote allocation as the Penrose scheme.",0805.3251v2,math.GM,2008-05-21 10:57:28+00:00,[arxiv.Result.Author('Martin Kurth')],
608,Transparent Voting Platform Based on Permissioned Blockchain,"Since 2004, different research was handling the challenges in the centralized
voting systems, e-voting protocols and recently the decentralized voting. So
electronic voting puts forward some difficulties regarding the voter anonymity,
the secure casting of the votes and to prevent the voting process from
frauding. The Decentralized property of the technology called ""blockchain""
could have the solution for many of the challenges in voting research area and
brings a new secure mechanism of safe and transparent voting. In this paper, a
broad comparison between ongoing voting systems has studied by analyzing their
structure and the drawbacks that should consider in future to improve the whole
election process from keeping the privacy of the voter, casting a vote with the
possibility to check if it was counted correctly to publishing the results. The
result of the paper will give a new approach to extend the target of the
election from small scale to large scale despite the fact of Ethereum
limitation which can cast on the blockchain just five votes per minute. The
primary challenge is to find an answer for this question: ""How to balance
between voter privacy and transparency without breaking the important rule
where the voter can proof for a specific candidate that he voted for him in a
bribe situation?"".",1802.10134v1,cs.CY,2018-02-22 14:20:35+00:00,[arxiv.Result.Author('Nazim Faour')],
609,Distributed Voting/Ranking with Optimal Number of States per Node,"Considering a network with $n$ nodes, where each node initially votes for one
(or more) choices out of $K$ possible choices, we present a Distributed
Multi-choice Voting/Ranking (DMVR) algorithm to determine either the choice
with maximum vote (the voting problem) or to rank all the choices in terms of
their acquired votes (the ranking problem). The algorithm consolidates node
votes across the network by updating the states of interacting nodes using two
key operations, the union and the intersection. The proposed algorithm is
simple, independent from network size, and easily scalable in terms of the
number of choices $K$, using only $K\times 2^{K-1}$ nodal states for voting,
and $K\times K!$ nodal states for ranking. We prove the number of states to be
optimal in the ranking case, this optimality is conjectured to also apply to
the voting case. The time complexity of the algorithm is analyzed in complete
graphs. We show that the time complexity for both ranking and voting is
$O(\log(n))$ for given vote percentages, and is inversely proportional to the
minimum of the vote percentage differences among various choices.",1703.08838v1,cs.DC,2017-03-26 16:19:31+00:00,"[arxiv.Result.Author('Saber Salehkaleybar'), arxiv.Result.Author('Arsalan Sharif-Nassab'), arxiv.Result.Author('S. Jamaloddin Golestani')]",
610,Assessment Voting in Large Electorates,"We analyze Assessment Voting, a new two-round voting procedure that can be
applied to binary decisions in democratic societies. In the first round, a
randomly-selected number of citizens cast their vote on one of the two
alternatives at hand, thereby irrevocably exercising their right to vote. In
the second round, after the results of the first round have been published, the
remaining citizens decide whether to vote for one alternative or to ab- stain.
The votes from both rounds are aggregated, and the final outcome is obtained by
applying the majority rule, with ties being broken by fair randomization.
Within a costly voting framework, we show that large elec- torates will choose
the preferred alternative of the majority with high prob- ability, and that
average costs will be low. This result is in contrast with the literature on
one-round voting, which predicts either higher voting costs (when voting is
compulsory) or decisions that often do not represent the preferences of the
majority (when voting is voluntary).",1712.05470v2,econ.EM,2017-12-14 22:47:38+00:00,"[arxiv.Result.Author('Hans Gersbach'), arxiv.Result.Author('Akaki Mamageishvili'), arxiv.Result.Author('Oriol Tejada')]",
611,Heuristics in Multi-Winner Approval Voting,"In many real world situations, collective decisions are made using voting.
Moreover, scenarios such as committee or board elections require voting rules
that return multiple winners. In multi-winner approval voting (AV), an agent
may vote for as many candidates as they wish. Winners are chosen by tallying up
the votes and choosing the top-$k$ candidates receiving the most votes. An
agent may manipulate the vote to achieve a better outcome by voting in a way
that does not reflect their true preferences. In complex and uncertain
situations, agents may use heuristics to strategize, instead of incurring the
additional effort required to compute the manipulation which most favors them.
In this paper, we examine voting behavior in multi-winner approval voting
scenarios with complete information. We show that people generally manipulate
their vote to obtain a better outcome, but often do not identify the optimal
manipulation. Instead, voters tend to prioritize the candidates with the
highest utilities. Using simulations, we demonstrate the effectiveness of these
heuristics in situations where agents only have access to partial information.",1905.12104v2,cs.GT,2019-05-28 21:44:07+00:00,"[arxiv.Result.Author('Jaelle Scheuerman'), arxiv.Result.Author('Jason L. Harman'), arxiv.Result.Author('Nicholas Mattei'), arxiv.Result.Author('K. Brent Venable')]",
612,Quantum voting and its physical interpretation,"Voting is a game with a no-go theorem. New proofs of Arrow's impossibility
theorem are given based on quantum information theory. We show that the
Arrowian dictator is equivalent to the perfect cloning circuit. We present
\textit{Gedankenexperiment} of voting and Bell-like inequalities of voting. We
provide the thermodynamic interpretation of voting with Landauer's principle.",1912.05356v1,quant-ph,2019-12-09 14:37:35+00:00,[arxiv.Result.Author('Woong-seon Yoo')],
613,Self-consistency of voting implies majority vote,"Paper develops axiomatic characterization of the family of majority vote
rules in the way alternative to characterization of the majority vote given in
paper of Kenneth O. May in the 1952. This, similar but different, axiomatics
focuses on the consistency of the voting procedure. Both approaches are
compared. Relation to famous Kenneth J. Arrow's Impossibility Theorem is also
discussed.",1807.00170v1,cs.GT,2018-06-30 12:49:55+00:00,[arxiv.Result.Author('Artur Poplawski')],
614,Bucklin Voting is Broadly Resistant to Control,"Electoral control models ways of changing the outcome of an election via such
actions as adding/deleting/partitioning either candidates or voters. These
actions modify an election's participation structure and aim at either making a
favorite candidate win (""constructive control"") or prevent a despised candidate
from winning (""destructive control""), which yields a total of 22 standard
control scenarios. To protect elections from such control attempts,
computational complexity has been used to show that electoral control, though
not impossible, is computationally prohibitive. Among natural voting systems
with a polynomial-time winner problem, the two systems with the highest number
of proven resistances to control types (namely 19 out of 22) are
""sincere-strategy preference-based approval voting"" (SP-AV, a modification of a
system proposed by Brams and Sanver) and fallback voting. Both are hybrid
systems; e.g., fallback voting combines approval with Bucklin voting. In this
paper, we study the control complexity of Bucklin voting itself and show that
it behaves equally well in terms of control resistance for the 20 cases
investigated so far. As Bucklin voting is a special case of fallback voting,
all resistances shown for Bucklin voting in this paper strengthen the
corresponding resistance for fallback voting.",1005.4115v1,cs.CC,2010-05-22 09:12:04+00:00,"[arxiv.Result.Author('Gábor Erdélyi'), arxiv.Result.Author('Lena Piras'), arxiv.Result.Author('Jörg Rothe')]",
615,"Square root voting system, optimal threshold and π","The problem of designing an optimal weighted voting system for the two-tier
voting, applicable in the case of the Council of Ministers of the European
Union (EU), is investigated. Various arguments in favour of the square root
voting system, where the voting weights of member states are proportional to
the square root of their population are discussed and a link between this
solution and the random walk in the one-dimensional lattice is established. It
is known that the voting power of every member state is approximately equal to
its voting weight, if the threshold q for the qualified majority in the voting
body is optimally chosen. We analyze the square root voting system for a
generic 'union' of M states and derive in this case an explicit approximate
formula for the level of the optimal threshold: q \simeq 1/2+1/\sqrt{{\pi} M}.
The prefactor 1/\sqrt{{\pi}} appears here as a result of averaging over the
ensemble of unions with random populations.",1104.5213v2,physics.soc-ph,2011-04-27 18:51:41+00:00,"[arxiv.Result.Author('Karol Zyczkowski'), arxiv.Result.Author('Wojciech Slomczynski')]",
616,General Properties of Quantum Zero-Knowledge Proofs,"This paper studies the complexity classes QZK and HVQZK of problems having a
quantum computational zero-knowledge proof system and an honest-verifier
quantum computational zero-knowledge proof system, respectively. The results
proved in this paper include: (a) HVQZK = QZK, (b) any problem in QZK has a
public-coin quantum computational zero-knowledge proof system, (c) any problem
in QZK has a quantum computational zero-knowledge proof system of perfect
completeness, and (d) any problem in QZK has a three-message public-coin
quantum computational zero-knowledge proof system of perfect completeness with
arbitrarily small constant error in soundness. All the results above are
unconditional and do not rely any computational assumptions. For the classes
QPZK, HVQPZK, and QSZK of problems having a quantum perfect zero-knowledge
proof system, an honest-verifier quantum perfect zero-knowledge proof system,
and a quantum statistical zero-knowledge proof system, respectively, the
following new properties are proved: (e) HVQPZK = QPZK, (f) any problem in QPZK
has a public-coin quantum perfect zero-knowledge proof system, (g) any problem
in QSZK has a quantum statistical zero-knowledge proof system of perfect
completeness, and (h) any problem in QSZK has a three-message public-coin
quantum statistical zero-knowledge proof system of perfect completeness with
arbitrarily small constant error in soundness. It is stressed that our proofs
are direct and do not use complete promise problems or those equivalents. This
gives a unified framework that works well for all of quantum perfect,
statistical, and computational zero-knowledge proofs, and enables us to prove
properties even on the computational and perfect zero-knowledge proofs for
which no complete promise problems are known.",0705.1129v1,quant-ph,2007-05-08 17:20:45+00:00,[arxiv.Result.Author('Hirotada Kobayashi')],
617,Zero-knowledge against quantum attacks,"This paper proves that several interactive proof systems are zero-knowledge
against quantum attacks. This includes a few well-known classical
zero-knowledge proof systems as well as quantum interactive proof systems for
the complexity class HVQSZK, which comprises all problems having ""honest
verifier"" quantum statistical zero-knowledge proofs. It is also proved that
zero-knowledge proofs for every language in NP exist that are secure against
quantum attacks, assuming the existence of quantum computationally concealing
commitment schemes. Previously no non-trivial proof systems were known to be
zero-knowledge against quantum attacks, except in restricted settings such as
the honest-verifier and common reference string models. This paper therefore
establishes for the first time that true zero-knowledge is indeed possible in
the presence of quantum information and computation.",quant-ph/0511020v1,quant-ph,2005-11-03 19:22:45+00:00,[arxiv.Result.Author('John Watrous')],
618,On Concurrent and Resettable Zero-Knowledge Proofs for NP,"A proof is concurrent zero-knowledge if it remains zero-knowledge when many
copies of the proof are run in an asynchronous environment, such as the
Internet. It is known that zero-knowledge is not necessarily preserved in such
an environment. Designing concurrent zero-knowledge proofs is a fundamental
issue in the study of zero-knowledge since known zero-knowledge protocols
cannot be run in a realistic modern computing environment. In this paper we
present a concurrent zero-knowledge proof systems for all languages in NP.
Currently, the proof system we present is the only known proof system that
retains the zero-knowledge property when copies of the proof are allowed to run
in an asynchronous environment. Our proof system has $\tilde{O}(\log^2 k)$
rounds (for a security parameter $k$), which is almost optimal, as it is shown
by Canetti Kilian Petrank and Rosen that black-box concurrent zero-knowledge
requires $\tilde{\Omega}(\log k)$ rounds.
  Canetti, Goldreich, Goldwasser and Micali introduced the notion of {\em
resettable} zero-knowledge, and modified an earlier version of our proof system
to obtain the first resettable zero-knowledge proof system. This protocol
requires $k^{\theta(1)}$ rounds. We note that their technique also applies to
our current proof system, yielding a resettable zero-knowledge proof for NP
with $\tilde{O}(\log^2 k)$ rounds.",cs/0107004v1,cs.CR,2001-07-03 18:28:24+00:00,"[arxiv.Result.Author('Joe Kilian'), arxiv.Result.Author('Erez Petrank'), arxiv.Result.Author('Ransom Richardson')]",
619,Quantum statistical zero-knowledge,"In this paper we propose a definition for (honest verifier) quantum
statistical zero-knowledge interactive proof systems and study the resulting
complexity class, which we denote QSZK. We prove several facts regarding this
class that establish close connections between classical statistical
zero-knowledge and our definition for quantum statistical zero-knowledge, and
give some insight regarding the effect of this zero-knowledge restriction on
quantum interactive proof systems.",quant-ph/0202111v1,quant-ph,2002-02-20 00:54:18+00:00,[arxiv.Result.Author('John Watrous')],
620,"Physical Zero-knowledge Proofs for Flow Free, Hamiltonian Cycles, and Many-to-many k-disjoint Covering Paths","In this paper we describe protocols which use a standard deck of cards to
provide a perfectly sound zero-knowledge proof for Hamiltonian cycles and Flow
Free puzzles. The latter can easily be extended to provide a protocol for a
zero-knowledge proof of many-to-many k-disjoint path coverings.",2202.04113v1,cs.CR,2022-02-08 19:33:16+00:00,"[arxiv.Result.Author('Eammon Hart'), arxiv.Result.Author('Joshua A. McGinnis')]",
621,Lower Bounds for Zero-knowledge on the Internet,"We consider zero knowledge interactive proofs in a richer, more realistic
communication environment. In this setting, one may simultaneously engage in
many interactive proofs, and these proofs may take place in an asynchronous
fashion. It is known that zero-knowledge is not necessarily preserved in such
an environment; we show that for a large class of protocols, it cannot be
preserved. Any 4 round (computational) zero-knowledge interactive proof (or
argument) for a non-trivial language L is not black-box simulatable in the
asynchronous setting.",cs/0107003v2,cs.CR,2001-07-02 12:32:54+00:00,"[arxiv.Result.Author('Joe Kilian'), arxiv.Result.Author('Erez Petrank'), arxiv.Result.Author('Charles Rackoff')]",
622,Zero-Knowledge Proofs of the Conjugacy for Permutation Groups,"We design a perfect zero-knowledge proof system for recognition if two
permutation groups are conjugate.",0801.4917v1,cs.CC,2008-01-31 16:33:15+00:00,[arxiv.Result.Author('Oleg Verbitsky')],"Bulletin of the Lviv University, Series in Mechanics and
  Mathematics. Vol. 61, pp. 195--205 (2003)"
623,Quantum Zero-Knowledge Protocol Using Quantum Bit Commitment without Quantum Memory,"Zero-knowledge proof system is an important protocol that can be used as a
basic block for construction of other more complex cryptographic protocols.
Quantum zero-knowledge protocols have been proposed but, since their
implementation requires advanced quantum technology devices, experimental
implementation of zero-knowledge protocols have not being reported. In this
work, we present a quantum zero-knowledge protocol based on a quantum bit
commitment protocol that can be implemented with today technology. Hence, our
quantum zero-knowledge protocol can be readily implemented.",0801.0696v1,quant-ph,2008-01-04 15:59:42+00:00,"[arxiv.Result.Author('Rubens Viana Ramos'), arxiv.Result.Author('Jose Claudio do Nascimento')]",
624,Oracle Separations for Quantum Statistical Zero-Knowledge,"This paper investigates the power of quantum statistical zero knowledge
interactive proof systems in the relativized setting. We prove the existence of
an oracle relative to which quantum statistical zero-knowledge does not contain
UP intersect coUP, and we prove that quantum statistical zero knowledge does
not contain UP relative to a random oracle with probability 1. Our proofs of
these statements rely on a bound on output state discrimination for relativized
quantum circuits based on the quantum adversary method of Ambainis, following a
technique similar to one used by Ben-David and Kothari to prove limitations on
a query complexity variant of quantum statistical zero-knowledge.",1801.08967v1,cs.CC,2018-01-26 20:40:46+00:00,"[arxiv.Result.Author('Sanketh Menda'), arxiv.Result.Author('John Watrous')]",
625,On the Concurrent Composition of Quantum Zero-Knowledge,"We study the notion of zero-knowledge secure against quantum polynomial-time
verifiers (referred to as quantum zero-knowledge) in the concurrent composition
setting. Despite being extensively studied in the classical setting, concurrent
composition in the quantum setting has hardly been studied. We initiate a
formal study of concurrent quantum zero-knowledge. Our results are as follows:
  -Bounded Concurrent QZK for NP and QMA: Assuming post-quantum one-way
functions, there exists a quantum zero-knowledge proof system for NP in the
bounded concurrent setting. In this setting, we fix a priori the number of
verifiers that can simultaneously interact with the prover. Under the same
assumption, we also show that there exists a quantum zero-knowledge proof
system for QMA in the bounded concurrency setting.
  -Quantum Proofs of Knowledge: Assuming quantum hardness of learning with
errors (QLWE), there exists a bounded concurrent zero-knowledge proof system
for NP satisfying quantum proof of knowledge property. Our extraction mechanism
simultaneously allows for extraction probability to be negligibly close to
acceptance probability (extractability) and also ensures that the prover's
state after extraction is statistically close to the prover's state after
interacting with the verifier (simulatability). The seminal work of [Unruh
EUROCRYPT'12], and all its followups, satisfied a weaker version of
extractability property and moreover, did not achieve simulatability. Our
result yields a proof of quantum knowledge system for QMA with better
parameters than prior works.",2012.03139v4,quant-ph,2020-12-05 23:09:29+00:00,"[arxiv.Result.Author('Prabhanjan Ananth'), arxiv.Result.Author('Kai-Min Chung'), arxiv.Result.Author('Rolando L. La Placa')]",
626,Certified Everlasting Zero-Knowledge Proof for QMA,"In known constructions of classical zero-knowledge protocols for NP, either
of zero-knowledge or soundness holds only against computationally bounded
adversaries. Indeed, achieving both statistical zero-knowledge and statistical
soundness at the same time with classical verifier is impossible for NP unless
the polynomial-time hierarchy collapses, and it is also believed to be
impossible even with a quantum verifier. In this work, we introduce a novel
compromise, which we call the certified everlasting zero-knowledge proof for
QMA. It is a computational zero-knowledge proof for QMA, but the verifier
issues a classical certificate that shows that the verifier has deleted its
quantum information. If the certificate is valid, even unbounded malicious
verifier can no longer learn anything beyond the validity of the statement. We
construct a certified everlasting zero-knowledge proof for QMA. For the
construction, we introduce a new quantum cryptographic primitive, which we call
commitment with statistical binding and certified everlasting hiding, where the
hiding property becomes statistical once the receiver has issued a valid
certificate that shows that the receiver has deleted the committed information.
We construct commitment with statistical binding and certified everlasting
hiding from quantum encryption with certified deletion by Broadbent and Islam
[TCC 2020] (in a black box way), and then combine it with the quantum
sigma-protocol for QMA by Broadbent and Grilo [FOCS 2020] to construct the
certified everlasting zero-knowledge proof for QMA. Our constructions are
secure in the quantum random oracle model. Commitment with statistical binding
and certified everlasting hiding itself is of independent interest, and there
will be many other useful applications beyond zero-knowledge.",2109.14163v1,quant-ph,2021-09-29 03:05:44+00:00,"[arxiv.Result.Author('Taiga Hiroka'), arxiv.Result.Author('Tomoyuki Morimae'), arxiv.Result.Author('Ryo Nishimaki'), arxiv.Result.Author('Takashi Yamakawa')]",CRYPTO 2022
627,Analysis of Quantum Multi-Prover Zero-Knowledge Systems: Elimination of the Honest Condition and Computational Zero-Knowledge Systems for QMIP*,"Zero-knowledge and multi-prover systems are both central notions in classical
and quantum complexity theory. There is, however, little research in quantum
multi-prover zero-knowledge systems. This paper studies complexity-theoretical
aspects of the quantum multi-prover zero-knowledge systems. This paper has two
results:
  1.QMIP* systems with honest zero-knowledge can be converted into general
zero-knowledge systems without any assumptions.
  2.QMIP* has computational quantum zero-knowledge systems if a natural
computational conjecture holds.
  One of the main tools is a test (called the GHZ test) that uses GHZ states
shared by the provers, which prevents the verifier's attack in the above two
results. Another main tool is what we call the Local Hamiltonian based
Interactive protocol (LHI protocol). The LHI protocol makes previous research
for Local Hamiltonians applicable to check the history state of interactive
proofs, and we then apply Broadbent et al.'s zero-knowledge protocol for QMA
\cite{BJSW} to quantum multi-prover systems in order to obtain the second
result.",1902.10851v1,quant-ph,2019-02-28 00:45:54+00:00,[arxiv.Result.Author('Yusuke Kinoshita')],
628,Demystifying the Role of zk-SNARKs in Zcash,"Zero-knowledge proofs have always provided a clear solution when it comes to
conveying information from a prover to a verifier or vice versa without
revealing essential information about the process. Advancements in
zero-knowledge have helped develop proofs which are succinct and provide
non-interactive arguments of knowledge along with maintaining the
zero-knowledge criteria. zk-SNARKs (Zero knowledge Succinct Non-Interactive
Argument of Knowledge) are one such method that outshines itself when it comes
to advancement of zero-knowledge proofs. The underlying principle of the Zcash
algorithm is such that it delivers a full-fledged ledger-based digital currency
with strong privacy guarantees and the root of ensuring privacy lies fully on
the construction of a proper zk-SNARK. In this paper we elaborate and construct
a concrete zk-SNARK proof from scratch and explain its role in the Zcash
algorithm.",2008.00881v5,cs.CR,2020-08-03 13:59:07+00:00,"[arxiv.Result.Author('Aritra Banerjee'), arxiv.Result.Author('Michael Clear'), arxiv.Result.Author('Hitesh Tewari')]",
629,Non-Interactive Quantum Statistical and Perfect Zero-Knowledge,"This paper introduces quantum analogues of non-interactive perfect and
statistical zero-knowledge proof systems. Similar to the classical cases, it is
shown that sharing randomness or entanglement is necessary for non-trivial
protocols of non-interactive quantum perfect and statistical zero-knowledge. It
is also shown that, with sharing EPR pairs a priori, the class of languages
having one-sided bounded error non-interactive quantum perfect zero-knowledge
proof systems has a natural complete problem. Non-triviality of such a proof
system is based on the fact proved in this paper that the Graph
Non-Automorphism problem, which is not known in BQP, can be reduced to our
complete problem. Our results may be the first non-trivial quantum
zero-knowledge proofs secure even against dishonest quantum verifiers, since
our protocols are non-interactive, and thus the zero-knowledge property does
not depend on whether the verifier in the protocol is honest or not. A
restricted version of our complete problem derives a natural complete problem
for BQP.",quant-ph/0207158v1,quant-ph,2002-07-29 10:22:34+00:00,[arxiv.Result.Author('Hirotada Kobayashi')],
630,Quantum protocols for transference of proof of zero-knowledge systems,"Zero-knowledge proof system is an important protocol that can be used as a
basic block for construction of other more complex cryptographic protocols. An
intrinsic characteristic of a zero-knowledge systems is the assumption that is
impossible for the verifier to show to a third part that he has interacted with
the prover. However, it has been shown that using quantum correlations the
impossibility of transferring proofs can be successfully attacked. In this work
we show two new protocols for proof transference, being the first one based on
teleportation and the second one without using entangled states.",0705.2580v1,quant-ph,2007-05-17 18:30:02+00:00,"[arxiv.Result.Author('Jose Claudio do Nascimento'), arxiv.Result.Author('Rubens Viana Ramos')]",
631,Oracle Separations Between Quantum and Non-interactive Zero-Knowledge Classes,"We study the relationship between problems solvable by quantum algorithms in
polynomial time and those for which zero-knowledge proofs exist. In prior work,
Aaronson [arxiv:quant-ph/0111102] showed an oracle separation between BQP and
SZK, i.e. an oracle $A$ such that $\mathrm{SZK}^A \not\subseteq
\mathrm{BQP}^A$. In this paper we give a simple extension of Aaronson's result
to non-interactive zero-knowledge proofs with perfect security. This class,
NIPZK, is the most restrictive zero-knowledge class. We show that even for this
class we can construct an $A$ with $\mathrm{NIPZK}^A \not\subseteq
\mathrm{BQP}^A$.",1907.03205v1,cs.CC,2019-07-06 23:47:48+00:00,"[arxiv.Result.Author('Benjamin Morrison'), arxiv.Result.Author('Adam Groce')]",
632,"zk-Fabric, a Polylithic Syntax Zero Knowledge Joint Proof System","In this paper, we create a single-use and full syntax zero-knowledge proof
system, a.k.a zk-Fabric. Comparing with zk-SNARKS and another variant
zero-knowledge proofing system, zkBOO and it's variant zkBOO++. We present
multiple new approaches on how to use partitioned garbled circuits to achieve a
joint zero-knowledge proof system, with the benefits of less overhead and full
syntax verification. zk-Fabric based on partitioned garbled circuits has the
advantage of being versatile and single-use, meaning it can be applied to
arbitrary circuits with more comprehensive statements, and it can achieve the
non-interactivity among all participants. One of the protocols proposed within
is used for creating a new kind of partitioned garbled circuits to match the
comprehensive Boolean logical expression with multiple variables, we use the
term ""polythitic syntax"" to refer to the context-based multiple variables in a
comprehensive statement. We also designed a joint zero knowledge proof protocol
that uses partitioned garbled circuits",2110.07449v1,cs.CR,2021-10-14 15:17:07+00:00,"[arxiv.Result.Author('Sheng Sun'), arxiv.Result.Author('Dr. Tong Wen')]",
633,Zero-knowledge proof systems for QMA,"Prior work has established that all problems in NP admit classical
zero-knowledge proof systems, and under reasonable hardness assumptions for
quantum computations, these proof systems can be made secure against quantum
attacks. We prove a result representing a further quantum generalization of
this fact, which is that every problem in the complexity class QMA has a
quantum zero-knowledge proof system. More specifically, assuming the existence
of an unconditionally binding and quantum computationally concealing commitment
scheme, we prove that every problem in the complexity class QMA has a quantum
interactive proof system that is zero-knowledge with respect to efficient
quantum computations.
  Our QMA proof system is sound against arbitrary quantum provers, but only
requires an honest prover to perform polynomial-time quantum computations,
provided that it holds a quantum witness for a given instance of the QMA
problem under consideration. The proof system relies on a new variant of the
QMA-complete local Hamiltonian problem in which the local terms are described
by Clifford operations and standard basis measurements. We believe that the
QMA-completeness of this problem may have other uses in quantum complexity.",1604.02804v1,quant-ph,2016-04-11 06:21:36+00:00,"[arxiv.Result.Author('Anne Broadbent'), arxiv.Result.Author('Zhengfeng Ji'), arxiv.Result.Author('Fang Song'), arxiv.Result.Author('John Watrous')]","Proceedings of the 2016 IEEE 57th Annual Symposium on Foundations
  of Computer Science (FOCS 2016) pp.31-40"
634,Intractable Group-theoretic Problems Around Zero-knowledge Proofs,"While the amount of data created and stored continues to increase at striking
rates, data protection and concealment increases its importance as a field of
scientific study that requires more effort. It is essential to protect critical
data at every stage while it is being stored and transferred. One cryptographic
tool that is of interest and can be widely used in this medium is
zero-knowledge proof systems. This cryptographic structure allows one party to
securely guarantee the authenticity and accuracy of the data at hand, without
leaking any confidential information during communication. The strength of
zero-knowledge protocols is mostly based on a few hard-to-solve problems. There
is a need to design more secure and efficient zero-knowledge systems. This need
brings the necessity of determining suitable difficult problems to design
secure zero-knowledge schemes. In this study, after a brief overview of
zero-knowledge proof systems, the relationship of these structures to
group-theoretic algorithmic problems and an annotated list of intractable
algorithmic problems in group theory are given.",2206.13350v2,cs.CR,2022-06-08 20:46:06+00:00,[arxiv.Result.Author('Cansu Betin Onur')],
635,A Zero Knowledge Sumcheck and its Applications,"Many seminal results in Interactive Proofs (IPs) use algebraic techniques
based on low-degree polynomials, the study of which is pervasive in theoretical
computer science. Unfortunately, known methods for endowing such proofs with
zero knowledge guarantees do not retain this rich algebraic structure.
  In this work, we develop algebraic techniques for obtaining zero knowledge
variants of proof protocols in a way that leverages and preserves their
algebraic structure. Our constructions achieve unconditional (perfect) zero
knowledge in the Interactive Probabilistically Checkable Proof (IPCP) model of
Kalai and Raz [KR08] (the prover first sends a PCP oracle, then the prover and
verifier engage in an Interactive Proof in which the verifier may query the
PCP).
  Our main result is a zero knowledge variant of the sumcheck protocol [LFKN92]
in the IPCP model. The sumcheck protocol is a key building block in many IPs,
including the protocol for polynomial-space computation due to Shamir [Sha92],
and the protocol for parallel computation due to Goldwasser, Kalai, and
Rothblum [GKR15]. A core component of our result is an algebraic commitment
scheme, whose hiding property is guaranteed by algebraic query complexity lower
bounds [AW09,JKRS09]. This commitment scheme can then be used to considerably
strengthen our previous work [BCFGRS16] that gives a sumcheck protocol with
much weaker zero knowledge guarantees, itself using algebraic techniques based
on algorithms for polynomial identity testing [RS05,BW04].
  We demonstrate the applicability of our techniques by deriving zero knowledge
variants of well-known protocols based on algebraic techniques, including the
protocols of Shamir and of Goldwasser, Kalai, and Rothblum, as well as the
protocol of Babai, Fortnow, and Lund [BFL91].",1704.02086v1,cs.CC,2017-04-07 03:54:42+00:00,"[arxiv.Result.Author('Alessandro Chiesa'), arxiv.Result.Author('Michael A. Forbes'), arxiv.Result.Author('Nicholas Spooner')]",
636,Zero-Knowledge Authentication,"In the thesis we focus on designing an authentication system to authenticate
users over a network with a username and a password. The system uses the
zero-knowledge proof (ZKP) system as a password verification mechanism. The ZKP
protocol used is based on the quadratic residuosity problem. The authentication
system is defined as a method in the extensible authentication protocol (EAP).
Using a ZKP system yields interesting security properties that make the system
favourable to be used over insecure networks.",2205.05847v1,cs.CR,2022-05-12 02:40:30+00:00,"[arxiv.Result.Author('Jakob Povsic'), arxiv.Result.Author('Andrej Brodnik')]",
637,Can Alice and Bob be random: a study on human playing zero knowledge protocols,"The research described in this abstract was initiated by discussions between
the author and Giovanni Di Crescenzo in Barcelona in early 2004. It was during
Advanced Course on Contemporary Cryptology that Di Crescenzo gave a course on
zero knowledge protocols (ZKP), see [1]. After that course we started to play
with unorthodox ideas for breaking ZKP, especially one based on graph
3-coloring. It was chosen for investigation because it is being considered as a
""benchmark"" ZKP, see [2], [3]. At this point we briefly recall such a
protocol's description.",0708.3230v1,cs.CR,2007-08-23 18:39:56+00:00,[arxiv.Result.Author('Kamil Kulesza')],
638,On Zero-Knowledge Proofs over the Quantum Internet,"This paper presents a new method for quantum identity authentication (QIA)
protocols. The logic of classical zero-knowledge proofs (ZKPs) due to Schnorr
is applied in quantum circuits and algorithms. This novel approach gives an
exact way with which a prover $P$ can prove they know some secret without
transmitting that directly to a verifier $V$ by means of a quantum channel -
allowing for a ZKP wherein an eavesdropper or manipulation can be detected with
a `fail safe' design. With the anticipated advent of a `quantum internet', such
protocols and ideas may soon have utility and execution in the real world.",2212.03027v1,quant-ph,2022-12-06 14:57:00+00:00,[arxiv.Result.Author('Mark Carney')],
639,Wood traceability system using blockchain and zero-knowledge proof,"The system proposed in this study uses zero-knowledge proof (ZKP) to verify
the traceability of wood recorded in a public blockchain. Wood is a byproduct
of several states, ranging from standing trees to logs, lumber, and wood
products (hereinafter ``wood objects''). The advantage of using the blockchain
for record keeping is that participants can freely record the information at
their discretion, without any restrictions. However, the openness of the
blockchain may allow a malicious third party to introduce disinformation. In
this study, we employ ZKP and near-field communication (NFC) chips to eliminate
the possibility of disinformation introduction. ZKP is used to prove/validate
changes in the state of wood objects, and the unique nonce associated with that
state is encrypted and recorded on an NFC chip. The nonce is concealed and id
of the wood object is defined as hash value of this nonce. We developed a
prototype system based on an Android application and an Ethereum smart
contract. We confirm that wood traceability and verification can be performed
using the prototype system.",2211.11136v1,cs.CR,2022-11-21 00:56:44+00:00,"[arxiv.Result.Author('Kyohei Shibano'), arxiv.Result.Author('Tohru Nakajima'), arxiv.Result.Author('Gento Mogi')]",
640,A Survey on Zero Knowledge Range Proofs and Applications,"In last years, there has been an increasing effort to leverage Distributed
Ledger Technology (DLT), including blockchain. One of the main topics of
interest, given its importance, is the research and development of privacy
mechanisms, as for example is the case of Zero Knowledge Proofs (ZKP). ZKP is a
cryptographic technique that can be used to hide information that is put into
the ledger, while still allowing to perform validation of this data. In this
work we describe different strategies to construct Zero Knowledge Range Proofs
(ZKRP), as for example the scheme proposed by Boudot in 2001; the one proposed
in 2008 by Camenisch et al, and Bulletproofs, proposed in 2017. We also compare
these strategies and discuss possible use cases. Since Bulletproofs is the most
efficient construction, we will give a detailed description of its algorithms
and optimizations. Bulletproofs is not only more efficient than previous
schemes, but also avoids the trusted setup, which is a requirement that is not
desirable in the context of Distributed Ledger Technology (DLT) and blockchain.
In case of cryptocurrencies, if the setup phase is compromised, it would be
possible to generate money out of thin air. Interestingly, Bulletproofs can
also be used to construct generic Zero Knowledge Proofs (ZKP), in the sense
that it can be used to prove generic statements, and thus it is not only
restricted to ZKRP, but it can be used for any kind of Proof of Knowledge
(PoK). Hence Bulletproofs leads to a more powerful tool to provide privacy for
DLT. Here we describe in detail the algorithms involved in Bulletproofs
protocol for ZKRP. Also, we present our implementation, which was open sourced.",1907.06381v1,cs.CR,2019-07-15 09:16:39+00:00,"[arxiv.Result.Author('Eduardo Morais'), arxiv.Result.Author('Tommy Koens'), arxiv.Result.Author('Cees van Wijk'), arxiv.Result.Author('Aleksei Koren')]",
641,Blockchain-enabled Identity Verification for Safe Ridesharing Leveraging Zero-Knowledge Proof,"The on-demand mobility market, including ridesharing, is becoming
increasingly important with e-hailing fares growing at a rate of approximately
130% per annum since 2013. By increasing utilization of existing vehicles and
empty seats, ridesharing can provide many benefits including reduced traffic
congestion and environmental impact from vehicle usage and production. However,
the safety of riders and drivers has become of paramount concern and a method
for privacy-preserving identity verification between untrusted parties is
essential for protecting users. To this end, we propose a novel
privacy-preserving identity verification system, extending zero-knowledge proof
(ZKP) and blockchain for use in ridesharing applications. We design a
permissioned blockchain network to perform the ZKP verification of a driver's
identity, which also acts as an immutable ledger to store ride logs and ZKP
records. For the ZKP module, we design a protocol to facilitate user
verification without requiring the exchange of any private information. We
prototype the proposed system on the Hyperledger Fabric platform, with the
Hyperledger Ursa cryptography library, and conduct extensive experimentation.
To measure the prototype's performance, we utilize the Hyperledger Caliper
benchmark tool to perform extensive analysis and the results show that our
system is suitable for use in real-world ridesharing applications.",2010.14037v3,cs.CR,2020-10-27 03:43:39+00:00,"[arxiv.Result.Author('Wanxin Li'), arxiv.Result.Author('Collin Meese'), arxiv.Result.Author('Hao Guo'), arxiv.Result.Author('Mark Nejad')]",
642,"On How Zero-Knowledge Proof Blockchain Mixers Improve, and Worsen User Privacy","One of the most prominent and widely-used blockchain privacy solutions are
zero-knowledge proof (ZKP) mixers operating on top of smart contract-enabled
blockchains. ZKP mixers typically advertise their level of privacy through a
so-called anonymity set size, similar to k-anonymity, where a user hides among
a set of $k$ other users.
  In reality, however, these anonymity set claims are mostly inaccurate, as we
find through empirical measurements of the currently most active ZKP mixers. We
propose five heuristics that, in combination, can increase the probability that
an adversary links a withdrawer to the correct depositor on average by 51.94%
(108.63%) on the most popular Ethereum (ETH) and Binance Smart Chain (BSC)
mixer, respectively. Our empirical evidence is hence also the first to suggest
a differing privacy-predilection of users on ETH and BSC. We further identify
105 Decentralized Finance (DeFi) attackers leveraging ZKP mixers as the initial
funds and to deposit attack revenue (e.g., from phishing scams, hacking
centralized exchanges, and blockchain project attacks).
  State-of-the-art mixers are moreover tightly intertwined with the growing
DeFi ecosystem by offering ``anonymity mining'' (AM) incentives, i.e., mixer
users receive monetary rewards for mixing coins. However, contrary to the
claims of related work, we find that AM does not always contribute to improving
the quality of an anonymity set size of a mixer, because AM tends to attract
privacy-ignorant users naively reusing addresses.",2201.09035v1,cs.CR,2022-01-22 12:20:50+00:00,"[arxiv.Result.Author('Zhipeng Wang'), arxiv.Result.Author('Stefanos Chaliasos'), arxiv.Result.Author('Kaihua Qin'), arxiv.Result.Author('Liyi Zhou'), arxiv.Result.Author('Lifeng Gao'), arxiv.Result.Author('Pascal Berrang'), arxiv.Result.Author('Ben Livshits'), arxiv.Result.Author('Arthur Gervais')]",
643,Physical ZKP for Makaro Using a Standard Deck of Cards,"Makaro is a logic puzzle with an objective to fill numbers into a rectangular
grid to satisfy certain conditions. In 2018, Bultel et al. developed a physical
zero-knowledge proof (ZKP) protocol for Makaro using a deck of cards, which
allows a prover to physically convince a verifier that he/she knows a solution
of the puzzle without revealing it. However, their protocol requires several
identical copies of some cards, making it impractical as a deck of playing
cards found in everyday life typically consists of all different cards. In this
paper, we propose a new ZKP protocol for Makaro that can be implemented using a
standard deck (a deck consisting of all different cards). Our protocol also
uses asymptotically less cards than the protocol of Bultel et al. Most
importantly, we develop a general method to encode a number with a sequence of
all different cards. This allows us to securely compute several numerical
functions using a standard deck, such as verifying that two given numbers are
different and verifying that a number is the largest one among the given
numbers.",2112.12042v3,cs.CR,2021-12-22 17:11:32+00:00,"[arxiv.Result.Author('Suthee Ruangwises'), arxiv.Result.Author('Toshiya Itoh')]",
644,Device-Independent-Quantum-Randomness-Enhanced Zero-Knowledge Proof,"Zero-knowledge proof (ZKP) is a fundamental cryptographic primitive that
allows a prover to convince a verifier of the validity of a statement without
leaking any further information. As an efficient variant of ZKP,
non-interactive zero-knowledge proof (NIZKP) adopting the Fiat-Shamir heuristic
is essential to a wide spectrum of applications, such as federated learning,
blockchain and social networks. However, the heuristic is typically built upon
the random oracle model making ideal assumptions about hash functions, which
does not hold in reality and thus undermines the security of the protocol.
Here, we present a quantum resolution to the problem. Instead of resorting to a
random oracle model, we implement a quantum randomness service. This service
generates random numbers certified by the loophole-free Bell test and delivers
them with postquantum cryptography (PQC) authentication. Employing this
service, we conceive and implement a NIZKP of the three-colouring problem. By
bridging together three prominent research themes, quantum non-locality, PQC
and ZKP, we anticipate this work to open a new paradigm of quantum information
science.",2111.06717v1,quant-ph,2021-11-12 13:36:43+00:00,"[arxiv.Result.Author('Cheng-Long Li'), arxiv.Result.Author('Kai-Yi Zhang'), arxiv.Result.Author('Xingjian Zhang'), arxiv.Result.Author('Kui-Xing Yang'), arxiv.Result.Author('Yu Han'), arxiv.Result.Author('Su-Yi Cheng'), arxiv.Result.Author('Hongrui Cui'), arxiv.Result.Author('Wen-Zhao Liu'), arxiv.Result.Author('Ming-Han Li'), arxiv.Result.Author('Yang Liu'), arxiv.Result.Author('Bing Bai'), arxiv.Result.Author('Hai-Hao Dong'), arxiv.Result.Author('Jun Zhang'), arxiv.Result.Author('Xiongfeng Ma'), arxiv.Result.Author('Yu Yu'), arxiv.Result.Author('Jingyun Fan'), arxiv.Result.Author('Qiang Zhang'), arxiv.Result.Author('Jian-Wei Pan')]",
645,Two Standard Decks of Playing Cards are Sufficient for a ZKP for Sudoku,"Sudoku is a famous logic puzzle where the player has to fill a number between
1 and 9 into each empty cell of a $9 \times 9$ grid such that every number
appears exactly once in each row, each column, and each $3 \times 3$ block. In
2020, Sasaki et al. developed a physical card-based protocol of zero-knowledge
proof (ZKP) for Sudoku, which enables a prover to convince a verifier that
he/she knows a solution of the puzzle without revealing it. Their protocol uses
90 cards, but requires nine identical copies of some cards, which cannot be
found in a standard deck of playing cards (consisting of 52 different cards and
two jokers). Hence, nine identical standard decks are required to perform that
protocol, making the protocol not very practical. In this paper, we propose a
new ZKP protocol for Sudoku that can be performed using only two standard decks
of playing cards, regardless of whether the two decks are identical or
different. In general, we also develop the first ZKP protocol for a generalized
$n \times n$ Sudoku that can be performed using a deck of all different cards.",2106.13646v3,cs.CR,2021-06-25 14:03:36+00:00,[arxiv.Result.Author('Suthee Ruangwises')],"New Generation Computing, 40(1): 49-65 (2022)"
646,Ballot stuffing and participation privacy in pollsite voting,"We study the problem of simultaneously addressing both ballot stuffing and
participation privacy for pollsite voting systems. Ballot stuffing is the
attack where fake ballots (not cast by any eligible voter) are inserted into
the system. Participation privacy is about hiding which eligible voters have
actually cast their vote. So far, the combination of ballot stuffing and
participation privacy has been mostly studied for internet voting, where voters
are assumed to own trusted computing devices. Such approaches are inapplicable
to pollsite voting where voters typically vote bare handed. We present an
eligibility audit protocol to detect ballot stuffing in pollsite voting
protocols. This is done while protecting participation privacy from a remote
observer - one who does not physically observe voters during voting. Our
protocol can be instantiated as an additional layer on top of most existing
pollsite E2E-V voting protocols. To achieve our guarantees, we develop an
efficient zero-knowledge proof (ZKP), that, given a value $v$ and a set $\Phi$
of commitments, proves $v$ is committed by some commitment in $\Phi$, without
revealing which one. We call this a ZKP of reverse set membership because of
its relationship to the popular ZKPs of set membership. This ZKP may be of
independent interest.",2210.14833v1,cs.CR,2022-10-26 16:29:23+00:00,"[arxiv.Result.Author('Prashant Agrawal'), arxiv.Result.Author('Abhinav Nakarmi'), arxiv.Result.Author('Mahabir Prasad Jhanwar'), arxiv.Result.Author('Subodh Sharma'), arxiv.Result.Author('Subhashis Banerjee')]",
647,ZKPs: Does This Make The Cut? Recent Advances and Success of Zero-Knowledge Security Protocols,"How someone can get health insurance without sharing his health information?
How you can get a loan without disclosing your credit score? There is a method
to certify certain attributes of various data, either this is health metrics or
finance information, without revealing the data itself or any other kind of
personal data. This method is known as zero-knowledge proofs. Zero-Knowledge
techniques are mathematical methods used to verify things without sharing or
revealing underlying data. Zero-Knowledge protocols have vast applications from
simple identity schemes and blockchains to defense research programs and
nuclear arms control",2006.09990v1,cs.CR,2020-06-17 16:46:47+00:00,"[arxiv.Result.Author('Stavros Kassaras'), arxiv.Result.Author('Leandros Maglaras')]",
648,SurferMonkey: A Decentralized Anonymous Blockchain Intercommunication System via Zero Knowledge Proofs,"Blockchain intercommunication systems enable the exchanges of messages
between blockchains. This interoperability promotes innovation, unlocks
liquidity and access to assets. However, blockchains are isolated systems that
originally were not designed for interoperability. This makes cross-chain
communication, or bridges for short, insecure by nature. More precisely,
cross-chain systems face security challenges in terms of selfish rational
players such as maximal extractable value (MEV) and censorship.
  We propose to solve these challenges using zero knowledge proofs (ZKPs) for
cross-chain communication. Securing cross-chain communication is remarkably
more complex than securing single-chain events as such a system must preserve
user security against both on- and off-chain analysis.
  To achieve this goal, we propose the following pair of contributions: the
DACT protocol and the SurferMonkey infrastructure that supports the DACT
protocol. The DACT protocol is a global solution for the anonymity and security
challenges of agnostic blockchain intercommunication. DACT breaks on- and
off-chain analysis thanks to the use of ZKPs. SurferMonkey is a decentralized
infrastructure that implements DACT in practice. Since SurferMonkey works at
the blockchain application layer, any decentralized application (dApp) can use
SurferMonkey to send any type of message to a dApp on another blockchain. With
SurferMonkey, users can neither be censored nor be exposed to MEV. By applying
decentralized proactive security, we obtain resilience against selfish rational
players, and raise the security bar against cyberattacks. We have implemented a
proof of concept (PoC) of SurferMonkey by reverse engineering Tornado Cash and
by applying IDEN3 ZKP circuits. SurferMonkey enables new usecases, ranging from
anonymous voting and gaming, to a new phase of anonymous decentralized finance
(aDeFi).",2210.13242v1,cs.CR,2022-10-24 13:43:04+00:00,"[arxiv.Result.Author('Miguel Díaz Montiel'), arxiv.Result.Author('Rachid Guerraoui'), arxiv.Result.Author('Pierre-Louis Roman')]",
649,BANZKP: a Secure Authentication Scheme Using Zero Knowledge Proof for WBANs,"-Wireless body area network(WBAN) has shown great potential in improving
healthcare quality not only for patients but also for medical staff. However,
security and privacy are still an important issue in WBANs especially in
multi-hop architectures. In this paper, we propose and present the design and
the evaluation of a secure lightweight and energy efficient authentication
scheme BANZKP based on an efficient cryptographic protocol, Zero Knowledge
Proof (ZKP) and a commitment scheme. ZKP is used to confirm the identify of the
sensor nodes, with small computational requirement, which is favorable for body
sensors given their limited resources, while the commitment scheme is used to
deal with replay attacks and hence the injection attacks by committing a
message and revealing the key later. Our scheme reduces the memory requirement
by 56.13 % compared to TinyZKP [13], the comparable alternative so far for Body
Area Networks, and uses 10 % less energy.",1602.00895v1,cs.CR,2016-02-02 12:08:59+00:00,"[arxiv.Result.Author('Nesrine Khernane'), arxiv.Result.Author('Maria Potop-Butucaru'), arxiv.Result.Author('Claude Chaudet')]",
650,Zero Knowledge Proof based authentication protocol using graph isomorphism,"We live in an era of information and it is very important to handle the
exchange of information. While sending data to an authorized source, we need to
protect it from unauthorized sources, changes, and authentication. ZKP
technique can be used in designing secure authentication systems that dont
involve any direct exchange of information between the claimant and the
verifier thus preventing any possible leak of personal information. We propose
a Zero-Knowledge Proof (ZKP) algorithm based on isomorphic graphs. We suggest
most of the computations should be carried out on the users' web browser
without revealing the password to the server at any point in time. Instead, it
will generate random graphs and their permutations based on the login ID and
password.",1911.09329v1,cs.CR,2019-11-21 08:13:47+00:00,"[arxiv.Result.Author('Lavish Saluja'), arxiv.Result.Author('Ashutosh Bhatia')]",
651,Towards Verifiable Differentially-Private Polling,"Analyses that fulfill differential privacy provide plausible deniability to
individuals while allowing analysts to extract insights from data. However,
beyond an often acceptable accuracy tradeoff, these statistical disclosure
techniques generally inhibit the verifiability of the provided information, as
one cannot check the correctness of the participants' truthful information, the
differentially private mechanism, or the unbiased random number generation.
While related work has already discussed this opportunity, an efficient
implementation with a precise bound on errors and corresponding proofs of the
differential privacy property is so far missing. In this paper, we follow an
approach based on zero-knowledge proofs~(ZKPs), in specific succinct
non-interactive arguments of knowledge, as a verifiable computation technique
to prove the correctness of a differentially private query output. In
particular, we ensure the guarantees of differential privacy hold despite the
limitations of ZKPs that operate on finite fields and have limited branching
capabilities. We demonstrate that our approach has practical performance and
discuss how practitioners could employ our primitives to verifiably query
individuals' age from their digitally signed ID card in a differentially
private manner.",2206.07220v1,cs.CR,2022-06-15 00:26:59+00:00,"[arxiv.Result.Author('Gonzalo Munilla Garrido'), arxiv.Result.Author('Matthias Babel'), arxiv.Result.Author('Johannes Sedlmeir')]",
652,00,"What is the funniest number in cryptography (Episode 2)? 0 [1]. The reason is
that $\forall x, x \cdot 0 = 0$, i.e., the equation is satisfied no matter what
$x$ is. We'll use zero to attack zero-knowledge proof (ZKP). In particular,
we'll discuss a critical issue in a cutting-edge ZKP PLONK [2] C++
implementation which allows an attacker to create a forged proof that all
verifiers will accept. We'll show how theory guides the attack's direction. In
practice, the attack works like a charm and we'll show how the attack falls
through a chain of perfectly aligned software cracks. In the same codebase,
there is an independent critical ECDSA bug where (r, s) = (0, 0) is a valid
signature for arbitrary keys and messages, but we won't discuss it further
because it's a known ECDSA attack vector in the Google Wycheproof cryptanalysis
project [3] that I worked on a few years ago.
  All bugs have been responsibly disclosed through the vendor's bug bounty
program with total reward $\sim \$15,000$ (thank you).",2201.00815v1,cs.CR,2021-12-15 00:46:48+00:00,[arxiv.Result.Author('Nguyen Thoi Minh Quan')],
653,Post-Quantum Cryptographic Hardware Primitives,"The development and implementation of post-quantum cryptosystems have become
a pressing issue in the design of secure computing systems, as general quantum
computers have become more feasible in the last two years. In this work, we
introduce a set of hardware post-quantum cryptographic primitives (PCPs)
consisting of four frequently used security components, i.e., public-key
cryptosystem (PKC), key exchange (KEX), oblivious transfer (OT), and
zero-knowledge proof (ZKP). In addition, we design a high speed polynomial
multiplier to accelerate these primitives. These primitives will aid
researchers and designers in constructing quantum-proof secure computing
systems in the post-quantum era.",1903.03735v1,cs.CR,2019-03-09 04:45:43+00:00,"[arxiv.Result.Author('Lake Bu'), arxiv.Result.Author('Rashmi Agrawal'), arxiv.Result.Author('Hai Cheng'), arxiv.Result.Author('Michel A. Kinsy')]",
654,BAN-GZKP: Optimal Zero Knowledge Proof based Scheme for Wireless Body Area Networks,"BANZKP is the best to date Zero Knowledge Proof (ZKP) based secure
lightweight and energy efficient authentication scheme designed for Wireless
Area Network (WBAN). It is vulnerable to several security attacks such as the
replay attack, Distributed Denial-of-Service (DDoS) attacks at sink and
redundancy information crack. However, BANZKP needs an end-to-end
authentication which is not compliant with the human body postural mobility. We
propose a new scheme BAN-GZKP. Our scheme improves both the security and
postural mobility resilience of BANZKP. Moreover, BAN-GZKP uses only a
three-phase authentication which is optimal in the class of ZKP protocols. To
fix the security vulnerabilities of BANZKP, BAN-GZKP uses a novel random key
allocation and a Hop-by-Hop authentication definition. We further prove the
reliability of our scheme to various attacks including those to which BANZKP is
vulnerable. Furthermore, via extensive simulations we prove that our scheme,
BAN-GZKP, outperforms BANZKP in terms of reliability to human body postural
mobility for various network parameters (end-to-end delay, number of packets
exchanged in the network, number of transmissions). We compared both schemes
using representative convergecast strategies with various transmission rates
and human postural mobility. Finally, it is important to mention that BAN-GZKP
has no additional cost compared to BANZKP in terms memory, computational
complexity or energy consumption.",1802.07023v1,cs.NI,2018-02-20 09:19:11+00:00,"[arxiv.Result.Author('Gewu Bu'), arxiv.Result.Author('Maria Potop-Butucaru')]",
655,SANS: Self-sovereign Authentication for Network Slices,"5G communications proposed significant improvements over 4G in terms of
efficiency and security. Among these novelties, the 5G Network Slicing seems to
have a prominent role: deploy multiple virtual network slices, each providing a
different service with different needs and features. Like this, a Slice
Operator (SO) ruling a specific slice may want to offer a service for users
meeting some requirements. It is of paramount importance to provide a robust
authentication protocol, able to ensure that users meet the requirements, but
providing at the same time a privacy-by-design architecture. This makes even
more sense having a growing density of Internet of Things (IoT) devices
exchanging private information over the network. In this paper, we improve the
5G network slicing authentication using a Self-Sovereign Identity (SSI) scheme:
granting users full control over their data. We introduce an approach to allow
a user to prove his right to access a specific service without leaking any
information about him. Such an approach is SANS, a protocol that provides
non-linkable protection for any issued information, preventing an SO or an
eavesdropper from tracking users' activity and relating it with their real
identities. Furthermore, our protocol is scalable and can be taken as a
framework for improving related technologies in similar scenarios, like
authentication in the 5G Radio Access Network (RAN) or other wireless networks
and services. Such features can be achieved using cryptographic primitives
called Zero-Knowledge Proofs (ZKP). Upon implementing our solution using a
state-of-the-art ZKP library and performing several experiments, we provide
benchmarks demonstrating that our approach is affordable in speed and memory
consumption.",2010.15867v1,cs.CR,2020-10-29 18:12:43+00:00,"[arxiv.Result.Author('Xavier Salleras'), arxiv.Result.Author('Vanesa Daza')]",
